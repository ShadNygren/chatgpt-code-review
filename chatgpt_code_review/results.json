ShadDEBUG before parse arguments
ShadDEBUG after parse arguments - before results
ShadDEBUG: before utils
repo_url = https://github.com/ShadNygren/kwaai-pai
extensions = ['py']
ShadDEBUG repo_url = https://github.com/ShadNygren/kwaai-pai
ShadDEBUG clone_github_repository
ShadDEBUG repo_url = https://github.com/ShadNygren/kwaai-pai
ShadDEBUG local_path = kwaai-pai
ShadDEBUG - os.path.exists(local_path) = True
ShadDEBUG local_path = kwaai-pai
ShadDEBUG after results
[
    {
        "file": "kwaai-pai/app/manage.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "#!/usr/bin/env python\n\"\"\"Django's command-line utility for administrative tasks.\"\"\"\nimport os\nimport sys\n\n\ndef main():\n    \"\"\"Run administrative tasks.\"\"\"\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'app.settings')\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError as exc:\n        raise ImportError(\n            \"Couldn't import Django. Are you sure it's installed and \"\n            \"available on your PYTHONPATH environment variable? Did you \"\n            \"forget to activate a virtual environment?\"\n        ) from exc\n    execute_from_command_line(sys.argv)\n\n\nif __name__ == '__main__':\n    main()\n"
    },
    {
        "file": "kwaai-pai/app/inbox_email/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/inbox_email/apps.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from django.apps import AppConfig\n\nclass InboxEmailConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'inbox_email'"
    },
    {
        "file": "kwaai-pai/app/inbox_email/api/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/inbox_email/api/views.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from rest_framework import status, generics\nfrom rest_framework.response import Response\nimport logging\n\nfrom utilities.get_unseen_emails import get_unseen_emails\n\n\nclass ImapInboxView(generics.GenericAPIView):\n    \"\"\"   \n        Handle POST requests to retrieve unseen emails from the Inbox.\n        Returns:\n            Response: JSON response containing a list of dictionaries \n            representing the unseen emails with the keys of:\n            'Id', 'Subject', 'From', 'Date', 'Body' and 'Message-ID'.\n    \"\"\"\n    \n    def post(self, request,*args, **kwargs):\n        try:\n            unseen_emails = get_unseen_emails()\n            return Response(unseen_emails, status=status.HTTP_200_OK)   \n        except Exception as e:            \n            logging.exception(\"Unexpected error occurred when getting unseen emails.\")\n            return Response({\"detail\": \" An unexpected error occurred, \" + str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR) \n\n"
    },
    {
        "file": "kwaai-pai/app/inbox_email/api/urls.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"URLs for Email API handler\"\"\"\nfrom django.urls import path\nfrom inbox_email.api.views import ImapInboxView\n\nurlpatterns = [\n    path('', ImapInboxView.as_view(), name='email-credentials'),\n]"
    },
    {
        "file": "kwaai-pai/app/trl/setup.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\" trl is an open library for RL with transformer models.\n\nNote:\n\n   VERSION needs to be formatted following the MAJOR.MINOR.PATCH convention\n   (we need to follow this convention to be able to retrieve versioned scripts)\n\nSimple check list for release from AllenNLP repo: https://github.com/allenai/allennlp/blob/master/setup.py\n\nTo create the package for pypi.\n\n0. Prerequisites:\n   - Dependencies:\n     - twine: \"pip install twine\"\n   - Create an account in (and join the 'trl' project):\n     - PyPI: https://pypi.org/\n     - Test PyPI: https://test.pypi.org/\n\n1. Change the version in:\n   - __init__.py\n   - setup.py\n\n2. Commit these changes: \"git commit -m 'Release: VERSION'\"\n\n3. Add a tag in git to mark the release: \"git tag VERSION -m 'Add tag VERSION for pypi'\"\n   Push the tag to remote: git push --tags origin main\n\n4. Build both the sources and the wheel. Do not change anything in setup.py between\n   creating the wheel and the source distribution (obviously).\n\n   First, delete any \"build\" directory that may exist from previous builds.\n\n   For the wheel, run: \"python setup.py bdist_wheel\" in the top level directory.\n   (this will build a wheel for the python version you use to build it).\n\n   For the sources, run: \"python setup.py sdist\"\n   You should now have a /dist directory with both .whl and .tar.gz source versions.\n\n5. Check that everything looks correct by uploading the package to the pypi test server:\n\n   twine upload dist/* -r pypitest --repository-url=https://test.pypi.org/legacy/\n\n   Check that you can install it in a virtualenv/notebook by running:\n   pip install huggingface_hub fsspec aiohttp\n   pip install -U tqdm\n   pip install -i https://testpypi.python.org/pypi evaluate\n\n6. Upload the final version to actual pypi:\n   twine upload dist/* -r pypi\n\n7. Fill release notes in the tag in github once everything is looking hunky-dory.\n\n8. Change the version in __init__.py and setup.py to X.X.X+1.dev0 (e.g. VERSION=1.18.3 -> 1.18.4.dev0).\n   Then push the change with a message 'set dev version'\n\"\"\"\n\nfrom setuptools import find_packages, setup\n\n\n__version__ = \"0.7.10.dev0\"  # expected format is one of x.y.z.dev0, or x.y.z.rc1 or x.y.z (no to dashes, yes to dots)\n\nREQUIRED_PKGS = [\n    \"torch>=1.4.0\",\n    \"transformers>=4.31.0\",\n    \"numpy>=1.18.2\",\n    \"accelerate\",\n    \"datasets\",\n    \"tyro>=0.5.11\",\n]\nEXTRAS = {\n    \"test\": [\"parameterized\", \"pytest\", \"pytest-xdist\", \"accelerate\"],\n    \"peft\": [\"peft>=0.4.0\"],\n    \"diffusers\": [\"diffusers>=0.18.0\"],\n    \"deepspeed\": [\"deepspeed>=0.9.5\"],\n    \"benchmark\": [\"wandb\", \"ghapi\", \"openrlbenchmark==0.2.1a5\", \"requests\", \"deepspeed\"],\n    \"quantization\": [\"bitsandbytes<=0.41.1\"],\n}\nEXTRAS[\"dev\"] = []\nfor reqs in EXTRAS.values():\n    EXTRAS[\"dev\"].extend(reqs)\n\nsetup(\n    name=\"trl\",\n    license=\"Apache 2.0\",\n    classifiers=[\n        \"Development Status :: 2 - Pre-Alpha\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Natural Language :: English\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n    ],\n    url=\"https://github.com/huggingface/trl\",\n    packages=find_packages(),\n    include_package_data=True,\n    install_requires=REQUIRED_PKGS,\n    extras_require=EXTRAS,\n    python_requires=\">=3.7\",\n    long_description=open(\"README.md\", encoding=\"utf-8\").read(),\n    long_description_content_type=\"text/markdown\",\n    zip_safe=False,\n    version=__version__,\n    description=\"Train transformer language models with reinforcement learning.\",\n    keywords=\"ppo, transformers, huggingface, gpt2, language modeling, rlhf\",\n    author=\"Leandro von Werra\",\n    author_email=\"leandro.vonwerra@gmail.com\",\n)\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/core.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\nimport gc\nimport random\nimport warnings\nfrom contextlib import contextmanager\nfrom typing import Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import top_k_top_p_filtering\n\nfrom .import_utils import is_npu_available, is_xpu_available\n\n\ntry:\n    from collections.abc import Mapping\nexcept ImportError:\n    from collections import Mapping\n\n\nWANDB_PADDING = -1\n\n\ndef flatten_dict(nested: Dict, sep: str = \"/\") -> Dict:\n    \"\"\"Flatten dictionary and concatenate nested keys with separator.\"\"\"\n\n    def recurse(nest: Dict, prefix: str, into: Dict) -> None:\n        for k, v in nest.items():\n            if sep in k:\n                raise ValueError(f\"separator '{sep}' not allowed to be in key '{k}'\")\n            if isinstance(v, Mapping):\n                recurse(v, prefix + k + sep, into)\n            else:\n                into[prefix + k] = v\n\n    flat = {}\n    recurse(nested, \"\", flat)\n    return flat\n\n\ndef convert_to_scalar(stats: Dict) -> Dict:\n    \"\"\"\n    Converts the stats from a flattened dict to single scalar dicts\n    \"\"\"\n    tensorboard_stats = {}\n    for k, v in stats.items():\n        # for tensorboard compatibility - arrays and tensors are ignored with tensorboard\n        # therefore we convert single element tensors to scalars\n        if (isinstance(v, torch.Tensor) or isinstance(v, np.ndarray)) and (\n            len(v.shape) == 0 or (len(v.shape) == 1 and v.shape[0] == 1)\n        ):\n            v = v.item()\n        tensorboard_stats[k] = v\n    return tensorboard_stats\n\n\ndef stack_dicts(stats_dicts: List[Dict]) -> Dict:\n    \"\"\"Stack the values of a dict.\"\"\"\n    results = dict()\n    for k in stats_dicts[0]:\n        stats_list = [torch.flatten(d[k]) for d in stats_dicts]\n        results[k] = pad_sequence(stats_list, batch_first=True, padding_value=WANDB_PADDING)\n    return results\n\n\ndef add_suffix(input_dict: Dict, suffix: str) -> Dict:\n    \"\"\"Add suffix to dict keys.\"\"\"\n    return dict((k + suffix, v) for k, v in input_dict.items())\n\n\ndef pad_to_size(tensor: torch.Tensor, size: int, dim: int = 1, padding: int = 50256) -> torch.Tensor:\n    \"\"\"Pad tensor to size.\"\"\"\n    t_size = tensor.size()[dim]\n    if t_size == size:\n        return tensor\n    else:\n        return torch.nn.functional.pad(tensor, (0, size - t_size), \"constant\", padding)\n\n\ndef logprobs_from_logits(logits: torch.Tensor, labels: torch.Tensor, gather: bool = True) -> torch.Tensor:\n    \"\"\"\n    See: https://github.com/pytorch/pytorch/issues/563#issuecomment-330103591\n    \"\"\"\n    logp = F.log_softmax(logits, dim=2)\n\n    if not gather:\n        return logp\n    logpy = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n    return logpy\n\n\ndef whiten(values: torch.Tensor, shift_mean: bool = True) -> torch.Tensor:\n    \"\"\"Whiten values.\"\"\"\n    mean, var = torch.mean(values), torch.var(values)\n    whitened = (values - mean) * torch.rsqrt(var + 1e-8)\n    if not shift_mean:\n        whitened += mean\n    return whitened\n\n\ndef masked_mean(values: torch.Tensor, mask: torch.Tensor, axis: bool = None) -> torch.Tensor:\n    \"\"\"Compute mean of tensor with a masked values.\"\"\"\n    if axis is not None:\n        return (values * mask).sum(axis=axis) / mask.sum(axis=axis)\n    else:\n        return (values * mask).sum() / mask.sum()\n\n\ndef masked_var(values: torch.Tensor, mask: torch.Tensor, unbiased: bool = True) -> torch.Tensor:\n    \"\"\"Compute variance of tensor with masked values.\"\"\"\n    mean = masked_mean(values, mask)\n    centered_values = values - mean\n    variance = masked_mean(centered_values**2, mask)\n    if unbiased:\n        mask_sum = mask.sum()\n        if mask_sum == 0:\n            raise ValueError(\n                \"The sum of the mask is zero, which can happen when `mini_batch_size=1`;\"\n                \"try increase the `mini_batch_size` or `gradient_accumulation_steps`\"\n            )\n        # note that if mask_sum == 1, then there is a division by zero issue\n        # to avoid it you just need to use a larger minibatch_size\n        bessel_correction = mask_sum / (mask_sum - 1)\n        variance = variance * bessel_correction\n    return variance\n\n\ndef masked_whiten(values: torch.Tensor, mask: torch.Tensor, shift_mean: bool = True) -> torch.Tensor:\n    \"\"\"Whiten values with masked values.\"\"\"\n    mean, var = masked_mean(values, mask), masked_var(values, mask)\n    whitened = (values - mean) * torch.rsqrt(var + 1e-8)\n    if not shift_mean:\n        whitened += mean\n    return whitened\n\n\ndef clip_by_value(x: torch.Tensor, tensor_min: float, tensor_max: float) -> torch.Tensor:\n    \"\"\"\n    Tensor extension to torch.clamp\n    https://github.com/pytorch/pytorch/issues/2793#issuecomment-428784713\n    \"\"\"\n    clipped = torch.max(torch.min(x, tensor_max), tensor_min)\n    return clipped\n\n\ndef entropy_from_logits(logits: torch.Tensor) -> torch.Tensor:\n    \"\"\"Calculate entropy from logits.\"\"\"\n    pd = torch.nn.functional.softmax(logits, dim=-1)\n    entropy = torch.logsumexp(logits, axis=-1) - torch.sum(pd * logits, axis=-1)\n    return entropy\n\n\ndef average_torch_dicts(list_of_dicts: List[Dict]) -> Dict:\n    \"\"\"Average values of a list of dicts with torch tensors.\"\"\"\n    average_dict = dict()\n    for key in list_of_dicts[0].keys():\n        average_dict[key] = torch.mean(torch.stack([d[key] for d in list_of_dicts]), axis=0)\n    return average_dict\n\n\ndef stats_to_np(stats_dict: Dict) -> Dict:\n    \"\"\"Cast all torch.tensors in dict to numpy arrays.\"\"\"\n    new_dict = dict()\n    for k, v in stats_dict.items():\n        if isinstance(v, torch.Tensor):\n            new_dict[k] = v.detach().cpu()\n            if new_dict[k].dtype == torch.bfloat16:\n                new_dict[k] = new_dict[k].float()\n            new_dict[k] = new_dict[k].numpy()\n        else:\n            new_dict[k] = v\n        if np.isscalar(new_dict[k]):\n            new_dict[k] = float(new_dict[k])\n    return new_dict\n\n\ndef respond_to_batch(\n    model: nn.Module, queries: List[torch.LongTensor], txt_len: int = 20, top_k: int = 0, top_p: float = 1.0\n) -> torch.LongTensor:\n    \"\"\"Sample text from language model.\"\"\"\n    input_ids = queries\n    for i in range(txt_len):\n        # Get Logits\n        outputs = model(input_ids)\n        next_token_logits = outputs[0][:, -1, :]\n        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n        # Sample\n        probs = F.softmax(next_token_logits, dim=-1)\n        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n    return input_ids[:, -txt_len:]\n\n\ndef set_seed(seed: int) -> None:\n    \"\"\"\n    Helper function for reproducible behavior to set the seed in `random`, `numpy`, and `torch`.\n\n    Args:\n        seed (`int`): The seed to set.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if is_xpu_available():\n        torch.xpu.manual_seed_all(seed)\n    elif is_npu_available():\n        torch.npu.manual_seed_all(seed)\n    else:\n        torch.cuda.manual_seed_all(seed)\n\n\nclass LengthSampler:\n    \"\"\"\n    Samples a length\n    \"\"\"\n\n    def __init__(self, min_value: int, max_value: int):\n        self.values = list(range(min_value, max_value))\n\n    def __call__(self) -> int:\n        return np.random.choice(self.values)\n\n\nclass PPODecorators(object):\n    optimize_device_cache = False\n\n    @classmethod\n    @contextmanager\n    def empty_device_cache(cls):\n        yield\n        if cls.optimize_device_cache:\n            if is_xpu_available():\n                gc.collect()\n                torch.xpu.empty_cache()\n                gc.collect()\n            elif is_npu_available():\n                gc.collect()\n                torch.npu.empty_cache()\n                gc.collect()\n            elif torch.cuda.is_available():\n                gc.collect()\n                torch.cuda.empty_cache()\n                gc.collect()\n\n\ndef randn_tensor(\n    shape: Union[Tuple, List],\n    generator: Optional[Union[List[torch.Generator], torch.Generator]] = None,\n    device: Optional[torch.device] = None,\n    dtype: Optional[torch.dtype] = None,\n    layout: Optional[torch.layout] = None,\n) -> torch.Tensor:\n    \"\"\"A helper function to create random tensors on the desired `device` with the desired `dtype`. When\n    passing a list of generators, you can seed each batch size individually. If CPU generators are passed, the tensor\n    is always created on the CPU.\n    \"\"\"\n    # device on which tensor is created defaults to device\n    rand_device = device\n    batch_size = shape[0]\n\n    layout = layout or torch.strided\n    device = device or torch.device(\"cpu\")\n\n    if generator is not None:\n        gen_device_type = generator.device.type if not isinstance(generator, list) else generator[0].device.type\n        if gen_device_type != device.type and gen_device_type == \"cpu\":\n            rand_device = \"cpu\"\n            if device != \"mps\":\n                warnings.warn(\n                    f\"The passed generator was created on 'cpu' even though a tensor on {device} was expected.\"\n                    f\" Tensors will be created on 'cpu' and then moved to {device}. Note that one can probably\"\n                    f\" slighly speed up this function by passing a generator that was created on the {device} device.\"\n                )\n        elif gen_device_type != device.type and gen_device_type == \"cuda\":\n            raise ValueError(f\"Cannot generate a {device} tensor from a generator of type {gen_device_type}.\")\n\n    # make sure generator list of length 1 is treated like a non-list\n    if isinstance(generator, list) and len(generator) == 1:\n        generator = generator[0]\n\n    if isinstance(generator, list):\n        shape = (1,) + shape[1:]\n        latents = [\n            torch.randn(shape, generator=generator[i], device=rand_device, dtype=dtype, layout=layout)\n            for i in range(batch_size)\n        ]\n        latents = torch.cat(latents, dim=0).to(device)\n    else:\n        latents = torch.randn(shape, generator=generator, device=rand_device, dtype=dtype, layout=layout).to(device)\n\n    return latents\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/__init__.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# flake8: noqa\n\n__version__ = \"0.7.10.dev0\"\n\nfrom .core import set_seed\nfrom .environment import TextEnvironment, TextHistory\nfrom .extras import BestOfNSampler\nfrom .import_utils import (\n    is_bitsandbytes_available,\n    is_diffusers_available,\n    is_npu_available,\n    is_peft_available,\n    is_wandb_available,\n    is_xpu_available,\n)\nfrom .models import (\n    AutoModelForCausalLMWithValueHead,\n    AutoModelForSeq2SeqLMWithValueHead,\n    PreTrainedModelWrapper,\n    create_reference_model,\n)\nfrom .trainer import (\n    DataCollatorForCompletionOnlyLM,\n    DPOTrainer,\n    IterativeSFTTrainer,\n    PPOConfig,\n    PPOTrainer,\n    RewardConfig,\n    RewardTrainer,\n    SFTTrainer,\n)\n\n\nif is_diffusers_available():\n    from .models import (\n        DDPOPipelineOutput,\n        DDPOSchedulerOutput,\n        DDPOStableDiffusionPipeline,\n        DefaultDDPOStableDiffusionPipeline,\n    )\n    from .trainer import DDPOConfig, DDPOTrainer\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/import_utils.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\nimport importlib\nimport sys\n\n\nif sys.version_info < (3, 8):\n    _is_python_greater_3_8 = False\nelse:\n    _is_python_greater_3_8 = True\n\n\ndef is_peft_available() -> bool:\n    return importlib.util.find_spec(\"peft\") is not None\n\n\ndef is_unsloth_available() -> bool:\n    return importlib.util.find_spec(\"unsloth\") is not None\n\n\ndef is_accelerate_greater_20_0() -> bool:\n    if _is_python_greater_3_8:\n        from importlib.metadata import version\n\n        accelerate_version = version(\"accelerate\")\n    else:\n        import pkg_resources\n\n        accelerate_version = pkg_resources.get_distribution(\"accelerate\").version\n    return accelerate_version >= \"0.20.0\"\n\n\ndef is_transformers_greater_than(version: str) -> bool:\n    _transformers_version = importlib.metadata.version(\"transformers\")\n    return _transformers_version > version\n\n\ndef is_torch_greater_2_0() -> bool:\n    if _is_python_greater_3_8:\n        from importlib.metadata import version\n\n        torch_version = version(\"torch\")\n    else:\n        import pkg_resources\n\n        torch_version = pkg_resources.get_distribution(\"torch\").version\n    return torch_version >= \"2.0\"\n\n\ndef is_diffusers_available() -> bool:\n    return importlib.util.find_spec(\"diffusers\") is not None\n\n\ndef is_bitsandbytes_available() -> bool:\n    import torch\n\n    # bnb can be imported without GPU but is not usable.\n    return importlib.util.find_spec(\"bitsandbytes\") is not None and torch.cuda.is_available()\n\n\ndef is_torchvision_available() -> bool:\n    return importlib.util.find_spec(\"torchvision\") is not None\n\n\ndef is_rich_available() -> bool:\n    return importlib.util.find_spec(\"rich\") is not None\n\n\ndef is_wandb_available() -> bool:\n    return importlib.util.find_spec(\"wandb\") is not None\n\n\ndef is_xpu_available() -> bool:\n    if is_accelerate_greater_20_0():\n        import accelerate\n\n        return accelerate.utils.is_xpu_available()\n    else:\n        if importlib.util.find_spec(\"intel_extension_for_pytorch\") is None:\n            return False\n        try:\n            import torch\n\n            return hasattr(torch, \"xpu\") and torch.xpu.is_available()\n        except RuntimeError:\n            return False\n\n\ndef is_npu_available() -> bool:\n    \"\"\"Checks if `torch_npu` is installed and potentially if a NPU is in the environment\"\"\"\n    if importlib.util.find_spec(\"torch\") is None or importlib.util.find_spec(\"torch_npu\") is None:\n        return False\n\n    import torch\n    import torch_npu  # noqa: F401\n\n    return hasattr(torch, \"npu\") and torch.npu.is_available()\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/trainer/base.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\n\nfrom huggingface_hub import PyTorchModelHubMixin\n\n\nclass BaseTrainer(PyTorchModelHubMixin):\n    r\"\"\"\n    Base class for all trainers - this base class implements the basic functions that we\n    need for a trainer.\n\n    The trainer needs to have the following functions:\n        - step: takes in a batch of data and performs a step of training\n        - loss: takes in a batch of data and returns the loss\n        - compute_rewards: takes in a batch of data and returns the rewards\n        - _build_models_and_tokenizer: builds the models and tokenizer\n        - _build_dataset: builds the dataset\n    Each user is expected to implement their own trainer class that inherits from this base\n    if they want to use a new training algorithm.\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n\n    def step(self, *args):\n        raise NotImplementedError(\"Not implemented\")\n\n    def loss(self, *args):\n        raise NotImplementedError(\"Not implemented\")\n\n    def compute_rewards(self, *args):\n        raise NotImplementedError(\"Not implemented\")\n\n    def _save_pretrained(self, save_directory):\n        raise NotImplementedError(\"Not implemented\")\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/trainer/ppo_config.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\nimport json\nimport os\nimport sys\nimport warnings\nfrom dataclasses import dataclass, field\nfrom typing import Literal, Optional\n\nimport numpy as np\nimport tyro\nfrom typing_extensions import Annotated\n\nfrom trl.trainer.utils import exact_div\n\nfrom ..core import flatten_dict\nfrom ..import_utils import is_wandb_available\n\n\nJSONDict = Annotated[Optional[dict], tyro.conf.arg(metavar=\"JSON\", constructor=json.loads)]\n\n\n@dataclass\nclass PPOConfig:\n    \"\"\"\n    Configuration class for PPOTrainer\n    \"\"\"\n\n    # common parameters\n    exp_name: str = os.path.basename(sys.argv[0])[: -len(\".py\")]\n    \"\"\"the name of this experiment (by default is the file name without the extension name)\"\"\"\n    seed: int = 0\n    \"\"\"Seed value for random generations\"\"\"\n    log_with: Optional[Literal[\"wandb\", \"tensorboard\"]] = None\n    \"\"\"Log with either 'wandb' or 'tensorboard', check  https://huggingface.co/docs/accelerate/usage_guides/tracking for more details\"\"\"\n    task_name: Optional[str] = None\n    \"\"\"Name of task to use - used only for tracking purposes\"\"\"\n    model_name: Optional[str] = None\n    \"\"\"Name of model to use - used only for tracking purposes\"\"\"\n    query_dataset: Optional[str] = None\n    \"\"\"Name of dataset to query - used only for tracking purposes\"\"\"\n    reward_model: Optional[str] = None\n    \"\"\"The reward model to use - used only for tracking purposes\"\"\"\n    remove_unused_columns: bool = True\n    \"\"\"Remove unused columns from the dataset if `datasets.Dataset` is used\"\"\"\n    tracker_kwargs: JSONDict = field(default_factory=dict)\n    \"\"\"Keyword arguments for the tracker (e.g. python ppo.py --ppo_config.tracker_kwargs='{\"wandb\": {\"entity\": \"my_wandb_entity\", \"name\": \"my_exp_name\"}}'\"\"\"\n    accelerator_kwargs: JSONDict = field(default_factory=dict)\n    \"\"\"Keyword arguments for the accelerator\"\"\"\n    project_kwargs: JSONDict = field(default_factory=dict)\n    \"\"\"Keyword arguments for the accelerator project config (e.g. `logging_dir`)\"\"\"\n    tracker_project_name: str = \"trl\"\n    \"\"\"Name of project to use for tracking\"\"\"\n    push_to_hub_if_best_kwargs: JSONDict = field(default_factory=dict)\n    \"\"\"Keyword arguments for pushing model to the hub during training (e.g. repo_id)\"\"\"\n\n    # hyperparameters\n    steps: int = 20000\n    \"\"\"Number of training steps\"\"\"\n    learning_rate: float = 1e-5\n    \"\"\"Adam learning rate\"\"\"\n    adap_kl_ctrl: bool = True\n    \"\"\"Use adaptive KL control, otherwise linear\"\"\"\n    init_kl_coef: Optional[float] = 0.2\n    \"\"\"Initial KL penalty coefficient (used for adaptive and linear control)\"\"\"\n    kl_penalty: Literal[\"kl\", \"abs\", \"mse\", \"full\"] = \"kl\"\n    \"\"\"kl penalty options: 'kl': model_logp - ref_logp,  'abs': abs(kl),  'mse': mean squared error mse(kl) and 'full': the actual kl for all tokens in the distribution\"\"\"\n    target: Optional[float] = 6\n    \"\"\"Target KL value for adaptive KL control\"\"\"\n    horizon: Optional[float] = 10000\n    \"\"\"Horizon for adaptive KL control\"\"\"\n    gamma: float = 1\n    \"\"\"Gamma parameter for advantage calculation\"\"\"\n    lam: float = 0.95\n    \"\"\"Lambda parameter for advantage calculation\"\"\"\n    cliprange: float = 0.2\n    \"\"\"Range for clipping in PPO policy gradient loss\"\"\"\n    cliprange_value: float = 0.2\n    \"\"\"Range for clipping values in loss calculation\"\"\"\n    vf_coef: float = 0.1\n    \"\"\"Scaling factor for value loss\"\"\"\n    batch_size: int = 256\n    \"\"\"Number of samples per optimisation step\"\"\"\n    forward_batch_size: Optional[int] = None\n    \"\"\"DEPRECATED: use `mini_batch_size` instead, which does the same thing.\"\"\"\n    mini_batch_size: int = 1\n    \"\"\"Number of samples optimized in each mini batch\"\"\"\n    gradient_accumulation_steps: int = 1\n    \"\"\"The number of gradient accumulation steps\"\"\"\n    world_size: tyro.conf.Suppress[int] = None\n    \"\"\"The world size for distributed training\"\"\"\n    ppo_epochs: int = 4\n    \"\"\"Number of optimisation epochs per batch of samples\"\"\"\n    max_grad_norm: Optional[float] = None\n    \"\"\"Maximum gradient norm for gradient clipping\"\"\"\n    optimize_cuda_cache: Optional[bool] = None\n    \"\"\"DEPRECATED: use `optimize_device_cache` instead, which does the same thing.\"\"\"\n    optimize_device_cache: Optional[bool] = False\n    \"\"\"Optimize device cache for slightly more memory-efficient training\"\"\"\n    early_stopping: bool = False\n    \"\"\"Whether to stop the PPO optimization loop early is the KL too high\"\"\"\n    target_kl: float = 1\n    \"\"\"Stop early if we exceed this value by over 50%\"\"\"\n    compare_steps: int = 1\n    \"\"\"Number of steps between comparison of the current reward with the best seen so far\"\"\"\n    ratio_threshold: float = 10.0\n    \"\"\"Skip mini-batches with high PPO ratios that can cause loss spikes\"\"\"\n    use_score_scaling: bool = False\n    \"\"\"Use score scaling\"\"\"\n    use_score_norm: bool = False\n    \"\"\"Use score normalization. Only applicable if use_score_scaling is True\"\"\"\n    score_clip: Optional[float] = None\n    \"\"\"Score clipping\"\"\"\n    whiten_rewards: bool = False\n    \"\"\"Whiten the rewards before compute advantages\"\"\"\n\n    # computed hyperparameters at runtime; we use `tyro.conf.Suppress` to hide them from the help text\n    is_encoder_decoder: Optional[tyro.conf.Suppress[bool]] = None\n    \"\"\"TO BE FILLED In RUNTIME: Whether the model is an encoder-decoder model\"\"\"\n    is_peft_model: Optional[tyro.conf.Suppress[bool]] = None\n    \"\"\"TO BE FILLED In RUNTIME: Whether the model is a PEFT model\"\"\"\n    backward_batch_size: tyro.conf.Suppress[int] = None\n    \"\"\"TO BE FILLED In RUNTIME: Number of samples optimized in an `optimizer.step()` call\"\"\"\n    global_backward_batch_size: tyro.conf.Suppress[int] = None\n    \"\"\"TO BE FILLED In RUNTIME: the effective `backward_batch_size` across all processes\"\"\"\n    global_batch_size: tyro.conf.Suppress[int] = None\n    \"\"\"TO BE FILLED In RUNTIME: the effective `batch_size` across all processes\"\"\"\n\n    if optimize_cuda_cache is not None:\n        warnings.warn(\n            \"The `optimize_cuda_cache` argument will be deprecated soon, please use `optimize_device_cache` instead.\"\n        )\n        optimize_device_cache = optimize_cuda_cache\n    else:\n        optimize_device_cache = False\n\n    def __post_init__(self):\n        if self.forward_batch_size is not None:\n            warnings.warn(\n                \"Note that using `forward_batch_size` is deprecated, use `mini_batch_size` instead. By setting it you overwrite `mini_batch_size` which affects both the batch size during forward passes and also the mini batch size for PPO optimization.\"\n            )\n            self.mini_batch_size = self.forward_batch_size\n\n        self.backward_batch_size = self.mini_batch_size * self.gradient_accumulation_steps\n        exact_div(\n            self.batch_size,\n            self.backward_batch_size,\n            \"`batch_size`\",\n            \"`mini_batch_size * gradient_accumulation_steps`\",\n            \"`batch_size` must be a multiple of `mini_batch_size * gradient_accumulation_steps`\",\n        )\n\n        # check if wandb is installed\n        if self.log_with == \"wandb\":\n            # raise error if wandb is not installed\n            if not is_wandb_available():\n                raise ImportError(\n                    \"Please install wandb to use wandb logging. You can do this by running `pip install wandb`.\"\n                )\n\n        self.total_ppo_epochs = int(np.ceil(self.steps / self.batch_size))\n        assert self.kl_penalty in [\"kl\", \"abs\", \"mse\", \"full\"]\n\n    def to_dict(self):\n        output_dict = {}\n        for key, value in self.__dict__.items():\n            output_dict[key] = value\n        return flatten_dict(output_dict)\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/trainer/ddpo_trainer.py",
        "recommendation": "The code file is too long to analyze. Please select a shorter file.",
        "code_snippet": "import os\nimport warnings\nfrom collections import defaultdict\nfrom concurrent import futures\nfrom typing import Any, Callable, Optional, Tuple\nfrom warnings import warn\n\nimport torch\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import ProjectConfiguration, set_seed\nfrom huggingface_hub import whoami\n\nfrom ..models import DDPOStableDiffusionPipeline\nfrom . import BaseTrainer, DDPOConfig\nfrom .utils import PerPromptStatTracker\n\n\nlogger = get_logger(__name__)\n\n\nMODEL_CARD_TEMPLATE = \"\"\"---\nlicense: apache-2.0\ntags:\n- trl\n- ddpo\n- diffusers\n- reinforcement-learning\n- text-to-image\n- stable-diffusion\n---\n\n# {model_name}\n\nThis is a diffusion model that has been fine-tuned with reinforcement learning to\n guide the model outputs according to a value, function, or human feedback. The model can be used for image generation conditioned with text.\n\n\"\"\"\n\n\nclass DDPOTrainer(BaseTrainer):\n    \"\"\"\n    The DDPOTrainer uses Deep Diffusion Policy Optimization to optimise diffusion models.\n    Note, this trainer is heavily inspired by the work here: https://github.com/kvablack/ddpo-pytorch\n    As of now only Stable Diffusion based pipelines are supported\n\n    Attributes:\n        **config** (`DDPOConfig`) -- Configuration object for DDPOTrainer. Check the documentation of `PPOConfig` for more\n         details.\n        **reward_function** (Callable[[torch.Tensor, Tuple[str], Tuple[Any]], torch.Tensor]) -- Reward function to be used\n        **prompt_function** (Callable[[], Tuple[str, Any]]) -- Function to generate prompts to guide model\n        **sd_pipeline** (`DDPOStableDiffusionPipeline`) -- Stable Diffusion pipeline to be used for training.\n        **image_samples_hook** (Optional[Callable[[Any, Any, Any], Any]]) -- Hook to be called to log images\n    \"\"\"\n\n    _tag_names = [\"trl\", \"ddpo\"]\n\n    def __init__(\n        self,\n        config: DDPOConfig,\n        reward_function: Callable[[torch.Tensor, Tuple[str], Tuple[Any]], torch.Tensor],\n        prompt_function: Callable[[], Tuple[str, Any]],\n        sd_pipeline: DDPOStableDiffusionPipeline,\n        image_samples_hook: Optional[Callable[[Any, Any, Any], Any]] = None,\n    ):\n        if image_samples_hook is None:\n            warn(\"No image_samples_hook provided; no images will be logged\")\n\n        self.prompt_fn = prompt_function\n        self.reward_fn = reward_function\n        self.config = config\n        self.image_samples_callback = image_samples_hook\n\n        accelerator_project_config = ProjectConfiguration(**self.config.project_kwargs)\n\n        if self.config.resume_from:\n            self.config.resume_from = os.path.normpath(os.path.expanduser(self.config.resume_from))\n            if \"checkpoint_\" not in os.path.basename(self.config.resume_from):\n                # get the most recent checkpoint in this directory\n                checkpoints = list(\n                    filter(\n                        lambda x: \"checkpoint_\" in x,\n                        os.listdir(self.config.resume_from),\n                    )\n                )\n                if len(checkpoints) == 0:\n                    raise ValueError(f\"No checkpoints found in {self.config.resume_from}\")\n                checkpoint_numbers = sorted([int(x.split(\"_\")[-1]) for x in checkpoints])\n                self.config.resume_from = os.path.join(\n                    self.config.resume_from,\n                    f\"checkpoint_{checkpoint_numbers[-1]}\",\n                )\n\n                accelerator_project_config.iteration = checkpoint_numbers[-1] + 1\n\n        # number of timesteps within each trajectory to train on\n        self.num_train_timesteps = int(self.config.sample_num_steps * self.config.train_timestep_fraction)\n\n        self.accelerator = Accelerator(\n            log_with=self.config.log_with,\n            mixed_precision=self.config.mixed_precision,\n            project_config=accelerator_project_config,\n            # we always accumulate gradients across timesteps; we want config.train.gradient_accumulation_steps to be the\n            # number of *samples* we accumulate across, so we need to multiply by the number of training timesteps to get\n            # the total number of optimizer steps to accumulate across.\n            gradient_accumulation_steps=self.config.train_gradient_accumulation_steps * self.num_train_timesteps,\n            **self.config.accelerator_kwargs,\n        )\n\n        is_okay, message = self._config_check()\n        if not is_okay:\n            raise ValueError(message)\n\n        is_using_tensorboard = config.log_with is not None and config.log_with == \"tensorboard\"\n\n        if self.accelerator.is_main_process:\n            self.accelerator.init_trackers(\n                self.config.tracker_project_name,\n                config=dict(ddpo_trainer_config=config.to_dict()) if not is_using_tensorboard else config.to_dict(),\n                init_kwargs=self.config.tracker_kwargs,\n            )\n\n        logger.info(f\"\\n{config}\")\n\n        set_seed(self.config.seed, device_specific=True)\n\n        self.sd_pipeline = sd_pipeline\n\n        self.sd_pipeline.set_progress_bar_config(\n            position=1,\n            disable=not self.accelerator.is_local_main_process,\n            leave=False,\n            desc=\"Timestep\",\n            dynamic_ncols=True,\n        )\n\n        # For mixed precision training we cast all non-trainable weights (vae, non-lora text_encoder and non-lora unet) to half-precision\n        # as these weights are only used for inference, keeping weights in full precision is not required.\n        if self.accelerator.mixed_precision == \"fp16\":\n            inference_dtype = torch.float16\n        elif self.accelerator.mixed_precision == \"bf16\":\n            inference_dtype = torch.bfloat16\n        else:\n            inference_dtype = torch.float32\n\n        self.sd_pipeline.vae.to(self.accelerator.device, dtype=inference_dtype)\n        self.sd_pipeline.text_encoder.to(self.accelerator.device, dtype=inference_dtype)\n        self.sd_pipeline.unet.to(self.accelerator.device, dtype=inference_dtype)\n\n        trainable_layers = self.sd_pipeline.get_trainable_layers()\n\n        self.accelerator.register_save_state_pre_hook(self._save_model_hook)\n        self.accelerator.register_load_state_pre_hook(self._load_model_hook)\n\n        # Enable TF32 for faster training on Ampere GPUs,\n        # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n        if self.config.allow_tf32:\n            torch.backends.cuda.matmul.allow_tf32 = True\n\n        self.optimizer = self._setup_optimizer(\n            trainable_layers.parameters() if not isinstance(trainable_layers, list) else trainable_layers\n        )\n\n        self.neg_prompt_embed = self.sd_pipeline.text_encoder(\n            self.sd_pipeline.tokenizer(\n                [\"\"] if self.config.negative_prompts is None else self.config.negative_prompts,\n                return_tensors=\"pt\",\n                padding=\"max_length\",\n                truncation=True,\n                max_length=self.sd_pipeline.tokenizer.model_max_length,\n            ).input_ids.to(self.accelerator.device)\n        )[0]\n\n        if config.per_prompt_stat_tracking:\n            self.stat_tracker = PerPromptStatTracker(\n                config.per_prompt_stat_tracking_buffer_size,\n                config.per_prompt_stat_tracking_min_count,\n            )\n\n        # NOTE: for some reason, autocast is necessary for non-lora training but for lora training it isn't necessary and it uses\n        # more memory\n        self.autocast = self.sd_pipeline.autocast or self.accelerator.autocast\n\n        if hasattr(self.sd_pipeline, \"use_lora\") and self.sd_pipeline.use_lora:\n            unet, self.optimizer = self.accelerator.prepare(trainable_layers, self.optimizer)\n            self.trainable_layers = list(filter(lambda p: p.requires_grad, unet.parameters()))\n        else:\n            self.trainable_layers, self.optimizer = self.accelerator.prepare(trainable_layers, self.optimizer)\n\n        if self.config.async_reward_computation:\n            self.executor = futures.ThreadPoolExecutor(max_workers=config.max_workers)\n\n        if config.resume_from:\n            logger.info(f\"Resuming from {config.resume_from}\")\n            self.accelerator.load_state(config.resume_from)\n            self.first_epoch = int(config.resume_from.split(\"_\")[-1]) + 1\n        else:\n            self.first_epoch = 0\n\n    def compute_rewards(self, prompt_image_pairs, is_async=False):\n        if not is_async:\n            rewards = []\n            for images, prompts, prompt_metadata in prompt_image_pairs:\n                reward, reward_metadata = self.reward_fn(images, prompts, prompt_metadata)\n                rewards.append(\n                    (\n                        torch.as_tensor(reward, device=self.accelerator.device),\n                        reward_metadata,\n                    )\n                )\n        else:\n            rewards = self.executor.map(lambda x: self.reward_fn(*x), prompt_image_pairs)\n            rewards = [\n                (torch.as_tensor(reward.result(), device=self.accelerator.device), reward_metadata.result())\n                for reward, reward_metadata in rewards\n            ]\n\n        return zip(*rewards)\n\n    def step(self, epoch: int, global_step: int):\n        \"\"\"\n        Perform a single step of training.\n\n        Args:\n            epoch (int): The current epoch.\n            global_step (int): The current global step.\n\n        Side Effects:\n            - Model weights are updated\n            - Logs the statistics to the accelerator trackers.\n            - If `self.image_samples_callback` is not None, it will be called with the prompt_image_pairs, global_step, and the accelerator tracker.\n\n        Returns:\n            global_step (int): The updated global step.\n\n        \"\"\"\n        samples, prompt_image_data = self._generate_samples(\n            iterations=self.config.sample_num_batches_per_epoch,\n            batch_size=self.config.sample_batch_size,\n        )\n\n        # collate samples into dict where each entry has shape (num_batches_per_epoch * sample.batch_size, ...)\n        samples = {k: torch.cat([s[k] for s in samples]) for k in samples[0].keys()}\n        rewards, rewards_metadata = self.compute_rewards(\n            prompt_image_data, is_async=self.config.async_reward_computation\n        )\n\n        for i, image_data in enumerate(prompt_image_data):\n            image_data.extend([rewards[i], rewards_metadata[i]])\n\n        if self.image_samples_callback is not None:\n            self.image_samples_callback(prompt_image_data, global_step, self.accelerator.trackers[0])\n\n        rewards = torch.cat(rewards)\n        rewards = self.accelerator.gather(rewards).cpu().numpy()\n\n        self.accelerator.log(\n            {\n                \"reward\": rewards,\n                \"epoch\": epoch,\n                \"reward_mean\": rewards.mean(),\n                \"reward_std\": rewards.std(),\n            },\n            step=global_step,\n        )\n\n        if self.config.per_prompt_stat_tracking:\n            # gather the prompts across processes\n            prompt_ids = self.accelerator.gather(samples[\"prompt_ids\"]).cpu().numpy()\n            prompts = self.sd_pipeline.tokenizer.batch_decode(prompt_ids, skip_special_tokens=True)\n            advantages = self.stat_tracker.update(prompts, rewards)\n        else:\n            advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n\n        # ungather advantages;  keep the entries corresponding to the samples on this process\n        samples[\"advantages\"] = (\n            torch.as_tensor(advantages)\n            .reshape(self.accelerator.num_processes, -1)[self.accelerator.process_index]\n            .to(self.accelerator.device)\n        )\n\n        del samples[\"prompt_ids\"]\n\n        total_batch_size, num_timesteps = samples[\"timesteps\"].shape\n\n        for inner_epoch in range(self.config.train_num_inner_epochs):\n            # shuffle samples along batch dimension\n            perm = torch.randperm(total_batch_size, device=self.accelerator.device)\n            samples = {k: v[perm] for k, v in samples.items()}\n\n            # shuffle along time dimension independently for each sample\n            # still trying to understand the code below\n            perms = torch.stack(\n                [torch.randperm(num_timesteps, device=self.accelerator.device) for _ in range(total_batch_size)]\n            )\n\n            for key in [\"timesteps\", \"latents\", \"next_latents\", \"log_probs\"]:\n                samples[key] = samples[key][\n                    torch.arange(total_batch_size, device=self.accelerator.device)[:, None],\n                    perms,\n                ]\n\n            original_keys = samples.keys()\n            original_values = samples.values()\n            # rebatch them as user defined train_batch_size is different from sample_batch_size\n            reshaped_values = [v.reshape(-1, self.config.train_batch_size, *v.shape[1:]) for v in original_values]\n\n            # Transpose the list of original values\n            transposed_values = zip(*reshaped_values)\n            # Create new dictionaries for each row of transposed values\n            samples_batched = [dict(zip(original_keys, row_values)) for row_values in transposed_values]\n\n            self.sd_pipeline.unet.train()\n            global_step = self._train_batched_samples(inner_epoch, epoch, global_step, samples_batched)\n            # ensure optimization step at the end of the inner epoch\n            if not self.accelerator.sync_gradients:\n                raise ValueError(\n                    \"Optimization step should have been performed by this point. Please check calculated gradient accumulation settings.\"\n                )\n\n        if epoch != 0 and epoch % self.config.save_freq == 0 and self.accelerator.is_main_process:\n            self.accelerator.save_state()\n\n        return global_step\n\n    def calculate_loss(self, latents, timesteps, next_latents, log_probs, advantages, embeds):\n        \"\"\"\n        Calculate the loss for a batch of an unpacked sample\n\n        Args:\n            latents (torch.Tensor):\n                The latents sampled from the diffusion model, shape: [batch_size, num_channels_latents, height, width]\n            timesteps (torch.Tensor):\n                The timesteps sampled from the diffusion model, shape: [batch_size]\n            next_latents (torch.Tensor):\n                The next latents sampled from the diffusion model, shape: [batch_size, num_channels_latents, height, width]\n            log_probs (torch.Tensor):\n                The log probabilities of the latents, shape: [batch_size]\n            advantages (torch.Tensor):\n                The advantages of the latents, shape: [batch_size]\n            embeds (torch.Tensor):\n                The embeddings of the prompts, shape: [2*batch_size or batch_size, ...]\n                Note: the \"or\" is because if train_cfg is True, the expectation is that negative prompts are concatenated to the embeds\n\n        Returns:\n            loss (torch.Tensor), approx_kl (torch.Tensor), clipfrac (torch.Tensor)\n            (all of these are of shape (1,))\n        \"\"\"\n        with self.autocast():\n            if self.config.train_cfg:\n                noise_pred = self.sd_pipeline.unet(\n                    torch.cat([latents] * 2),\n                    torch.cat([timesteps] * 2),\n                    embeds,\n                ).sample\n                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + self.config.sample_guidance_scale * (\n                    noise_pred_text - noise_pred_uncond\n                )\n            else:\n                noise_pred = self.sd_pipeline.unet(\n                    latents,\n                    timesteps,\n                    embeds,\n                ).sample\n            # compute the log prob of next_latents given latents under the current model\n\n            scheduler_step_output = self.sd_pipeline.scheduler_step(\n                noise_pred,\n                timesteps,\n                latents,\n                eta=self.config.sample_eta,\n                prev_sample=next_latents,\n            )\n\n            log_prob = scheduler_step_output.log_probs\n\n        advantages = torch.clamp(\n            advantages,\n            -self.config.train_adv_clip_max,\n            self.config.train_adv_clip_max,\n        )\n\n        ratio = torch.exp(log_prob - log_probs)\n\n        loss = self.loss(advantages, self.config.train_clip_range, ratio)\n\n        approx_kl = 0.5 * torch.mean((log_prob - log_probs) ** 2)\n\n        clipfrac = torch.mean((torch.abs(ratio - 1.0) > self.config.train_clip_range).float())\n\n        return loss, approx_kl, clipfrac\n\n    def loss(\n        self,\n        advantages: torch.Tensor,\n        clip_range: float,\n        ratio: torch.Tensor,\n    ):\n        unclipped_loss = -advantages * ratio\n        clipped_loss = -advantages * torch.clamp(\n            ratio,\n            1.0 - clip_range,\n            1.0 + clip_range,\n        )\n        return torch.mean(torch.maximum(unclipped_loss, clipped_loss))\n\n    def _setup_optimizer(self, trainable_layers_parameters):\n        if self.config.train_use_8bit_adam:\n            import bitsandbytes\n\n            optimizer_cls = bitsandbytes.optim.AdamW8bit\n        else:\n            optimizer_cls = torch.optim.AdamW\n\n        return optimizer_cls(\n            trainable_layers_parameters,\n            lr=self.config.train_learning_rate,\n            betas=(self.config.train_adam_beta1, self.config.train_adam_beta2),\n            weight_decay=self.config.train_adam_weight_decay,\n            eps=self.config.train_adam_epsilon,\n        )\n\n    def _save_model_hook(self, models, weights, output_dir):\n        self.sd_pipeline.save_checkpoint(models, weights, output_dir)\n        weights.pop()  # ensures that accelerate doesn't try to handle saving of the model\n\n    def _load_model_hook(self, models, input_dir):\n        self.sd_pipeline.load_checkpoint(models, input_dir)\n        models.pop()  # ensures that accelerate doesn't try to handle loading of the model\n\n    def _generate_samples(self, iterations, batch_size):\n        \"\"\"\n        Generate samples from the model\n\n        Args:\n            iterations (int): Number of iterations to generate samples for\n            batch_size (int): Batch size to use for sampling\n\n        Returns:\n            samples (List[Dict[str, torch.Tensor]]), prompt_image_pairs (List[List[Any]])\n        \"\"\"\n        samples = []\n        prompt_image_pairs = []\n        self.sd_pipeline.unet.eval()\n\n        sample_neg_prompt_embeds = self.neg_prompt_embed.repeat(batch_size, 1, 1)\n\n        for _ in range(iterations):\n            prompts, prompt_metadata = zip(*[self.prompt_fn() for _ in range(batch_size)])\n\n            prompt_ids = self.sd_pipeline.tokenizer(\n                prompts,\n                return_tensors=\"pt\",\n                padding=\"max_length\",\n                truncation=True,\n                max_length=self.sd_pipeline.tokenizer.model_max_length,\n            ).input_ids.to(self.accelerator.device)\n            prompt_embeds = self.sd_pipeline.text_encoder(prompt_ids)[0]\n\n            with self.autocast():\n                sd_output = self.sd_pipeline(\n                    prompt_embeds=prompt_embeds,\n                    negative_prompt_embeds=sample_neg_prompt_embeds,\n                    num_inference_steps=self.config.sample_num_steps,\n                    guidance_scale=self.config.sample_guidance_scale,\n                    eta=self.config.sample_eta,\n                    output_type=\"pt\",\n                )\n\n                images = sd_output.images\n                latents = sd_output.latents\n                log_probs = sd_output.log_probs\n\n            latents = torch.stack(latents, dim=1)  # (batch_size, num_steps + 1, ...)\n            log_probs = torch.stack(log_probs, dim=1)  # (batch_size, num_steps, 1)\n            timesteps = self.sd_pipeline.scheduler.timesteps.repeat(batch_size, 1)  # (batch_size, num_steps)\n\n            samples.append(\n                {\n                    \"prompt_ids\": prompt_ids,\n                    \"prompt_embeds\": prompt_embeds,\n                    \"timesteps\": timesteps,\n                    \"latents\": latents[:, :-1],  # each entry is the latent before timestep t\n                    \"next_latents\": latents[:, 1:],  # each entry is the latent after timestep t\n                    \"log_probs\": log_probs,\n                    \"negative_prompt_embeds\": sample_neg_prompt_embeds,\n                }\n            )\n            prompt_image_pairs.append([images, prompts, prompt_metadata])\n\n        return samples, prompt_image_pairs\n\n    def _train_batched_samples(self, inner_epoch, epoch, global_step, batched_samples):\n        \"\"\"\n        Train on a batch of samples. Main training segment\n\n        Args:\n            inner_epoch (int): The current inner epoch\n            epoch (int): The current epoch\n            global_step (int): The current global step\n            batched_samples (List[Dict[str, torch.Tensor]]): The batched samples to train on\n\n        Side Effects:\n            - Model weights are updated\n            - Logs the statistics to the accelerator trackers.\n\n        Returns:\n            global_step (int): The updated global step\n        \"\"\"\n        info = defaultdict(list)\n        for i, sample in enumerate(batched_samples):\n            if self.config.train_cfg:\n                # concat negative prompts to sample prompts to avoid two forward passes\n                embeds = torch.cat([sample[\"negative_prompt_embeds\"], sample[\"prompt_embeds\"]])\n            else:\n                embeds = sample[\"prompt_embeds\"]\n\n            for j in range(self.num_train_timesteps):\n                with self.accelerator.accumulate(self.sd_pipeline.unet):\n                    loss, approx_kl, clipfrac = self.calculate_loss(\n                        sample[\"latents\"][:, j],\n                        sample[\"timesteps\"][:, j],\n                        sample[\"next_latents\"][:, j],\n                        sample[\"log_probs\"][:, j],\n                        sample[\"advantages\"],\n                        embeds,\n                    )\n                    info[\"approx_kl\"].append(approx_kl)\n                    info[\"clipfrac\"].append(clipfrac)\n                    info[\"loss\"].append(loss)\n\n                    self.accelerator.backward(loss)\n                    if self.accelerator.sync_gradients:\n                        self.accelerator.clip_grad_norm_(\n                            self.trainable_layers.parameters()\n                            if not isinstance(self.trainable_layers, list)\n                            else self.trainable_layers,\n                            self.config.train_max_grad_norm,\n                        )\n                    self.optimizer.step()\n                    self.optimizer.zero_grad()\n\n                # Checks if the accelerator has performed an optimization step behind the scenes\n                if self.accelerator.sync_gradients:\n                    # log training-related stuff\n                    info = {k: torch.mean(torch.stack(v)) for k, v in info.items()}\n                    info = self.accelerator.reduce(info, reduction=\"mean\")\n                    info.update({\"epoch\": epoch, \"inner_epoch\": inner_epoch})\n                    self.accelerator.log(info, step=global_step)\n                    global_step += 1\n                    info = defaultdict(list)\n        return global_step\n\n    def _config_check(self) -> Tuple[bool, str]:\n        samples_per_epoch = (\n            self.config.sample_batch_size * self.accelerator.num_processes * self.config.sample_num_batches_per_epoch\n        )\n        total_train_batch_size = (\n            self.config.train_batch_size\n            * self.accelerator.num_processes\n            * self.config.train_gradient_accumulation_steps\n        )\n\n        if not self.config.sample_batch_size >= self.config.train_batch_size:\n            return (\n                False,\n                f\"Sample batch size ({self.config.sample_batch_size}) must be greater than or equal to the train batch size ({self.config.train_batch_size})\",\n            )\n        if not self.config.sample_batch_size % self.config.train_batch_size == 0:\n            return (\n                False,\n                f\"Sample batch size ({self.config.sample_batch_size}) must be divisible by the train batch size ({self.config.train_batch_size})\",\n            )\n        if not samples_per_epoch % total_train_batch_size == 0:\n            return (\n                False,\n                f\"Number of samples per epoch ({samples_per_epoch}) must be divisible by the total train batch size ({total_train_batch_size})\",\n            )\n        return True, \"\"\n\n    def train(self, epochs: Optional[int] = None):\n        \"\"\"\n        Train the model for a given number of epochs\n        \"\"\"\n        global_step = 0\n        if epochs is None:\n            epochs = self.config.num_epochs\n        for epoch in range(self.first_epoch, epochs):\n            global_step = self.step(epoch, global_step)\n\n    def create_model_card(self, path: str, model_name: Optional[str] = \"TRL DDPO Model\") -> None:\n        \"\"\"Creates and saves a model card for a TRL model.\n\n        Args:\n            path (`str`): The path to save the model card to.\n            model_name (`str`, *optional*): The name of the model, defaults to `TRL DDPO Model`.\n        \"\"\"\n        try:\n            user = whoami()[\"name\"]\n        # handle the offline case\n        except:  # noqa\n            warnings.warn(\"Cannot retrieve user information assuming you are running in offline mode.\")\n            return\n\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        model_card_content = MODEL_CARD_TEMPLATE.format(model_name=model_name, model_id=f\"{user}/{path}\")\n        with open(os.path.join(path, \"README.md\"), \"w\", encoding=\"utf-8\") as f:\n            f.write(model_card_content)\n\n    def _save_pretrained(self, save_directory):\n        self.sd_pipeline.save_pretrained(save_directory)\n        self.create_model_card(save_directory)\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/trainer/utils.py",
        "recommendation": "The code file is too long to analyze. Please select a shorter file.",
        "code_snippet": "\nimport random\nimport warnings\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import IterableDataset\nfrom transformers import DataCollatorForLanguageModeling, PreTrainedTokenizerBase\n\nfrom ..import_utils import is_unsloth_available\n\n\nclass AdaptiveKLController:\n    \"\"\"\n    Adaptive KL controller described in the paper:\n    https://arxiv.org/pdf/1909.08593.pdf\n    \"\"\"\n\n    def __init__(self, init_kl_coef, target, horizon):\n        self.value = init_kl_coef\n        self.target = target\n        self.horizon = horizon\n\n    def update(self, current, n_steps):\n        target = self.target\n        proportional_error = np.clip(current / target - 1, -0.2, 0.2)\n        mult = 1 + proportional_error * n_steps / self.horizon\n        self.value *= mult\n\n\nclass FixedKLController:\n    \"\"\"Fixed KL controller.\"\"\"\n\n    def __init__(self, kl_coef):\n        self.value = kl_coef\n\n    def update(self, current, n_steps):\n        pass\n\n\nclass DataCollatorForCompletionOnlyLM(DataCollatorForLanguageModeling):\n    \"\"\"\n    Data collator used for completion tasks. It ensures that all the tokens of the labels are set to an 'ignore_index'\n    when they do not come from the assistant. This ensure that the loss is only\n    calculated on the completion made by the assistant.\n\n    Args:\n        response_template (`Union[str, List[int]]`): the template form that indicates the start of the response, typically something like\n            '### Response:\\n'. It can also be passed as tokenized ids, which can be useful when using a tokenizer that encodes the response\n            differently if it does not have proper context.\n        instruction_template (`Union[str, List[int]]`): the template form that indicates the start of the human instruction, typically something like\n            '### Human:\\n'. Useful for assistant-style conversation datasets. It can also be passed as tokenized ids.\n        mlm (`bool`, *optional*, defaults to `False`): Whether or not to use masked language modeling in the underlying\n            `DataCollatorForLanguageModeling` class. Note that this option currently has no effect but is present\n             for flexibility and backwards-compatibility.\n        ignore_index (`int`, *optional*, defaults to `-100`):\n            The index to use to ignore the initial tokens with\n    \"\"\"\n\n    def __init__(\n        self,\n        response_template: Union[str, List[int]],\n        instruction_template: Union[str, List[int]] = None,\n        *args,\n        mlm: bool = False,\n        ignore_index: int = -100,\n        **kwargs,\n    ):\n        super().__init__(*args, mlm=mlm, **kwargs)\n\n        self.instruction_template = instruction_template\n        if isinstance(instruction_template, str):\n            # The user provides a string, must tokenize\n            self.instruction_token_ids = self.tokenizer.encode(self.instruction_template, add_special_tokens=False)\n        else:\n            # The user already provides the token ids\n            self.instruction_token_ids = instruction_template\n\n        self.response_template = response_template\n        if isinstance(response_template, str):\n            # The user provides a string, must tokenize\n            self.response_token_ids = self.tokenizer.encode(self.response_template, add_special_tokens=False)\n        else:\n            # The user already provides the token ids\n            self.response_token_ids = response_template\n\n        if not self.mlm and self.instruction_template and self.tokenizer.pad_token_id == self.tokenizer.eos_token_id:\n            warnings.warn(\n                \"The pad_token_id and eos_token_id values of this tokenizer are identical. \"\n                \"If you are planning for multi-turn training, \"\n                \"it can result in the model continuously generating questions and answers without eos token. \"\n                \"To avoid this, set the pad_token_id to a different value.\"\n            )\n\n        self.ignore_index = ignore_index\n\n    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n        batch = super().torch_call(examples)\n\n        if self.instruction_template is None:\n            for i in range(len(examples)):\n                response_token_ids_start_idx = None\n\n                for idx in np.where(batch[\"labels\"][i] == self.response_token_ids[0])[0]:\n                    # `response_token_ids` is `'### Response:\\n'`, here we are just making sure that the token IDs match\n                    if (\n                        self.response_token_ids\n                        == batch[\"labels\"][i][idx : idx + len(self.response_token_ids)].tolist()\n                    ):\n                        response_token_ids_start_idx = idx\n\n                if response_token_ids_start_idx is None:\n                    warnings.warn(\n                        f\"Could not find response key `{self.response_template}` in the \"\n                        f'following instance: {self.tokenizer.decode(batch[\"input_ids\"][i])} '\n                        f\"This instance will be ignored in loss calculation. \"\n                        f\"Note, if this happens often, consider increasing the `max_seq_length`.\"\n                    )\n                    batch[\"labels\"][i, :] = self.ignore_index\n                else:\n                    response_token_ids_end_idx = response_token_ids_start_idx + len(self.response_token_ids)\n\n                    # Make pytorch loss function ignore all tokens up through the end of the response key\n                    batch[\"labels\"][i, :response_token_ids_end_idx] = self.ignore_index\n\n        else:\n            for i in range(len(examples)):\n                response_token_ids_idxs = []\n                human_token_ids_idxs = []\n\n                for assistant_idx in np.where(batch[\"labels\"][i] == self.response_token_ids[0])[0]:\n                    # find the indexes of the start of a response.\n                    if (\n                        self.response_token_ids\n                        == batch[\"labels\"][i][assistant_idx : assistant_idx + len(self.response_token_ids)].tolist()\n                    ):\n                        response_token_ids_idxs.append(assistant_idx + len(self.response_token_ids))\n\n                if len(response_token_ids_idxs) == 0:\n                    warnings.warn(\n                        f\"Could not find response key `{self.response_template}` in the \"\n                        f'following instance: {self.tokenizer.decode(batch[\"input_ids\"][i])} '\n                        f\"This instance will be ignored in loss calculation. \"\n                        f\"Note, if this happens often, consider increasing the `max_seq_length`.\"\n                    )\n                    batch[\"labels\"][i, :] = self.ignore_index\n\n                human_token_ids = self.instruction_token_ids\n                for human_idx in np.where(batch[\"labels\"][i] == human_token_ids[0])[0]:\n                    # find the indexes of the start of a human answer.\n                    if human_token_ids == batch[\"labels\"][i][human_idx : human_idx + len(human_token_ids)].tolist():\n                        human_token_ids_idxs.append(human_idx)\n\n                if len(human_token_ids_idxs) == 0:\n                    warnings.warn(\n                        f\"Could not find instruction key `{self.instruction_template}` in the \"\n                        f'following instance: {self.tokenizer.decode(batch[\"input_ids\"][i])} '\n                        f\"This instance will be ignored in loss calculation. \"\n                        f\"Note, if this happens often, consider increasing the `max_seq_length`.\"\n                    )\n                    batch[\"labels\"][i, :] = self.ignore_index\n\n                if (\n                    len(human_token_ids_idxs) > 0\n                    and len(response_token_ids_idxs) > 0\n                    and human_token_ids_idxs[0] > response_token_ids_idxs[0]\n                ):\n                    human_token_ids_idxs = [0] + human_token_ids_idxs\n\n                for idx, (start, end) in enumerate(zip(human_token_ids_idxs, response_token_ids_idxs)):\n                    # Make pytorch loss function ignore all non response tokens\n                    if idx != 0:\n                        batch[\"labels\"][i, start:end] = self.ignore_index\n                    else:\n                        batch[\"labels\"][i, :end] = self.ignore_index\n\n                if len(response_token_ids_idxs) < len(human_token_ids_idxs):\n                    batch[\"labels\"][i, human_token_ids_idxs[-1] :] = self.ignore_index\n\n        return batch\n\n\n@dataclass\nclass RewardDataCollatorWithPadding:\n    r\"\"\"\n    Reward DataCollator class that pads the inputs to the maximum length of the batch.\n    Args:\n        tokenizer (`PreTrainedTokenizerBase`):\n            The tokenizer used for encoding the data.\n        padding (`Union[bool, str, `PaddingStrategy`]`, `optional`, defaults to `True`):\n            padding_strategy to pass to the tokenizer.\n        max_length (`Optional[int]`, `optional`, defaults to `None`):\n            The maximum length of the sequence to be processed.\n        pad_to_multiple_of (`Optional[int]`, `optional`, defaults to `None`):\n            If set will pad the sequence to a multiple of the provided value.\n        return_tensors (`str`, `optional`, defaults to `\"pt\"`):\n            The tensor type to use.\n    \"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    return_tensors: str = \"pt\"\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n        features_chosen = []\n        features_rejected = []\n        margin = []\n        # check if we have a margin. If we do, we need to batch it as well\n        has_margin = \"margin\" in features[0]\n        for feature in features:\n            # check if the keys are named as expected\n            if (\n                \"input_ids_chosen\" not in feature\n                or \"input_ids_rejected\" not in feature\n                or \"attention_mask_chosen\" not in feature\n                or \"attention_mask_rejected\" not in feature\n            ):\n                raise ValueError(\n                    \"The features should include `input_ids_chosen`, `attention_mask_chosen`, `input_ids_rejected` and `attention_mask_rejected`\"\n                )\n\n            features_chosen.append(\n                {\n                    \"input_ids\": feature[\"input_ids_chosen\"],\n                    \"attention_mask\": feature[\"attention_mask_chosen\"],\n                }\n            )\n            features_rejected.append(\n                {\n                    \"input_ids\": feature[\"input_ids_rejected\"],\n                    \"attention_mask\": feature[\"attention_mask_rejected\"],\n                }\n            )\n            if has_margin:\n                margin.append(feature[\"margin\"])\n        batch_chosen = self.tokenizer.pad(\n            features_chosen,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=self.return_tensors,\n        )\n        batch_rejected = self.tokenizer.pad(\n            features_rejected,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=self.return_tensors,\n        )\n        batch = {\n            \"input_ids_chosen\": batch_chosen[\"input_ids\"],\n            \"attention_mask_chosen\": batch_chosen[\"attention_mask\"],\n            \"input_ids_rejected\": batch_rejected[\"input_ids\"],\n            \"attention_mask_rejected\": batch_rejected[\"attention_mask\"],\n            \"return_loss\": True,\n        }\n        if has_margin:\n            margin = torch.tensor(margin, dtype=torch.float)\n            batch[\"margin\"] = margin\n        return batch\n\n\n@dataclass\nclass DPODataCollatorWithPadding:\n    r\"\"\"\n    DPO DataCollator class that pads the tokenized inputs to the maximum length of the batch.\n    Args:\n        pad_token_id (`int` defaults to 0):\n            The tokenizer's pad_token_id.\n        label_pad_token_id (`int`, defaults to -100):\n            The label used for masking.\n        is_encoder_decoder (`Optional[bool]`, `optional`, defaults to `None`):\n            Whether or not you model has an encoder_decoder architecture.\n    \"\"\"\n\n    pad_token_id: int = 0\n    label_pad_token_id: int = -100\n    is_encoder_decoder: Optional[bool] = False\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n        # first, pad everything to the same length\n        padded_batch = {}\n        for k in features[0].keys():\n            if k.endswith(\"_input_ids\") or k.endswith(\"_attention_mask\") or k.endswith(\"_labels\"):\n                if self.is_encoder_decoder:\n                    to_pad = [torch.LongTensor(ex[k]) for ex in features]\n\n                    if (k.startswith(\"prompt\")) and (k.endswith(\"input_ids\")):\n                        padding_value = self.pad_token_id\n                    elif k.endswith(\"_attention_mask\"):\n                        padding_value = 0\n                    elif (k.startswith(\"chosen\")) or (k.startswith(\"rejected\")) or (\"decoder\" in k):\n                        padding_value = self.label_pad_token_id\n                    else:\n                        raise ValueError(f\"Unexpected key in batch '{k}'\")\n                    padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value)\n                else:\n                    # adapted from https://stackoverflow.com/questions/73256206\n                    if \"prompt\" in k:\n                        to_pad = [torch.LongTensor(ex[k][::-1]) for ex in features]\n                    else:\n                        to_pad = [torch.LongTensor(ex[k]) for ex in features]\n                    if k.endswith(\"_input_ids\"):\n                        padding_value = self.pad_token_id\n                    elif k.endswith(\"_labels\"):\n                        padding_value = self.label_pad_token_id\n                    elif k.endswith(\"_attention_mask\"):\n                        padding_value = 0\n                    else:\n                        raise ValueError(f\"Unexpected key in batch '{k}'\")\n\n                    padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value)\n                    # for the prompt, flip back so padding is on left side\n                    if \"prompt\" in k:\n                        padded_batch[k] = padded_batch[k].flip(dims=[1])\n            elif k.endswith(\"_logps\"):\n                # the cached reference model logprobs\n                padded_batch[k] = torch.tensor([ex[k] for ex in features])\n            else:\n                padded_batch[k] = [ex[k] for ex in features]\n\n        return padded_batch\n\n\nclass ConstantLengthDataset(IterableDataset):\n    \"\"\"\n    Iterable dataset that returns constant length chunks of tokens from stream of text files.\n    The dataset also formats the text before tokenization with a specific format that is provided\n    by the user.\n\n        Args:\n            tokenizer (`transformers.PreTrainedTokenizer`):\n                The processor used for processing the data.\n            dataset (`dataset.Dataset`):\n                Dataset with text files.\n            dataset_text_field (`str`, **optional**):\n                Name of the field in the dataset that contains the text. Used only if `formatting_func` is `None`.\n            formatting_func (`Callable`, **optional**):\n                Function that formats the text before tokenization. Usually it is recommended to have follows a certain\n                pattern such as `\"### Question: {question}\\n ### Answer: {answer}\\n\"`\n            infinite (`bool`, *optional*, defaults to `False`):\n                If True the iterator is reset after dataset reaches end else stops.\n            seq_length (`int`, *optional*, defaults to `1024`):\n                Length of token sequences to return.\n            num_of_sequences (`int`, *optional*, defaults to `1024`):\n                Number of token sequences to keep in buffer.\n            chars_per_token (`int`, *optional*, defaults to `3.6`):\n                Number of characters per token used to estimate number of tokens in text buffer.\n            eos_token_id (`int`, *optional*, defaults to `0`):\n                Id of the end of sequence token if the passed tokenizer does not have an EOS token.\n            shuffle ('bool', *optional*, defaults to True)\n                Shuffle the examples before they are returned\n            append_concat_token ('bool', *optional*, defaults to True)\n                If true, appends `eos_token_id` at the end of each sample being packed.\n            add_special_tokens ('bool', *optional*, defaults to True)\n                If true, tokenizers adds special tokens to each sample being packed.\n    \"\"\"\n\n    def __init__(\n        self,\n        tokenizer,\n        dataset,\n        dataset_text_field=None,\n        formatting_func=None,\n        infinite=False,\n        seq_length=1024,\n        num_of_sequences=1024,\n        chars_per_token=3.6,\n        eos_token_id=0,\n        shuffle=True,\n        append_concat_token=True,\n        add_special_tokens=True,\n    ):\n        self.tokenizer = tokenizer\n\n        if tokenizer.eos_token_id is None:\n            warnings.warn(\n                \"The passed tokenizer does not have an EOS token. We will use the passed eos_token_id instead which corresponds\"\n                f\" to {eos_token_id}. If this is not the correct EOS token, make sure to pass the correct eos_token_id.\"\n            )\n\n        self.concat_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id else eos_token_id\n        self.dataset = dataset\n        self.seq_length = seq_length\n        self.infinite = infinite\n        self.current_size = 0\n        self.max_buffer_size = seq_length * chars_per_token * num_of_sequences\n        self.shuffle = shuffle\n        self.append_concat_token = append_concat_token\n        self.add_special_tokens = add_special_tokens\n        if formatting_func is None:\n            self.formatting_func = lambda x: x[dataset_text_field]\n        else:\n            self.formatting_func = formatting_func\n\n        if formatting_func is not None:\n            if formatting_func.__code__.co_argcount > 1:\n                warnings.warn(\n                    \"The passed formatting_func has more than one argument. Usually that function should have a single argument `example`\"\n                    \" which corresponds to the dictionary returned by each element of the dataset. Make sure you know what you are doing.\"\n                )\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __iter__(self):\n        iterator = iter(self.dataset)\n        more_examples = True\n        while more_examples:\n            buffer, buffer_len = [], 0\n            while True:\n                if buffer_len >= self.max_buffer_size:\n                    break\n                try:\n                    buffer.append(self.formatting_func(next(iterator)))\n                    buffer_len += len(buffer[-1])\n                except StopIteration:\n                    if self.infinite:\n                        iterator = iter(self.dataset)\n                        warnings.warn(\"The dataset reached end and the iterator is reset to the start.\")\n                    else:\n                        more_examples = False\n                        break\n            tokenized_inputs = self.tokenizer(buffer, add_special_tokens=self.add_special_tokens, truncation=False)[\n                \"input_ids\"\n            ]\n            all_token_ids = []\n            for tokenized_input in tokenized_inputs:\n                if self.append_concat_token:\n                    tokenized_input = tokenized_input + [self.concat_token_id]\n                all_token_ids.extend(tokenized_input)\n            examples = []\n            for i in range(0, len(all_token_ids), self.seq_length):\n                input_ids = all_token_ids[i : i + self.seq_length]\n                if len(input_ids) == self.seq_length:\n                    examples.append(input_ids)\n            if self.shuffle:\n                random.shuffle(examples)\n            for example in examples:\n                self.current_size += 1\n                yield {\n                    \"input_ids\": torch.LongTensor(example),\n                    \"labels\": torch.LongTensor(example),\n                }\n\n\nclass RunningMoments:\n    def __init__(self, accelerator):\n        \"\"\"\n        Calculates the running mean and standard deviation of a data stream. Reference:\n        https://github.com/OpenLMLab/MOSS-RLHF/blob/40b91eb2f2b71b16919addede0341d2bef70825d/utils.py#L75\n        \"\"\"\n        self.mean = 0\n        self.std = 1\n        self.var = 1\n        self.count = 1e-24\n        self.accelerator = accelerator\n\n    @torch.no_grad()\n    def update(self, xs: torch.Tensor) -> Tuple[float, float]:\n        \"\"\"\n        Updates running moments from batch's moments computed across ranks\n        \"\"\"\n        if self.accelerator.use_distributed:\n            xs_mean, xs_var, xs_count = get_global_statistics(self.accelerator, xs)\n        else:\n            xs_count = xs.numel()\n            xs_var, xs_mean = torch.var_mean(xs, unbiased=False)\n        xs_mean, xs_var = xs_mean.float(), xs_var.float()\n\n        delta = xs_mean - self.mean\n        tot_count = self.count + xs_count\n\n        new_sum = xs_var * xs_count\n        # correct old_sum deviation accounting for the new mean\n        old_sum = self.var * self.count + delta**2 * self.count * xs_count / tot_count\n        tot_sum = old_sum + new_sum\n\n        self.mean += delta * xs_count / tot_count\n        self.var = tot_sum / tot_count\n        self.std = (self.var * tot_count / (tot_count - 1)).float().sqrt()\n        self.count = tot_count\n\n        return xs_mean.item(), (xs_var * xs_count / (xs_count - 1)).float().sqrt().item()\n\n\n@torch.no_grad()\ndef get_global_statistics(accelerator, xs: torch.Tensor, mask=None, device=\"cpu\") -> Tuple[float, float, int]:\n    \"\"\"\n    Computes element-wise mean and variance of the tensor across processes. Reference:\n    https://github.com/OpenLMLab/MOSS-RLHF/blob/40b91eb2f2b71b16919addede0341d2bef70825d/utils.py#L57C1-L73C75\n    \"\"\"\n    xs = xs.to(accelerator.device)\n    sum_and_count = torch.tensor([xs.sum(), (xs.numel() if mask is None else mask.sum())], device=xs.device)\n    sum_and_count = accelerator.reduce(sum_and_count)\n    global_sum, count = sum_and_count\n    global_mean = global_sum / count\n\n    sum_var = torch.sum(((xs - global_mean) ** 2).mul(1 if mask is None else mask))\n    sum_var = accelerator.reduce(sum_var)\n    global_var = sum_var / count\n\n    return global_mean.to(device), global_var.to(device), count.to(device)\n\n\ndef compute_accuracy(eval_pred) -> Dict[str, float]:\n    predictions, labels = eval_pred\n    # Here, predictions is rewards_chosen and rewards_rejected.\n    # We want to see how much of the time rewards_chosen > rewards_rejected.\n    if np.array(predictions[:, 0] == predictions[:, 1], dtype=float).sum() > 0:\n        warnings.warn(\n            f\"There are {np.array(predictions[:, 0] == predictions[:, 1]).sum()} out of {len(predictions[:, 0])} instances where the predictions for both options are equal. As a consequence the accuracy can be misleading.\"\n        )\n    predictions = np.argmax(predictions, axis=1)\n\n    accuracy = np.array(predictions == labels, dtype=float).mean().item()\n    return {\"accuracy\": accuracy}\n\n\ndef pad_to_length(tensor: torch.Tensor, length: int, pad_value: Union[int, float], dim: int = -1) -> torch.Tensor:\n    if tensor.size(dim) >= length:\n        return tensor\n    else:\n        pad_size = list(tensor.shape)\n        pad_size[dim] = length - tensor.size(dim)\n        return torch.cat(\n            [\n                tensor,\n                pad_value * torch.ones(*pad_size, dtype=tensor.dtype, device=tensor.device),\n            ],\n            dim=dim,\n        )\n\n\ndef disable_dropout_in_model(model: torch.nn.Module) -> None:\n    for module in model.modules():\n        if isinstance(module, torch.nn.Dropout):\n            module.p = 0\n\n\ndef exact_div(a, b, a_str, b_str, custom_error_message=\"\"):\n    q = a // b\n    if a != q * b:\n        raise ValueError(f\"{custom_error_message}, {a_str}={a}, {b_str}={b}, inexact division: {a} / {b} = {a / b}\")\n    return q\n\n\n# copied from https://github.com/kvablack/ddpo-pytorch/blob/main/ddpo_pytorch/stat_tracking.py#L5\nclass PerPromptStatTracker:\n    r\"\"\"\n    Class for tracking statistics per prompt. Mainly used to calculate advantage for the DPPO algorithm\n\n    Args:\n        buffer_size (`int`):\n            Size of the buffer to keep for each prompt.\n        min_count (`int`):\n            Minimum number of samples to keep in the buffer before calculating the mean and std.\n    \"\"\"\n\n    def __init__(self, buffer_size, min_count):\n        self.buffer_size = buffer_size\n        self.min_count = min_count\n        self.stats = {}\n\n    def update(self, prompts, rewards):\n        prompts = np.array(prompts)\n        rewards = np.array(rewards)\n        unique = np.unique(prompts)\n        advantages = np.empty_like(rewards)\n        for prompt in unique:\n            prompt_rewards = rewards[prompts == prompt]\n            if prompt not in self.stats:\n                self.stats[prompt] = deque(maxlen=self.buffer_size)\n            self.stats[prompt].extend(prompt_rewards)\n\n            if len(self.stats[prompt]) < self.min_count:\n                mean = np.mean(rewards)\n                std = np.std(rewards) + 1e-6\n            else:\n                mean = np.mean(self.stats[prompt])\n                std = np.std(self.stats[prompt]) + 1e-6\n            advantages[prompts == prompt] = (prompt_rewards - mean) / std\n\n        return advantages\n\n    def get_stats(self):\n        return {k: {\"mean\": np.mean(v), \"std\": np.std(v), \"count\": len(v)} for k, v in self.stats.items()}\n\n\ndef neftune_post_forward_hook(module, input, output):\n    \"\"\"\n    Implements the NEFTune forward pass for the model using forward hooks. Note this works only for\n    torch.nn.Embedding layers. This method is slightly adapted from the original source code\n    that can be found here: https://github.com/neelsjain/NEFTune\n\n    Simply add it to your model as follows:\n    ```python\n    model = ...\n    model.embed_tokens.neftune_noise_alpha = 0.1\n    model.embed_tokens.register_forward_hook(neftune_post_forward_hook)\n    ```\n\n    Args:\n        module (`torch.nn.Module`):\n            The embedding module where the hook is attached. Note that you need to set\n            `module.neftune_noise_alpha` to the desired noise alpha value.\n        input (`torch.Tensor`):\n            The input tensor to the model.\n        output (`torch.Tensor`):\n            The output tensor of the model (i.e. the embeddings).\n    \"\"\"\n    if module.training:\n        dims = torch.tensor(output.size(1) * output.size(2))\n        mag_norm = module.neftune_noise_alpha / torch.sqrt(dims)\n        output = output + torch.zeros_like(output).uniform_(-mag_norm, mag_norm)\n    return output\n\n\ndef peft_module_casting_to_bf16(model):\n    from peft.tuners.tuners_utils import BaseTunerLayer\n\n    for name, module in model.named_modules():\n        if isinstance(module, BaseTunerLayer):\n            module = module.to(torch.bfloat16)\n        elif isinstance(module, torch.nn.LayerNorm) or \"norm\" in name:\n            module = module.to(torch.float32)\n        elif any(x in name for x in [\"lm_head\", \"embed_tokens\", \"wte\", \"wpe\"]):\n            if hasattr(module, \"weight\"):\n                if module.weight.dtype == torch.float32:\n                    module = module.to(torch.bfloat16)\n\n\ndef trl_sanitze_kwargs_for_tagging(model, tag_names, kwargs=None):\n    if is_unsloth_available():\n        # Unsloth adds a new attribute in the model config `unsloth_version`\n        # to keep track of models that have been patched with unsloth.\n        if hasattr(model, \"config\") and getattr(model.config, \"unsloth_version\", None) is not None:\n            tag_names.append(\"unsloth\")\n\n    if kwargs is not None:\n        if \"tags\" not in kwargs:\n            kwargs[\"tags\"] = tag_names\n        elif \"tags\" in kwargs and isinstance(kwargs[\"tags\"], list):\n            kwargs[\"tags\"].extend(tag_names)\n        elif \"tags\" in kwargs and isinstance(kwargs[\"tags\"], str):\n            tag_names.append(kwargs[\"tags\"])\n            kwargs[\"tags\"] = tag_names\n    return kwargs\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/trainer/reward_trainer.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import inspect\nimport warnings\nfrom dataclasses import FrozenInstanceError, replace\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom datasets import Dataset\nfrom transformers import DataCollator, PreTrainedModel, PreTrainedTokenizerBase, Trainer, TrainingArguments\nfrom transformers.trainer_callback import TrainerCallback\nfrom transformers.trainer_pt_utils import nested_detach\nfrom transformers.trainer_utils import EvalPrediction\n\nfrom ..import_utils import is_peft_available\nfrom .reward_config import RewardConfig\nfrom .utils import RewardDataCollatorWithPadding, compute_accuracy\n\n\nif is_peft_available():\n    from peft import PeftModel, get_peft_model, prepare_model_for_kbit_training\n\n\nclass RewardTrainer(Trainer):\n    r\"\"\"\n    The RewardTrainer can be used to train your custom Reward Model. It is a subclass of the\n    `transformers.Trainer` class and inherits all of its attributes and methods. It is recommended to use\n    an `AutoModelForSequenceClassification` as the reward model. The reward model should be trained on a dataset\n    of paired examples, where each example is a tuple of two sequences. The reward model should be trained to\n    predict which example in the pair is more relevant to the task at hand.\n\n    The reward trainer expects a very specific format for the dataset. The dataset should contain two 4 entries at least\n    if you don't use the default `RewardDataCollatorWithPadding` data collator. The entries should be named\n    - `input_ids_chosen`\n    - `attention_mask_chosen`\n    - `input_ids_rejected`\n    - `attention_mask_rejected`\n\n    Optionally, you can also pass a `margin` entry to the dataset. This entry should contain the margin used to modulate the\n    loss of the reward model as outlined in https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/.\n    If you don't pass a margin, no margin will be used.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Union[PreTrainedModel, nn.Module] = None,\n        args: Optional[RewardConfig] = None,\n        data_collator: Optional[DataCollator] = None,\n        train_dataset: Optional[Dataset] = None,\n        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n        model_init: Optional[Callable[[], PreTrainedModel]] = None,\n        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n        callbacks: Optional[List[TrainerCallback]] = None,\n        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (\n            None,\n            None,\n        ),\n        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n        max_length: Optional[int] = None,\n        peft_config: Optional[Dict] = None,\n    ):\n        \"\"\"\n        Initialize RewardTrainer.\n\n        Args:\n            model (`transformers.PreTrainedModel`):\n                The model to train, preferably an `AutoModelForSequenceClassification`.\n            args (`RewardConfig`):\n                The arguments to use for training.\n            data_collator (`transformers.DataCollator`):\n                The data collator to use for training. If None is specified, the default data collator (`RewardDataCollatorWithPadding`) will be used\n                which will pad the sequences to the maximum length of the sequences in the batch, given a dataset of paired sequences.\n            train_dataset (`datasets.Dataset`):\n                The dataset to use for training.\n            eval_dataset (`datasets.Dataset`):\n                The dataset to use for evaluation.\n            tokenizer (`transformers.PreTrainedTokenizerBase`):\n                The tokenizer to use for training. This argument is required if you want to use the default data collator.\n            model_init (`Callable[[], transformers.PreTrainedModel]`):\n                The model initializer to use for training. If None is specified, the default model initializer will be used.\n            compute_metrics (`Callable[[transformers.EvalPrediction], Dict]`, *optional* defaults to `compute_accuracy`):\n                The metrics to use for evaluation. If no metrics are specified, the default metric (`compute_accuracy`) will be used.\n            callbacks (`List[transformers.TrainerCallback]`):\n                The callbacks to use for training.\n            optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`):\n                The optimizer and scheduler to use for training.\n            preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`):\n                The function to use to preprocess the logits before computing the metrics.\n            peft_config (`Dict`, defaults to `None`):\n                The PEFT configuration to use for training. If you pass a PEFT configuration, the model will be wrapped in a PEFT model.\n        \"\"\"\n        if type(args) == TrainingArguments:\n            warnings.warn(\n                \"Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.\",\n                FutureWarning,\n            )\n            if max_length is not None:\n                warnings.warn(\n                    \"The `max_length` argument is deprecated and will be removed in a future version. Please use the `RewardConfig` to set `max_length` instead.\",\n                    FutureWarning,\n                )\n        else:\n            if max_length is not None and args.max_length is not None:\n                raise ValueError(\n                    \"You cannot specify both `max_length` and `args.max_length`. Please use the `RewardConfig` to set `max_length` once.\"\n                )\n            if max_length is not None and args.max_length is None:\n                warnings.warn(\n                    \"The `max_length` argument is deprecated and will be removed in a future version. Please use the `RewardConfig` to set `max_length` instead.\",\n                    FutureWarning,\n                )\n        if not is_peft_available() and peft_config is not None:\n            raise ValueError(\n                \"PEFT is not installed and you passed a `peft_config` in the trainer's kwargs, please install it to use the PEFT models\"\n            )\n        elif is_peft_available() and peft_config is not None:\n            if not isinstance(model, PeftModel):\n                if getattr(model, \"is_loaded_in_8bit\", False) or getattr(model, \"is_quantized\", False):\n                    _supports_gc_kwargs = \"gradient_checkpointing_kwargs\" in list(\n                        inspect.signature(prepare_model_for_kbit_training).parameters\n                    )\n\n                    preprare_model_kwargs = {\"use_gradient_checkpointing\": args.gradient_checkpointing}\n\n                    if not _supports_gc_kwargs and args.gradient_checkpointing_kwargs is not None:\n                        warnings.warn(\n                            \"You passed `gradient_checkpointing_kwargs` in the trainer's kwargs, but your peft version does not support it. \"\n                            \"please update to the latest version of peft to use `gradient_checkpointing_kwargs`.\"\n                        )\n                    elif _supports_gc_kwargs and args.gradient_checkpointing_kwargs is not None:\n                        preprare_model_kwargs[\"gradient_checkpointing_kwargs\"] = args.gradient_checkpointing_kwargs\n\n                    model = prepare_model_for_kbit_training(model, **preprare_model_kwargs)\n\n                model = get_peft_model(model, peft_config)\n\n        if compute_metrics is None:\n            compute_metrics = compute_accuracy\n\n        if data_collator is None:\n            if tokenizer is None:\n                raise ValueError(\n                    \"max_length or a tokenizer must be specified when using the default RewardDataCollatorWithPadding\"\n                )\n            if type(args) == TrainingArguments:\n                if max_length is None:\n                    warnings.warn(\n                        \"When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig.\"\n                        \" It will be set to `512` by default, but you should do it yourself in the future.\",\n                        UserWarning,\n                    )\n                    max_length = 512\n            else:\n                if max_length is None and args.max_length is None:\n                    warnings.warn(\n                        \"When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig.\"\n                        \" It will be set to `512` by default, but you should do it yourself in the future.\",\n                        UserWarning,\n                    )\n                    max_length = 512\n                if max_length is None and args.max_length is not None:\n                    max_length = args.max_length\n\n            data_collator = RewardDataCollatorWithPadding(tokenizer, max_length=max_length)\n\n            if args.remove_unused_columns:\n                try:  # for bc before https://github.com/huggingface/transformers/pull/25435\n                    args.remove_unused_columns = False\n                except FrozenInstanceError:\n                    args = replace(args, remove_unused_columns=False)\n                # warn users\n                warnings.warn(\n                    \"When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig\"\n                    \" we have set it for you, but you should do it yourself in the future.\",\n                    UserWarning,\n                )\n\n            self.use_reward_data_collator = True\n        else:\n            self.use_reward_data_collator = False\n        super().__init__(\n            model,\n            args,\n            data_collator,\n            train_dataset,\n            eval_dataset,\n            tokenizer,\n            model_init,\n            compute_metrics,\n            callbacks,\n            optimizers,\n            preprocess_logits_for_metrics,\n        )\n\n    def compute_loss(\n        self,\n        model: Union[PreTrainedModel, nn.Module],\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        return_outputs=False,\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict[str, torch.Tensor]]]:\n        if not self.use_reward_data_collator:\n            warnings.warn(\n                \"The current compute_loss is implemented for RewardDataCollatorWithPadding,\"\n                \" if you are using a custom data collator make sure you know what you are doing or\"\n                \" implement your own compute_loss method.\"\n            )\n        rewards_chosen = model(\n            input_ids=inputs[\"input_ids_chosen\"],\n            attention_mask=inputs[\"attention_mask_chosen\"],\n            return_dict=True,\n        )[\"logits\"]\n        rewards_rejected = model(\n            input_ids=inputs[\"input_ids_rejected\"],\n            attention_mask=inputs[\"attention_mask_rejected\"],\n            return_dict=True,\n        )[\"logits\"]\n        # calculate loss, optionally modulate with margin\n        if \"margin\" in inputs:\n            loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected - inputs[\"margin\"]).mean()\n        else:\n            loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()\n\n        if return_outputs:\n            return loss, {\n                \"rewards_chosen\": rewards_chosen,\n                \"rewards_rejected\": rewards_rejected,\n            }\n        return loss\n\n    def prediction_step(\n        self,\n        model: Union[PreTrainedModel, nn.Module],\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        prediction_loss_only: bool,\n        ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n        inputs = self._prepare_inputs(inputs)\n        if ignore_keys is None:\n            if hasattr(self.model, \"config\"):\n                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n            else:\n                ignore_keys = []\n\n        with torch.no_grad():\n            loss, logits_dict = self.compute_loss(model, inputs, return_outputs=True)\n\n        if prediction_loss_only:\n            return (loss, None, None)\n\n        loss = loss.detach()\n        logits = tuple(v for k, v in logits_dict.items() if k not in ignore_keys)\n        logits = nested_detach(logits)\n        # Stack accepted against rejected, mean over logits\n        # and softmax to get preferences between accepted and rejected to sum to 1\n        logits = torch.stack(logits).mean(dim=2).softmax(dim=0).T\n\n        labels = torch.zeros(logits.shape[0])\n        labels = self._prepare_inputs(labels)\n\n        return loss, logits, labels\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/trainer/sft_trainer.py",
        "recommendation": "The code file is too long to analyze. Please select a shorter file.",
        "code_snippet": "import dataclasses\nimport inspect\nimport warnings\nfrom functools import wraps\nfrom typing import Callable, Dict, List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom datasets import Dataset\nfrom datasets.arrow_writer import SchemaInferenceError\nfrom datasets.builder import DatasetGenerationError\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    DataCollator,\n    DataCollatorForLanguageModeling,\n    PreTrainedModel,\n    PreTrainedTokenizerBase,\n    Trainer,\n    TrainingArguments,\n)\nfrom transformers.modeling_utils import unwrap_model\nfrom transformers.trainer_callback import TrainerCallback\nfrom transformers.trainer_utils import EvalPrediction\n\nfrom ..extras.dataset_formatting import get_formatting_func_from_dataset\nfrom ..import_utils import is_peft_available\nfrom .utils import (\n    ConstantLengthDataset,\n    DataCollatorForCompletionOnlyLM,\n    neftune_post_forward_hook,\n    peft_module_casting_to_bf16,\n    trl_sanitze_kwargs_for_tagging,\n)\n\n\nif is_peft_available():\n    from peft import PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n\n\nclass SFTTrainer(Trainer):\n    r\"\"\"\n    Class definition of the Supervised Finetuning Trainer (SFT Trainer).\n    This class is a wrapper around the `transformers.Trainer` class and inherits all of its attributes and methods.\n    The trainer takes care of properly initializing the PeftModel in case a user passes a `PeftConfig` object.\n\n    Args:\n        model (Union[`transformers.PreTrainedModel`, `nn.Module`, `str`]):\n            The model to train, can be a `PreTrainedModel`, a `torch.nn.Module` or a string with the model name to\n            load from cache or download. The model can be also converted to a `PeftModel` if a `PeftConfig` object is\n            passed to the `peft_config` argument.\n        args (Optional[`transformers.TrainingArguments`]):\n            The arguments to tweak for training. Please refer to the official documentation of `transformers.TrainingArguments`\n            for more information.\n        data_collator (Optional[`transformers.DataCollator`]):\n            The data collator to use for training.\n        train_dataset (Optional[`datasets.Dataset`]):\n            The dataset to use for training. We recommend users to use `trl.trainer.ConstantLengthDataset` to create their dataset.\n        eval_dataset (Optional[Union[`datasets.Dataset`, Dict[`str`, `datasets.Dataset`]]]):\n            The dataset to use for evaluation. We recommend users to use `trl.trainer.ConstantLengthDataset` to create their dataset.\n        tokenizer (Optional[`transformers.PreTrainedTokenizer`]):\n            The tokenizer to use for training. If not specified, the tokenizer associated to the model will be used.\n        model_init (`Callable[[], transformers.PreTrainedModel]`):\n            The model initializer to use for training. If None is specified, the default model initializer will be used.\n        compute_metrics (`Callable[[transformers.EvalPrediction], Dict]`, *optional* defaults to None):\n            The function used to compute metrics during evaluation. It should return a dictionary mapping metric names to metric values.\n            If not specified, only the loss will be computed during evaluation.\n        callbacks (`List[transformers.TrainerCallback]`):\n            The callbacks to use for training.\n        optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`):\n            The optimizer and scheduler to use for training.\n        preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`):\n            The function to use to preprocess the logits before computing the metrics.\n        peft_config (`Optional[PeftConfig]`):\n            The PeftConfig object to use to initialize the PeftModel.\n        dataset_text_field (`Optional[str]`):\n            The name of the text field of the dataset, in case this is passed by a user, the trainer will automatically create a\n            `ConstantLengthDataset` based on the `dataset_text_field` argument.\n        formatting_func (`Optional[Callable]`):\n            The formatting function to be used for creating the `ConstantLengthDataset`.\n        max_seq_length (`Optional[int]`):\n            The maximum sequence length to use for the `ConstantLengthDataset` and for automatically creating the Dataset. Defaults to `512`.\n        infinite (`Optional[bool]`):\n            Whether to use an infinite dataset or not. Defaults to `False`.\n        num_of_sequences (`Optional[int]`):\n            The number of sequences to use for the `ConstantLengthDataset`. Defaults to `1024`.\n        chars_per_token (`Optional[float]`):\n            The number of characters per token to use for the `ConstantLengthDataset`. Defaults to `3.6`. You can check how this is computed in the\n            stack-llama example: https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53.\n        packing (`Optional[bool]`):\n            Used only in case `dataset_text_field` is passed. This argument is used by the `ConstantLengthDataset` to pack the sequences\n            of the dataset.\n        dataset_num_proc (`Optional[int]`):\n            The number of workers to use to tokenize the data. Only used when `packing=False`. Defaults to None.\n        dataset_batch_size (`int`):\n            The number of examples to tokenize per batch. If batch_size <= 0 or batch_size == None,\n            tokenize the full dataset as a single batch. Defaults to 1000.\n        neftune_noise_alpha (`Optional[float]`):\n            If not `None`, this will activate NEFTune noise embeddings. This has been proven to drastically improve model performances for instruction\n            fine-tuning. Check out the original paper here: https://arxiv.org/abs/2310.05914 and the original code here: https://github.com/neelsjain/NEFTune\n        model_init_kwargs: (`Optional[Dict]`, *optional*):\n            Dict of Optional kwargs to pass when instantiating the model from a string\n        dataset_kwargs: (`Optional[Dict]`, *optional*):\n            Dict of Optional kwargs to pass when creating packed or non-packed datasets\n    \"\"\"\n    _tag_names = [\"trl\", \"sft\"]\n\n    def __init__(\n        self,\n        model: Union[PreTrainedModel, nn.Module, str] = None,\n        args: TrainingArguments = None,\n        data_collator: Optional[DataCollator] = None,\n        train_dataset: Optional[Dataset] = None,\n        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n        model_init: Optional[Callable[[], PreTrainedModel]] = None,\n        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n        callbacks: Optional[List[TrainerCallback]] = None,\n        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),\n        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        dataset_text_field: Optional[str] = None,\n        packing: Optional[bool] = False,\n        formatting_func: Optional[Callable] = None,\n        max_seq_length: Optional[int] = None,\n        infinite: Optional[bool] = None,\n        num_of_sequences: Optional[int] = 1024,\n        chars_per_token: Optional[float] = 3.6,\n        dataset_num_proc: Optional[int] = None,\n        dataset_batch_size: int = 1000,\n        neftune_noise_alpha: Optional[float] = None,\n        model_init_kwargs: Optional[Dict] = None,\n        dataset_kwargs: Optional[Dict] = None,\n    ):\n        if model_init_kwargs is None:\n            model_init_kwargs = {}\n        elif not isinstance(model, str):\n            raise ValueError(\"You passed model_kwargs to the SFTTrainer. But your model is already instantiated.\")\n\n        if infinite is not None:\n            warnings.warn(\n                \"The `infinite` argument is deprecated and will be removed in a future version of TRL. Use `TrainingArguments.max_steps` or `TrainingArguments.num_train_epochs` instead to control training length.\"\n            )\n\n        if isinstance(model, str):\n            warnings.warn(\n                \"You passed a model_id to the SFTTrainer. This will automatically create an \"\n                \"`AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\"\n            )\n            model = AutoModelForCausalLM.from_pretrained(model, **model_init_kwargs)\n\n        if packing and data_collator is not None and isinstance(data_collator, DataCollatorForCompletionOnlyLM):\n            raise ValueError(\n                \"You passed a `DataCollatorForCompletionOnlyLM` to the SFTTrainer. This is not compatible with the `packing` argument.\"\n            )\n\n        if is_peft_available() and peft_config is not None:\n            if not isinstance(peft_config, PeftConfig):\n                raise ValueError(\n                    \"If you want to use the PeftModel, you need to pass a PeftConfig object to the SFTTrainer.\"\n                    f\" and you passed a {type(peft_config)}.\"\n                )\n\n            if not isinstance(model, PeftModel):\n                _support_gc_kwargs = hasattr(\n                    args, \"gradient_checkpointing_kwargs\"\n                ) and \"gradient_checkpointing_kwargs\" in list(\n                    inspect.signature(prepare_model_for_kbit_training).parameters\n                )\n                gradient_checkpointing_kwargs = getattr(args, \"gradient_checkpointing_kwargs\", None) or {}\n                if getattr(model, \"is_loaded_in_8bit\", False) or getattr(model, \"is_loaded_in_4bit\", False):\n                    preprare_model_kwargs = {\n                        \"use_gradient_checkpointing\": getattr(args, \"gradient_checkpointing\", False)\n                    }\n\n                    if _support_gc_kwargs:\n                        preprare_model_kwargs[\"gradient_checkpointing_kwargs\"] = gradient_checkpointing_kwargs\n\n                    model = prepare_model_for_kbit_training(model, **preprare_model_kwargs)\n\n                    if args is not None:\n                        args = dataclasses.replace(args, gradient_checkpointing=False)\n                elif getattr(args, \"gradient_checkpointing\", False) and (\n                    \"use_reentrant\" not in gradient_checkpointing_kwargs\n                    or gradient_checkpointing_kwargs[\"use_reentrant\"]\n                ):\n                    # For backward compatibility with older versions of transformers\n                    if hasattr(model, \"enable_input_require_grads\"):\n                        model.enable_input_require_grads()\n                    else:\n\n                        def make_inputs_require_grad(module, input, output):\n                            output.requires_grad_(True)\n\n                        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n                model = get_peft_model(model, peft_config)\n                if args.bf16 and getattr(model, \"is_loaded_in_4bit\", False):\n                    peft_module_casting_to_bf16(model)\n\n        if tokenizer is None:\n            tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)\n            if getattr(tokenizer, \"pad_token\", None) is None:\n                tokenizer.pad_token = tokenizer.eos_token\n\n        if max_seq_length is None:\n            # to overcome some issues with broken tokenizers\n            max_seq_length = min(tokenizer.model_max_length, 1024)\n\n            warnings.warn(\n                f\"You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to {max_seq_length}\"\n            )\n\n        self.dataset_num_proc = dataset_num_proc\n        self.dataset_batch_size = dataset_batch_size\n\n        self._trainer_supports_neftune = hasattr(args, \"neftune_noise_alpha\")\n\n        if neftune_noise_alpha is not None and self._trainer_supports_neftune:\n            args.neftune_noise_alpha = neftune_noise_alpha\n            warnings.warn(\n                \"You passed a `neftune_noise_alpha` argument to the SFTTrainer, the value you passed will override the one in the `TrainingArguments`.\"\n            )\n            # self.neftune_noise_alpha is done at Trainer level\n        elif not self._trainer_supports_neftune:\n            self.neftune_noise_alpha = neftune_noise_alpha\n\n        if formatting_func is None and dataset_text_field is None:\n            # check if dataset has ChatML format or instruction format and is supported\n            # if not stays #None\n            formatting_func = get_formatting_func_from_dataset(train_dataset, tokenizer)\n\n        if not packing:\n            if dataset_text_field is None and formatting_func is None:\n                raise ValueError(\n                    \"You passed `packing=False` to the SFTTrainer, but you didn't pass a `dataset_text_field` or `formatting_func` argument.\"\n                )\n\n            if data_collator is None:\n                data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n        if dataset_kwargs is None:\n            dataset_kwargs = {}\n        if train_dataset is not None:\n            train_dataset = self._prepare_dataset(\n                train_dataset,\n                tokenizer,\n                packing,\n                dataset_text_field,\n                max_seq_length,\n                formatting_func,\n                num_of_sequences,\n                chars_per_token,\n                remove_unused_columns=args.remove_unused_columns if args is not None else True,\n                **dataset_kwargs,\n            )\n        if eval_dataset is not None:\n            _multiple = isinstance(eval_dataset, dict)\n            _eval_datasets = eval_dataset if _multiple else {\"singleton\": eval_dataset}\n            for _eval_dataset_name, _eval_dataset in _eval_datasets.items():\n                _eval_datasets[_eval_dataset_name] = self._prepare_dataset(\n                    _eval_dataset,\n                    tokenizer,\n                    packing,\n                    dataset_text_field,\n                    max_seq_length,\n                    formatting_func,\n                    num_of_sequences,\n                    chars_per_token,\n                    remove_unused_columns=args.remove_unused_columns if args is not None else True,\n                    **dataset_kwargs,\n                )\n            if not _multiple:\n                eval_dataset = _eval_datasets[\"singleton\"]\n\n        if tokenizer.padding_side is not None and tokenizer.padding_side != \"right\":\n            warnings.warn(\n                \"You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to \"\n                \"overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\"\n            )\n\n        super().__init__(\n            model=model,\n            args=args,\n            data_collator=data_collator,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            tokenizer=tokenizer,\n            model_init=model_init,\n            compute_metrics=compute_metrics,\n            callbacks=callbacks,\n            optimizers=optimizers,\n            preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n        )\n\n        if self.args.max_steps > 0 and packing:\n            warnings.warn(\n                \"You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\"\n            )\n            self.train_dataset.infinite = True\n        elif self.args.max_steps == -1 and packing:\n            self.train_dataset.infinite = False\n\n    @wraps(Trainer.train)\n    def train(self, *args, **kwargs):\n        # Activate neftune right before training.\n        if self.neftune_noise_alpha is not None and not self._trainer_supports_neftune:\n            self.model = self._trl_activate_neftune(self.model)\n\n        output = super().train(*args, **kwargs)\n\n        # After training we make sure to retrieve back the original forward pass method\n        # for the embedding layer by removing the forward post hook.\n        if self.neftune_noise_alpha is not None and not self._trainer_supports_neftune:\n            unwrapped_model = unwrap_model(self.model)\n            if is_peft_available() and isinstance(unwrapped_model, PeftModel):\n                embeddings = unwrapped_model.base_model.model.get_input_embeddings()\n            else:\n                embeddings = unwrapped_model.get_input_embeddings()\n\n            self.neftune_hook_handle.remove()\n            del embeddings.neftune_noise_alpha\n\n        return output\n\n    @wraps(Trainer.push_to_hub)\n    def push_to_hub(self, commit_message: Optional[str] = \"End of training\", blocking: bool = True, **kwargs) -> str:\n        \"\"\"\n        Overwrite the `push_to_hub` method in order to force-add the tag \"sft\" when pushing the\n        model on the Hub. Please refer to `~transformers.Trainer.push_to_hub` for more details.\n        \"\"\"\n        kwargs = trl_sanitze_kwargs_for_tagging(model=self.model, tag_names=self._tag_names, kwargs=kwargs)\n\n        return super().push_to_hub(commit_message=commit_message, blocking=blocking, **kwargs)\n\n    def _prepare_dataset(\n        self,\n        dataset,\n        tokenizer,\n        packing,\n        dataset_text_field,\n        max_seq_length,\n        formatting_func,\n        num_of_sequences,\n        chars_per_token,\n        remove_unused_columns=True,\n        append_concat_token=True,\n        add_special_tokens=True,\n    ):\n        if dataset is None:\n            raise ValueError(\"The dataset should not be None\")\n\n        # check if torch dataset / dataloader and do nothing\n        if isinstance(dataset, (torch.utils.data.IterableDataset, torch.utils.data.Dataset, ConstantLengthDataset)):\n            return dataset\n\n        if not packing:\n            return self._prepare_non_packed_dataloader(\n                tokenizer,\n                dataset,\n                dataset_text_field,\n                max_seq_length,\n                formatting_func,\n                add_special_tokens,\n                remove_unused_columns,\n            )\n\n        else:\n            return self._prepare_packed_dataloader(\n                tokenizer,\n                dataset,\n                dataset_text_field,\n                max_seq_length,\n                num_of_sequences,\n                chars_per_token,\n                formatting_func,\n                append_concat_token,\n                add_special_tokens,\n            )\n\n    def _prepare_non_packed_dataloader(\n        self,\n        tokenizer,\n        dataset,\n        dataset_text_field,\n        max_seq_length,\n        formatting_func=None,\n        add_special_tokens=True,\n        remove_unused_columns=True,\n    ):\n        use_formatting_func = formatting_func is not None and dataset_text_field is None\n        self._dataset_sanity_checked = False\n\n        # Inspired from: https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt\n        def tokenize(element):\n            outputs = tokenizer(\n                element[dataset_text_field] if not use_formatting_func else formatting_func(element),\n                add_special_tokens=add_special_tokens,\n                truncation=True,\n                padding=False,\n                max_length=max_seq_length,\n                return_overflowing_tokens=False,\n                return_length=False,\n            )\n\n            if use_formatting_func and not self._dataset_sanity_checked:\n                if not isinstance(formatting_func(element), list):\n                    raise ValueError(\n                        \"The `formatting_func` should return a list of processed strings since it can lead to silent bugs.\"\n                    )\n                else:\n                    self._dataset_sanity_checked = True\n\n            return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"]}\n\n        tokenized_dataset = dataset.map(\n            tokenize,\n            batched=True,\n            remove_columns=dataset.column_names if remove_unused_columns else None,\n            num_proc=self.dataset_num_proc,\n            batch_size=self.dataset_batch_size,\n        )\n\n        return tokenized_dataset\n\n    def _prepare_packed_dataloader(\n        self,\n        tokenizer,\n        dataset,\n        dataset_text_field,\n        max_seq_length,\n        num_of_sequences,\n        chars_per_token,\n        formatting_func=None,\n        append_concat_token=True,\n        add_special_tokens=True,\n    ):\n        if dataset_text_field is not None or formatting_func is not None:\n            if tokenizer is None:\n                raise ValueError(\"You need to pass a tokenizer when using `dataset_text_field` with `SFTTrainer`.\")\n\n            constant_length_iterator = ConstantLengthDataset(\n                tokenizer,\n                dataset,\n                dataset_text_field=dataset_text_field,\n                formatting_func=formatting_func,\n                seq_length=max_seq_length,\n                infinite=False,\n                num_of_sequences=num_of_sequences,\n                chars_per_token=chars_per_token,\n                eos_token_id=tokenizer.eos_token_id,\n                append_concat_token=append_concat_token,\n                add_special_tokens=add_special_tokens,\n            )\n\n            def data_generator(constant_length_iterator):\n                for i in constant_length_iterator:\n                    yield i\n\n            try:\n                packed_dataset = Dataset.from_generator(\n                    data_generator, gen_kwargs={\"constant_length_iterator\": constant_length_iterator}\n                )\n            except (DatasetGenerationError, SchemaInferenceError):\n                raise ValueError(\n                    \"Error occurred while packing the dataset. Make sure that your dataset has enough samples to at least yield one packed sequence.\"\n                )\n            return packed_dataset\n        else:\n            raise ValueError(\n                \"You need to pass a `dataset_text_field` or `formatting_func` argument to the SFTTrainer if you want to use the `ConstantLengthDataset`.\"\n            )\n\n    def _trl_activate_neftune(self, model):\n        r\"\"\"\n        Activates the neftune as presented in this code: https://github.com/neelsjain/NEFTune and paper: https://arxiv.org/abs/2310.05914\n        Since in transformers Trainer we do have an `_activate_neftune` method, we need to rename this method to avoid conflicts.\n        \"\"\"\n        unwrapped_model = unwrap_model(model)\n        if is_peft_available() and isinstance(unwrapped_model, PeftModel):\n            embeddings = unwrapped_model.base_model.model.get_input_embeddings()\n        else:\n            embeddings = unwrapped_model.get_input_embeddings()\n\n        embeddings.neftune_noise_alpha = self.neftune_noise_alpha\n        hook_handle = embeddings.register_forward_hook(neftune_post_forward_hook)\n        self.neftune_hook_handle = hook_handle\n        return model\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/trainer/ddpo_config.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import os\nimport sys\nimport warnings\nfrom dataclasses import dataclass, field\nfrom typing import Literal, Optional\n\nfrom ..core import flatten_dict\nfrom ..import_utils import is_bitsandbytes_available, is_torchvision_available\n\n\n@dataclass\nclass DDPOConfig:\n    \"\"\"\n    Configuration class for DDPOTrainer\n    \"\"\"\n\n    # common parameters\n    exp_name: str = os.path.basename(sys.argv[0])[: -len(\".py\")]\n    \"\"\"the name of this experiment (by default is the file name without the extension name)\"\"\"\n    run_name: Optional[str] = \"\"\n    \"\"\"Run name for wandb logging and checkpoint saving.\"\"\"\n    seed: int = 0\n    \"\"\"Seed value for random generations\"\"\"\n    log_with: Optional[Literal[\"wandb\", \"tensorboard\"]] = None\n    \"\"\"Log with either 'wandb' or 'tensorboard', check  https://huggingface.co/docs/accelerate/usage_guides/tracking for more details\"\"\"\n    tracker_kwargs: dict = field(default_factory=dict)\n    \"\"\"Keyword arguments for the tracker (e.g. wandb_project)\"\"\"\n    accelerator_kwargs: dict = field(default_factory=dict)\n    \"\"\"Keyword arguments for the accelerator\"\"\"\n    project_kwargs: dict = field(default_factory=dict)\n    \"\"\"Keyword arguments for the accelerator project config (e.g. `logging_dir`)\"\"\"\n    tracker_project_name: str = \"trl\"\n    \"\"\"Name of project to use for tracking\"\"\"\n    logdir: str = \"logs\"\n    \"\"\"Top-level logging directory for checkpoint saving.\"\"\"\n\n    # hyperparameters\n    num_epochs: int = 100\n    \"\"\"Number of epochs to train.\"\"\"\n    save_freq: int = 1\n    \"\"\"Number of epochs between saving model checkpoints.\"\"\"\n    num_checkpoint_limit: int = 5\n    \"\"\"Number of checkpoints to keep before overwriting old ones.\"\"\"\n    mixed_precision: str = \"fp16\"\n    \"\"\"Mixed precision training.\"\"\"\n    allow_tf32: bool = True\n    \"\"\"Allow tf32 on Ampere GPUs.\"\"\"\n    resume_from: Optional[str] = \"\"\n    \"\"\"Resume training from a checkpoint.\"\"\"\n    sample_num_steps: int = 50\n    \"\"\"Number of sampler inference steps.\"\"\"\n    sample_eta: float = 1.0\n    \"\"\"Eta parameter for the DDIM sampler.\"\"\"\n    sample_guidance_scale: float = 5.0\n    \"\"\"Classifier-free guidance weight.\"\"\"\n    sample_batch_size: int = 1\n    \"\"\"Batch size (per GPU!) to use for sampling.\"\"\"\n    sample_num_batches_per_epoch: int = 2\n    \"\"\"Number of batches to sample per epoch.\"\"\"\n    train_batch_size: int = 1\n    \"\"\"Batch size (per GPU!) to use for training.\"\"\"\n    train_use_8bit_adam: bool = False\n    \"\"\"Whether to use the 8bit Adam optimizer from bitsandbytes.\"\"\"\n    train_learning_rate: float = 3e-4\n    \"\"\"Learning rate.\"\"\"\n    train_adam_beta1: float = 0.9\n    \"\"\"Adam beta1.\"\"\"\n    train_adam_beta2: float = 0.999\n    \"\"\"Adam beta2.\"\"\"\n    train_adam_weight_decay: float = 1e-4\n    \"\"\"Adam weight decay.\"\"\"\n    train_adam_epsilon: float = 1e-8\n    \"\"\"Adam epsilon.\"\"\"\n    train_gradient_accumulation_steps: int = 1\n    \"\"\"Number of gradient accumulation steps.\"\"\"\n    train_max_grad_norm: float = 1.0\n    \"\"\"Maximum gradient norm for gradient clipping.\"\"\"\n    train_num_inner_epochs: int = 1\n    \"\"\"Number of inner epochs per outer epoch.\"\"\"\n    train_cfg: bool = True\n    \"\"\"Whether or not to use classifier-free guidance during training.\"\"\"\n    train_adv_clip_max: float = 5\n    \"\"\"Clip advantages to the range.\"\"\"\n    train_clip_range: float = 1e-4\n    \"\"\"The PPO clip range.\"\"\"\n    train_timestep_fraction: float = 1.0\n    \"\"\"The fraction of timesteps to train on.\"\"\"\n    per_prompt_stat_tracking: bool = False\n    \"\"\"Whether to track statistics for each prompt separately.\"\"\"\n    per_prompt_stat_tracking_buffer_size: int = 16\n    \"\"\"Number of reward values to store in the buffer for each prompt.\"\"\"\n    per_prompt_stat_tracking_min_count: int = 16\n    \"\"\"The minimum number of reward values to store in the buffer.\"\"\"\n    async_reward_computation: bool = False\n    \"\"\"Whether to compute rewards asynchronously.\"\"\"\n    max_workers: int = 2\n    \"\"\"The maximum number of workers to use for async reward computation.\"\"\"\n    negative_prompts: Optional[str] = \"\"\n    \"\"\"Comma-separated list of prompts to use as negative examples.\"\"\"\n\n    def to_dict(self):\n        output_dict = {}\n        for key, value in self.__dict__.items():\n            output_dict[key] = value\n        return flatten_dict(output_dict)\n\n    def __post_init__(self):\n        if self.log_with not in [\"wandb\", \"tensorboard\"]:\n            warnings.warn(\n                (\"Accelerator tracking only supports image logging if `log_with` is set to 'wandb' or 'tensorboard'.\")\n            )\n\n        if self.log_with == \"wandb\" and not is_torchvision_available():\n            warnings.warn(\"Wandb image logging requires torchvision to be installed\")\n\n        if self.train_use_8bit_adam and not is_bitsandbytes_available():\n            raise ImportError(\n                \"You need to install bitsandbytes to use 8bit Adam. \"\n                \"You can install it with `pip install bitsandbytes`.\"\n            )\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/trainer/__init__.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# flake8: noqa\n\n\n\n# There is a circular import in the PPOTrainer if we let isort sort these\n# isort: off\nfrom .utils import (\n    AdaptiveKLController,\n    FixedKLController,\n    ConstantLengthDataset,\n    DataCollatorForCompletionOnlyLM,\n    RunningMoments,\n    disable_dropout_in_model,\n    peft_module_casting_to_bf16,\n)\n\n# isort: on\n\nfrom ..import_utils import is_diffusers_available\nfrom .base import BaseTrainer\nfrom .ddpo_config import DDPOConfig\n\n\nif is_diffusers_available():\n    from .ddpo_trainer import DDPOTrainer\n\nfrom .dpo_trainer import DPOTrainer\nfrom .iterative_sft_trainer import IterativeSFTTrainer\nfrom .ppo_config import PPOConfig\nfrom .ppo_trainer import PPOTrainer\nfrom .reward_config import RewardConfig\nfrom .reward_trainer import RewardTrainer, compute_accuracy\nfrom .sft_trainer import SFTTrainer\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/trainer/reward_config.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from dataclasses import dataclass\nfrom typing import Optional\n\nfrom transformers import TrainingArguments\n\n\n@dataclass\nclass RewardConfig(TrainingArguments):\n    \"\"\"\n    RewardConfig collects all training arguments related to the [`RewardTrainer`] class.\n\n    Using [`HfArgumentParser`] we can turn this class into\n    [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the\n    command line.\n\n    Parameters:\n        max_length (`int`, *optional*, defaults to `None`):\n            The maximum length of the sequences in the batch. This argument is required if you want to use the default data collator.\n        gradient_checkpointing (`bool`, *optional*, defaults to `True`):\n                If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n    \"\"\"\n\n    max_length: Optional[int] = None\n    \"\"\"The maximum length of the sequences in the batch. This argument is required if you want to use the default data collator.\"\"\"\n    gradient_checkpointing: Optional[bool] = True\n    \"\"\"If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"\"\"\n    gradient_checkpointing_kwargs: Optional[dict] = None\n    \"\"\"Keyword arguments to pass to the gradient checkpointing function.\"\"\"\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/trainer/iterative_sft_trainer.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import warnings\nfrom typing import Callable, Dict, List, Optional, Tuple, Union\n\nimport torch\nfrom datasets import Dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    DataCollator,\n    DataCollatorForLanguageModeling,\n    DataCollatorForSeq2Seq,\n    PreTrainedModel,\n    PreTrainedTokenizerBase,\n    Trainer,\n    TrainingArguments,\n)\nfrom transformers.trainer_utils import EvalLoopOutput\n\nfrom ..core import PPODecorators\nfrom ..import_utils import is_peft_available\n\n\nif is_peft_available():\n    from peft import PeftModel\n\n\nclass IterativeSFTTrainer(Trainer):\n    \"\"\"\n    The IterativeSFTTrainer can be used to finetune models with methods that requires some steps between optimization.\n\n    Attributes:\n        **model** (`PreTrainedModel`) -- Model to be optimized, either an 'AutoModelForCausalLM' or an 'AutoModelForSeq2SeqLM'.\n            Check the documentation of `PreTrainedModel` for more details.\n        **args** (`transformers.TrainingArguments`): -- The arguments to use for training.\n        **tokenizer** (`PreTrainedTokenizerBase`) -- Tokenizer to be used for encoding the\n            data. Check the documentation of `transformers.PreTrainedTokenizer` and\n            `transformers.PreTrainedTokenizerFast` for more details.\n        **optimizers** (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`): -- The optimizer and scheduler to use for training.\n        **data_collator** (Union[DataCollatorForLanguageModeling, DataCollatorForSeq2Seq], *optional*) -- Data collator to be used for training and\n            passed along the dataloader.\n        **eval_dataset** (`datasets.Dataset`): The dataset to use for evaluation.\n        **max_length** (`int`, defaults to `None`): -- The maximum length of the input.\n        **truncation_mode** (`str`, defaults to `keep_end`): -- The truncation mode to use, either `keep_end` or `keep_start`.\n        **preprocess_logits_for_metrics** (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`): -- The function to use to preprocess the logits before computing the metrics.\n        **compute_metrics** (`Callable[[EvalPrediction], Dict]`, *optional*): -- The function to use to compute the metrics. Must take a `EvalPrediction` and return a dictionary string to metric values.\n        **optimize_device_cache ** (`bool`, *optional*, defaults to `False`) -- Optimize CUDA cache for slightly more memory-efficient training.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: PreTrainedModel = None,\n        args: TrainingArguments = None,\n        tokenizer: PreTrainedTokenizerBase = None,\n        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (\n            None,\n            None,\n        ),\n        data_collator: Optional[DataCollator] = None,\n        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n        max_length: Optional[int] = None,\n        truncation_mode: Optional[str] = \"keep_end\",\n        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n        compute_metrics: Optional[Callable[[EvalLoopOutput], Dict]] = None,\n        optimize_device_cache: Optional[bool] = False,\n    ):\n        # Step 0: check positional arguments validity\n        if not isinstance(tokenizer, (PreTrainedTokenizerBase)):\n            raise ValueError(\n                f\"tokenizer must be a PreTrainedTokenizerBase like a PreTrainedTokenizer or a PreTrainedTokenizerFast, got {type(tokenizer)}\"\n            )\n        if not isinstance(model, PreTrainedModel):\n            raise ValueError(f\"model must be a PreTrainedModel, got {type(model)}\")\n        if not model.can_generate():\n            warnings.warn(\n                f\"The current model class {type(model)} is not compatible with `.generate()`\"\n                \"Please make sure that this is intended.\"\n            )\n        if optimizers[1] is None and args.max_steps == -1:\n            raise ValueError(\n                \"When no scheduler is provided, you need to set the total number of training steps to perform `max_steps`\"\n            )\n\n        self.is_encoder_decoder = getattr(model.config, \"is_encoder_decoder\", False)\n        self.is_peft_model = is_peft_available() and isinstance(model, PeftModel)\n\n        self.tokenizer = tokenizer\n\n        if data_collator is None:\n            if self.is_encoder_decoder:\n                warnings.warn(\n                    \"No data collator is provided. Using 'DataCollatorForSeq2Seq' with\"\n                    \"'labels_pad_token_id' set to '-100' and 'pad_to_multiple_of' set to 8.\"\n                )\n                self.data_collator = DataCollatorForSeq2Seq(tokenizer, label_pad_token_id=-100, pad_to_multiple_of=8)\n            else:\n                warnings.warn(\"No data collator is provided. Using 'DataCollatorForLanguageModeling'\")\n                self.data_collator = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n        else:\n            self.data_collator = data_collator\n\n        self.max_length = max_length\n        self.truncation_mode = truncation_mode\n        self.optimize_device_cache = optimize_device_cache\n\n        super().__init__(\n            model=model,\n            args=args,\n            data_collator=self.data_collator,\n            eval_dataset=eval_dataset,\n            tokenizer=tokenizer,\n            compute_metrics=compute_metrics,\n            optimizers=optimizers,\n            preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n        )\n\n        self.create_optimizer_and_scheduler(self.args.max_steps)\n\n        # prepare model, optimizer and lr_scheduler\n        self.model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(\n            self.model, self.optimizer, self.lr_scheduler\n        )\n\n        self.tokenizer.truncation_side = \"left\" if self.truncation_mode == \"keep_end\" else \"right\"\n\n        if not hasattr(self, \"accelerator\"):\n            raise AttributeError(\n                \"Your `Trainer` does not have an `accelerator` object. Consider upgrading `transformers`.\"\n            )\n\n        PPODecorators.optimize_device_cache = self.optimize_device_cache\n\n    def prepare_model_inputs(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor):\n        if attention_mask is None:\n            attention_mask = [torch.ones_like(ids) for ids in input_ids]\n\n        if self.is_encoder_decoder:\n            input_data = self.data_collator(\n                [\n                    {\"input_ids\": ids, \"attention_mask\": att, \"labels\": lab}\n                    for ids, att, lab in zip(input_ids, attention_mask, labels)\n                ]\n            ).to(self.model.device)\n\n            input_data.pop(\"decoder_input_ids\", None)  # This is directly computed inside the model\n\n            input_data[\"labels\"][input_data[\"labels\"] == self.tokenizer.pad_token_id] = -100\n\n        else:\n            input_data = self.data_collator(\n                [{\"input_ids\": ids, \"attention_mask\": att} for ids, att in zip(input_ids, attention_mask)]\n            ).to(self.model.device)\n\n        # truncate in case the user has provided input_ids, attention_mask and labels\n        if self.max_length is not None:\n            if self.truncation_mode == \"keep_start\":\n                input_data = {k: v[: self.max_length] for k, v in input_data.items()}\n            elif self.truncation_mode == \"keep_end\":\n                input_data = {k: v[-self.max_length :] for k, v in input_data.items()}\n            else:\n                raise ValueError(f\"Unknown truncation mode: {self.truncation_mode}\")\n\n        return input_data\n\n    @staticmethod\n    def _step_safety_checker(\n        input_ids: List[torch.LongTensor],\n        attention_mask: List[torch.LongTensor],\n        labels: List[torch.LongTensor],\n        texts: List[str],\n        texts_labels: List[str],\n    ):\n        \"\"\"\n        Check if the input data is valid for training.\n\n        Args:\n            input_ids (List[`torch.LongTensor`]):\n                List of tensors containing the input_ids\n            attention_mask (List[`torch.LongTensor`]):\n                List of tensors containing the attention_mask\n            labels (List[`torch.FloatTensor`]):\n                List of tensors containing the labels\n            texts (List[`str`]):\n                List of string containing the text input.\n            texts_labels (List[`str`]):\n                List of string containing the text labels.\n        Returns:\n            `tuple`: The input data.\n        \"\"\"\n        if texts is None:\n            if attention_mask is None:\n                for name, tensor_list in zip([\"input_ids\", \"labels\"], [input_ids, labels]):\n                    if not isinstance(tensor_list, list):\n                        raise ValueError(f\"{name} must be a list of tensors - got {type(tensor_list)}\")\n                    if not isinstance(tensor_list[0], torch.Tensor):\n                        raise ValueError(f\"Elements in {name} must be tensors - got {type(tensor_list[0])}\")\n            else:\n                for name, tensor_list in zip(\n                    [\"input_ids\", \"attention_mask\", \"labels\"], [input_ids, attention_mask, labels]\n                ):\n                    if not isinstance(tensor_list, list):\n                        raise ValueError(f\"{name} must be a list of tensors - got {type(tensor_list)}\")\n                    if not isinstance(tensor_list[0], torch.Tensor):\n                        raise ValueError(f\"Elements in {name} must be tensors - got {type(tensor_list[0])}\")\n        else:\n            if not isinstance(texts, list):\n                raise ValueError(f\"'text' must be a list of strings - got {type(texts)}\")\n            if not isinstance(texts[0], str):\n                raise ValueError(f\"Elements in 'text' must be strings - got {type(texts[0])}\")\n            if texts_labels is not None:\n                if not isinstance(texts_labels, list):\n                    raise ValueError(f\"'text_labels' must be a list of strings - got {type(texts_labels)}\")\n                if not isinstance(texts_labels[0], str):\n                    raise ValueError(f\"Elements in 'text_labels' must be strings - got {type(texts_labels[0])}\")\n\n        return input_ids, attention_mask, labels, texts, texts_labels\n\n    @PPODecorators.empty_device_cache()\n    def step(\n        self,\n        input_ids: Optional[List[torch.LongTensor]] = None,\n        attention_mask: Optional[List[torch.LongTensor]] = None,\n        labels: Optional[List[torch.LongTensor]] = None,\n        texts: Optional[List[str]] = None,\n        texts_labels: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        Run an optimisation step given a list of input_ids, attention_mask, and labels or a list of text and text_labels.\n        Args:\n            input_ids (List[`torch.LongTensor`]):\n                List of tensors containing the input_ids (if not provided, text will be used)\n            attention_mask (List[`torch.LongTensor`], , *optional*):\n                List of tensors containing the attention_mask\n            labels (List[`torch.FloatTensor`], *optional*):\n                List of tensors containing the labels (if set to None, will default to input_ids)\n            texts (List[`str`], *optional*):\n                List of strings containing the text input (if not provided, input_ids will directly be used)\n            texts_labels (List[`str`], *optional*):\n                List of strings containing the text labels (if set to None, will default to text)\n        Returns:\n            `dict[str, Any]`: A summary of the training statistics\n        \"\"\"\n        self.model.train()\n\n        if self.state.global_step == 0:\n            self.tr_loss = torch.tensor(0.0).to(self.args.device)\n            self._globalstep_last_logged = self.state.global_step\n\n        if input_ids is None and texts is None:\n            raise ValueError(\"Step should include `input_ids` or `texts` as keyword arguments.\")\n        elif input_ids is not None and texts is not None:\n            warnings.warn(\n                \"Both 'input_ids' and 'texts' are provided. 'input_ids' will be overwritten using inputs provided by the 'texts' keyword argument.\"\n            )\n\n        if labels is None and texts_labels is None and self.is_encoder_decoder:\n            raise ValueError(\n                \"No 'labels' or 'text_labels' are provided. When using an encoder-decoder architecture, 'labels' or 'text_labels' must be passed.\"\n            )\n\n        input_ids, attention_mask, labels, texts, texts_labels = self._step_safety_checker(\n            input_ids, attention_mask, labels, texts, texts_labels\n        )\n\n        if texts is not None:\n            model_inputs = self.tokenizer(\n                texts, max_length=self.max_length, truncation=True, padding=True, return_tensors=\"pt\"\n            )\n\n            input_ids, attention_mask = model_inputs[\"input_ids\"], model_inputs[\"attention_mask\"]\n\n        if texts_labels is not None:\n            labels = self.tokenizer(\n                texts, max_length=self.max_length, truncation=True, padding=True, return_tensors=\"pt\"\n            )[\"input_ids\"]\n\n        if labels is None:\n            warnings.warn(\"No labels are provided. Setting labels to input_ids\")\n            labels = input_ids\n\n        model_inputs = self.prepare_model_inputs(input_ids, attention_mask, labels)\n\n        model_inputs_names = list(model_inputs.keys())\n\n        batch_dict = {}\n        batch_dict.update(model_inputs)\n\n        def collator(data):\n            return_dict = dict()\n            for key in data[0]:\n                if key in [\"input_ids\", \"attention_mask\", \"labels\"]:\n                    return_dict[key] = torch.stack([d[key] for d in data]).to(self.model.device)\n            return return_dict\n\n        batch_data = Dataset.from_dict(batch_dict)\n        batch_data.set_format(\"torch\")\n\n        step_dataloader = DataLoader(\n            batch_data,\n            batch_size=self.args.per_device_train_batch_size,\n            shuffle=True,\n            collate_fn=collator,\n        )\n\n        for _, batch in enumerate(step_dataloader):\n            with self.accelerator.accumulate(self.model):\n                model_inputs = {k: batch[k] for k in model_inputs_names}\n                loss = self.compute_loss(self.model, model_inputs)\n\n                if self.args.n_gpu > 1:\n                    loss = loss.mean()\n\n                tr_loss_step = loss.detach()\n\n                self.accelerator.backward(loss)\n\n                if self.accelerator.sync_gradients and self.args.max_grad_norm is not None:\n                    self.accelerator.clip_grad_norm_(\n                        self.model.parameters(),\n                        self.args.max_grad_norm,\n                    )\n\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n                if self.lr_scheduler is not None:\n                    self.lr_scheduler.step()\n\n                self.state.global_step += 1\n\n                # update stats etc\n                self.tr_loss += tr_loss_step\n\n                self._maybe_log_save_evaluate()\n\n    def _maybe_log_save_evaluate(self):\n        # check if eval is required\n        if self.args.eval_steps is not None:\n            if self.state.global_step % self.args.eval_steps == 0 and self.state.global_step != 0:\n                self.evaluate(self.eval_dataset)\n\n        # check if logging is required\n        if self.args.logging_steps is not None:\n            if self.state.global_step % self.args.logging_steps == 0 and self.state.global_step != 0:\n                logs: Dict[str, float] = {}\n\n                tr_loss_scalar = self._nested_gather(self.tr_loss).mean().item()\n\n                # reset tr_loss to zero\n                self.tr_loss -= self.tr_loss\n\n                logs[\"loss\"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)\n                logs[\"learning_rate\"] = self._get_learning_rate()\n\n                self._globalstep_last_logged = self.state.global_step\n\n                self.log(logs)\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/trainer/dpo_trainer.py",
        "recommendation": "The code file is too long to analyze. Please select a shorter file.",
        "code_snippet": "import inspect\nimport random\nimport warnings\nfrom collections import defaultdict\nfrom contextlib import contextmanager, nullcontext\nfrom copy import deepcopy\nfrom functools import wraps\nfrom typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom accelerate.utils import is_deepspeed_available, tqdm\nfrom datasets import Dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoModelForCausalLM,\n    DataCollator,\n    PreTrainedModel,\n    PreTrainedTokenizerBase,\n    Trainer,\n    TrainingArguments,\n)\nfrom transformers.trainer_callback import TrainerCallback\nfrom transformers.trainer_utils import EvalLoopOutput\n\nfrom ..import_utils import is_peft_available, is_wandb_available\nfrom ..models import PreTrainedModelWrapper, create_reference_model\nfrom .utils import (\n    DPODataCollatorWithPadding,\n    disable_dropout_in_model,\n    pad_to_length,\n    peft_module_casting_to_bf16,\n    trl_sanitze_kwargs_for_tagging,\n)\n\n\nif is_peft_available():\n    from peft import PeftModel, get_peft_model, prepare_model_for_kbit_training\n\n\nif is_wandb_available():\n    import wandb\n\nif is_deepspeed_available():\n    import deepspeed\n\n\nclass DPOTrainer(Trainer):\n    r\"\"\"\n    Initialize DPOTrainer.\n\n    Args:\n        model (`transformers.PreTrainedModel`):\n            The model to train, preferably an `AutoModelForSequenceClassification`.\n        ref_model (`PreTrainedModelWrapper`):\n            Hugging Face transformer model with a casual language modelling head. Used for implicit reward computation and loss. If no\n            reference model is provided, the trainer will create a reference model with the same architecture as the model to be optimized.\n        beta (`float`, defaults to 0.1):\n            The beta factor in DPO loss. Higher beta means less divergence from the initial policy. For the IPO loss, beta is the regularization parameter denoted by tau in the paper.\n        label_smoothing (`float`, defaults to 0):\n            The robust DPO label smoothing parameter from the [cDPO](https://ericmitchell.ai/cdpo.pdf) report that should be between 0 and 0.5.\n        loss_type (`str`, defaults to `\"sigmoid\"`):\n            The type of DPO loss to use. Either `\"sigmoid\"` the default DPO loss,`\"hinge\"` loss from [SLiC](https://arxiv.org/abs/2305.10425) paper, `\"ipo\"` from [IPO](https://arxiv.org/abs/2310.12036) paper, or `\"kto\"` from the HALOs [report](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf).\n        args (`transformers.TrainingArguments`):\n            The arguments to use for training.\n        data_collator (`transformers.DataCollator`):\n            The data collator to use for training. If None is specified, the default data collator (`DPODataCollatorWithPadding`) will be used\n            which will pad the sequences to the maximum length of the sequences in the batch, given a dataset of paired sequences.\n        label_pad_token_id (`int`, defaults to `-100`):\n            The label pad token id. This argument is required if you want to use the default data collator.\n        padding_value (`int`, defaults to `0`):\n            The padding value if it is different to the tokenizer's pad_token_id.\n        truncation_mode (`str`, defaults to `keep_end`):\n            The truncation mode to use, either `keep_end` or `keep_start`. This argument is required if you want to use the default data collator.\n        train_dataset (`datasets.Dataset`):\n            The dataset to use for training.\n        eval_dataset (`datasets.Dataset`):\n            The dataset to use for evaluation.\n        tokenizer (`transformers.PreTrainedTokenizerBase`):\n            The tokenizer to use for training. This argument is required if you want to use the default data collator.\n        model_init (`Callable[[], transformers.PreTrainedModel]`):\n            The model initializer to use for training. If None is specified, the default model initializer will be used.\n        callbacks (`List[transformers.TrainerCallback]`):\n            The callbacks to use for training.\n        optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`):\n            The optimizer and scheduler to use for training.\n        preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`):\n            The function to use to preprocess the logits before computing the metrics.\n        max_length (`int`, defaults to `None`):\n            The maximum length of the sequences in the batch. This argument is required if you want to use the default data collator.\n        max_prompt_length (`int`, defaults to `None`):\n            The maximum length of the prompt. This argument is required if you want to use the default data collator.\n        max_target_length (`int`, defaults to `None`):\n            The maximum length of the target. This argument is required if you want to use the default data collator and your model is an encoder-decoder.\n        peft_config (`Dict`, defaults to `None`):\n            The PEFT configuration to use for training. If you pass a PEFT configuration, the model will be wrapped in a PEFT model.\n        is_encoder_decoder (`Optional[bool]`, `optional`, defaults to `None`):\n            If no model is provided, we need to know if the model_init returns an encoder-decoder.\n        disable_dropout (`bool`, defaults to `True`):\n            Whether or not to disable dropouts in `model` and `ref_model`.\n        generate_during_eval (`bool`, defaults to `False`):\n            Whether to sample and log generations during evaluation step.\n        compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):\n            The function to use to compute the metrics. Must take a `EvalPrediction` and return\n            a dictionary string to metric values.\n        precompute_ref_log_probs (`bool`, defaults to `False`):\n            Flag to precompute reference model log probabilities and evaluation datasets. This is useful if you want to train\n            without the reference model and reduce the total GPU memory needed.\n        model_init_kwargs: (`Optional[Dict]`, *optional*):\n            Dict of Optional kwargs to pass when instantiating the model from a string\n        ref_model_init_kwargs: (`Optional[Dict]`, *optional*):\n            Dict of Optional kwargs to pass when instantiating the ref model from a string\n        model_adapter_name (`str`, defaults to `None`):\n            Name of the train target PEFT adapter, when using LoRA with multiple adapters.\n        ref_adapter_name (`str`, defaults to `None`):\n            Name of the reference PEFT adapter, when using LoRA with multiple adapters.\n    \"\"\"\n\n    _tag_names = [\"trl\", \"dpo\"]\n\n    def __init__(\n        self,\n        model: Union[PreTrainedModel, nn.Module, str] = None,\n        ref_model: Optional[Union[PreTrainedModel, nn.Module, str]] = None,\n        beta: float = 0.1,\n        label_smoothing: float = 0,\n        loss_type: Literal[\"sigmoid\", \"hinge\", \"ipo\", \"kto\"] = \"sigmoid\",\n        args: TrainingArguments = None,\n        data_collator: Optional[DataCollator] = None,\n        label_pad_token_id: int = -100,\n        padding_value: int = None,\n        truncation_mode: str = \"keep_end\",\n        train_dataset: Optional[Dataset] = None,\n        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n        model_init: Optional[Callable[[], PreTrainedModel]] = None,\n        callbacks: Optional[List[TrainerCallback]] = None,\n        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),\n        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n        max_length: Optional[int] = None,\n        max_prompt_length: Optional[int] = None,\n        max_target_length: Optional[int] = None,\n        peft_config: Optional[Dict] = None,\n        is_encoder_decoder: Optional[bool] = None,\n        disable_dropout: bool = True,\n        generate_during_eval: bool = False,\n        compute_metrics: Optional[Callable[[EvalLoopOutput], Dict]] = None,\n        precompute_ref_log_probs: bool = False,\n        model_init_kwargs: Optional[Dict] = None,\n        ref_model_init_kwargs: Optional[Dict] = None,\n        model_adapter_name: str = None,\n        ref_adapter_name: str = None,\n    ):\n        if model_init_kwargs is None:\n            model_init_kwargs = {}\n        elif not isinstance(model, str):\n            raise ValueError(\"You passed model_kwargs to the DPOTrainer. But your model is already instantiated.\")\n\n        if ref_model_init_kwargs is None:\n            ref_model_init_kwargs = {}\n        elif not isinstance(ref_model, str):\n            raise ValueError(\n                \"You passed ref_model_kwargs to the DPOTrainer. But your ref_model is already instantiated.\"\n            )\n\n        if isinstance(model, str):\n            warnings.warn(\n                \"You passed a model_id to the DPOTrainer. This will automatically create an \"\n                \"`AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\"\n            )\n            model = AutoModelForCausalLM.from_pretrained(model, **model_init_kwargs)\n\n        if isinstance(ref_model, str):\n            warnings.warn(\n                \"You passed a ref model_id to the DPOTrainer. This will automatically create an \"\n                \"`AutoModelForCausalLM`\"\n            )\n            ref_model = AutoModelForCausalLM.from_pretrained(ref_model, **ref_model_init_kwargs)\n\n        # Initialize this variable to False. This helps tracking the case when `peft_module_casting_to_bf16`\n        # has been called in order to properly call autocast if needed.\n        self._peft_has_been_casted_to_bf16 = False\n\n        if not is_peft_available() and peft_config is not None:\n            raise ValueError(\n                \"PEFT is not installed and you passed a `peft_config` in the trainer's kwargs, please install it to use the PEFT models\"\n            )\n        elif is_peft_available() and peft_config is not None:\n            # if model is a peft model and we have a peft_config, we merge and unload it first\n            if isinstance(model, PeftModel):\n                model = model.merge_and_unload()\n\n            if getattr(model, \"is_loaded_in_8bit\", False) or getattr(model, \"is_loaded_in_4bit\", False):\n                _support_gc_kwargs = hasattr(\n                    args, \"gradient_checkpointing_kwargs\"\n                ) and \"gradient_checkpointing_kwargs\" in list(\n                    inspect.signature(prepare_model_for_kbit_training).parameters\n                )\n\n                preprare_model_kwargs = {\"use_gradient_checkpointing\": args.gradient_checkpointing}\n\n                if _support_gc_kwargs:\n                    preprare_model_kwargs[\"gradient_checkpointing_kwargs\"] = args.gradient_checkpointing_kwargs\n\n                model = prepare_model_for_kbit_training(model, **preprare_model_kwargs)\n            elif getattr(args, \"gradient_checkpointing\", False):\n                # For backward compatibility with older versions of transformers\n                if hasattr(model, \"enable_input_require_grads\"):\n                    model.enable_input_require_grads()\n                else:\n\n                    def make_inputs_require_grad(module, input, output):\n                        output.requires_grad_(True)\n\n                    model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n            # get peft model with the given config\n            model = get_peft_model(model, peft_config)\n            if args.bf16 and getattr(model, \"is_loaded_in_4bit\", False):\n                peft_module_casting_to_bf16(model)\n                # If args.bf16 we need to explicitly call `generate` with torch amp autocast context manager\n                self._peft_has_been_casted_to_bf16 = True\n\n        # For models that use gradient_checkpoiting, we need to attach a hook that enables input\n        # to explicitly have `requires_grad=True`, otherwise training will either silently\n        # fail or completely fail.\n        elif getattr(args, \"gradient_checkpointing\", False):\n            # For backward compatibility with older versions of transformers\n            if hasattr(model, \"enable_input_require_grads\"):\n                model.enable_input_require_grads()\n            else:\n\n                def make_inputs_require_grad(module, input, output):\n                    output.requires_grad_(True)\n\n                model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n        if generate_during_eval and not is_wandb_available():\n            raise ValueError(\n                \"`generate_during_eval=True` requires Weights and Biases to be installed.\"\n                \" Please install `wandb` to resolve.\"\n            )\n\n        if model is not None:\n            self.is_encoder_decoder = model.config.is_encoder_decoder\n        elif is_encoder_decoder is None:\n            raise ValueError(\"When no model is provided, you need to pass the parameter is_encoder_decoder.\")\n        else:\n            self.is_encoder_decoder = is_encoder_decoder\n\n        self.is_peft_model = is_peft_available() and isinstance(model, PeftModel)\n        self.model_adapter_name = model_adapter_name\n        self.ref_adapter_name = ref_adapter_name\n\n        if ref_model:\n            self.ref_model = ref_model\n        elif self.is_peft_model or precompute_ref_log_probs:\n            # The `model` with adapters turned off will be used as the reference model\n            self.ref_model = None\n        else:\n            self.ref_model = create_reference_model(model)\n\n        if tokenizer is None:\n            raise ValueError(\"tokenizer must be specified to tokenize a DPO dataset.\")\n        if max_length is None:\n            warnings.warn(\n                \"`max_length` is not set in the DPOTrainer's init\"\n                \" it will default to `512` by default, but you should do it yourself in the future.\",\n                UserWarning,\n            )\n            max_length = 512\n        if max_prompt_length is None:\n            warnings.warn(\n                \"`max_prompt_length` is not set in the DPOTrainer's init\"\n                \" it will default to `128` by default, but you should do it yourself in the future.\",\n                UserWarning,\n            )\n            max_prompt_length = 128\n\n        if max_target_length is None and self.is_encoder_decoder:\n            warnings.warn(\n                \"When using an encoder decoder architecture, you should set `max_target_length` in the DPOTrainer's init\"\n                \" it will default to `128` by default, but you should do it yourself in the future.\",\n                UserWarning,\n            )\n            max_target_length = 128\n\n        if data_collator is None:\n            data_collator = DPODataCollatorWithPadding(\n                pad_token_id=tokenizer.pad_token_id,\n                label_pad_token_id=label_pad_token_id,\n                is_encoder_decoder=self.is_encoder_decoder,\n            )\n\n            if args.remove_unused_columns:\n                args.remove_unused_columns = False\n                # warn users\n                warnings.warn(\n                    \"When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments\"\n                    \" we have set it for you, but you should do it yourself in the future.\",\n                    UserWarning,\n                )\n\n            self.use_dpo_data_collator = True\n        else:\n            self.use_dpo_data_collator = False\n\n        if disable_dropout:\n            disable_dropout_in_model(model)\n            if self.ref_model is not None:\n                disable_dropout_in_model(self.ref_model)\n\n        self.max_length = max_length\n        self.generate_during_eval = generate_during_eval\n        self.label_pad_token_id = label_pad_token_id\n        self.padding_value = padding_value if padding_value is not None else tokenizer.pad_token_id\n        self.max_prompt_length = max_prompt_length\n        self.truncation_mode = truncation_mode\n        self.max_target_length = max_target_length\n        self.tokenizer = tokenizer\n        self.precompute_ref_log_probs = precompute_ref_log_probs\n\n        # Since ref_logs are precomputed on the first call to get_train/eval_dataloader\n        # keep track of first called to avoid computation of future calls\n        self._precomputed_train_ref_log_probs = False\n        self._precomputed_eval_ref_log_probs = False\n\n        if loss_type in [\"hinge\", \"ipo\", \"kto_pair\"] and label_smoothing > 0:\n            warnings.warn(\n                \"You are using a loss type that does not support label smoothing. Ignoring label_smoothing parameter.\"\n            )\n\n        self.beta = beta\n        self.label_smoothing = label_smoothing\n        self.loss_type = loss_type\n\n        self._stored_metrics = defaultdict(lambda: defaultdict(list))\n\n        # tokenize the dataset\n        train_dataset = train_dataset.map(self.tokenize_row)\n        if eval_dataset is not None:\n            eval_dataset = eval_dataset.map(self.tokenize_row)\n\n        super().__init__(\n            model=model,\n            args=args,\n            data_collator=data_collator,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            tokenizer=tokenizer,\n            model_init=model_init,\n            compute_metrics=compute_metrics,\n            callbacks=callbacks,\n            optimizers=optimizers,\n            preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n        )\n\n        if not hasattr(self, \"accelerator\"):\n            raise AttributeError(\n                \"Your `Trainer` does not have an `accelerator` object. Consider upgrading `transformers`.\"\n            )\n\n        # Deepspeed Zero-3 does not support precompute_ref_log_probs\n        if self.is_deepspeed_enabled:\n            if self.accelerator.state.deepspeed_plugin.zero_stage == 3 and self.precompute_ref_log_probs:\n                raise ValueError(\n                    \"You cannot use `precompute_ref_log_probs=True` with Deepspeed ZeRO-3. Please set `precompute_ref_log_probs=False`.\"\n                )\n\n        if self.ref_model is None:\n            if not (self.is_peft_model or self.precompute_ref_log_probs):\n                raise ValueError(\n                    \"No reference model and model is not a Peft model. Try setting `precompute_ref_log_probs=True`\"\n                )\n        else:\n            if self.is_deepspeed_enabled:\n                self.ref_model = self._prepare_deepspeed(self.ref_model)\n            else:\n                self.ref_model = self.accelerator.prepare_model(self.ref_model, evaluation_mode=True)\n\n    def _prepare_deepspeed(self, model: PreTrainedModelWrapper):\n        # Adapted from accelerate: https://github.com/huggingface/accelerate/blob/739b135f8367becb67ffaada12fe76e3aa60fefd/src/accelerate/accelerator.py#L1473\n        deepspeed_plugin = self.accelerator.state.deepspeed_plugin\n        config_kwargs = deepcopy(deepspeed_plugin.deepspeed_config)\n\n        if model is not None:\n            if hasattr(model, \"config\"):\n                hidden_size = (\n                    max(model.config.hidden_sizes)\n                    if getattr(model.config, \"hidden_sizes\", None)\n                    else getattr(model.config, \"hidden_size\", None)\n                )\n                if hidden_size is not None and config_kwargs[\"zero_optimization\"][\"stage\"] == 3:\n                    # Note that `stage3_prefetch_bucket_size` can produce DeepSpeed messages like: `Invalidate trace cache @ step 0: expected module 1, but got module 0`\n                    # This is expected and is not an error, see: https://github.com/microsoft/DeepSpeed/discussions/4081\n                    config_kwargs.update(\n                        {\n                            \"zero_optimization.reduce_bucket_size\": hidden_size * hidden_size,\n                            \"zero_optimization.stage3_param_persistence_threshold\": 10 * hidden_size,\n                            \"zero_optimization.stage3_prefetch_bucket_size\": 0.9 * hidden_size * hidden_size,\n                        }\n                    )\n\n        # If ZeRO-3 is used, we shard both the active and reference model.\n        # Otherwise, we assume the reference model fits in memory and is initialized on each device with ZeRO disabled (stage 0)\n        if config_kwargs[\"zero_optimization\"][\"stage\"] != 3:\n            config_kwargs[\"zero_optimization\"][\"stage\"] = 0\n        model, *_ = deepspeed.initialize(model=model, config=config_kwargs)\n        model.eval()\n        return model\n\n    def get_train_dataloader(self) -> DataLoader:\n        \"\"\"\n        Returns the training [`~torch.utils.data.DataLoader`].\n\n        Subclass of transformers.src.transformers.trainer.get_train_dataloader to precompute `ref_log_probs`.\n        \"\"\"\n\n        if self.precompute_ref_log_probs and not self._precomputed_train_ref_log_probs:\n            dataloader_params = {\n                \"batch_size\": self.args.per_device_train_batch_size,\n                \"collate_fn\": self.data_collator,\n                \"num_workers\": self.args.dataloader_num_workers,\n                \"pin_memory\": self.args.dataloader_pin_memory,\n                \"shuffle\": False,\n            }\n\n            # prepare dataloader\n            data_loader = self.accelerator.prepare(DataLoader(self.train_dataset, **dataloader_params))\n\n            reference_chosen_logps = []\n            reference_rejected_logps = []\n            for padded_batch in tqdm(iterable=data_loader, desc=\"Train dataset reference log probs\"):\n                reference_chosen_logp, reference_rejected_logp = self.compute_reference_log_probs(padded_batch)\n                reference_chosen_logp, reference_rejected_logp = self.accelerator.gather_for_metrics(\n                    (reference_chosen_logp, reference_rejected_logp)\n                )\n                reference_chosen_logps.append(reference_chosen_logp.cpu())\n                reference_rejected_logps.append(reference_rejected_logp.cpu())\n\n            all_reference_chosen_logps = torch.cat(reference_chosen_logps).float().numpy()\n            all_reference_rejected_logps = torch.cat(reference_rejected_logps).float().numpy()\n\n            self.train_dataset = self.train_dataset.add_column(\n                name=\"reference_chosen_logps\", column=all_reference_chosen_logps\n            )\n            self.train_dataset = self.train_dataset.add_column(\n                name=\"reference_rejected_logps\", column=all_reference_rejected_logps\n            )\n\n            self._precomputed_train_ref_log_probs = True\n\n        return super().get_train_dataloader()\n\n    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:\n        \"\"\"\n        Returns the evaluation [`~torch.utils.data.DataLoader`].\n\n        Subclass of transformers.src.transformers.trainer.get_eval_dataloader to precompute `ref_log_probs`.\n\n        Args:\n            eval_dataset (`torch.utils.data.Dataset`, *optional*):\n                If provided, will override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns not accepted\n                by the `model.forward()` method are automatically removed. It must implement `__len__`.\n        \"\"\"\n        if eval_dataset is None and self.eval_dataset is None:\n            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n\n        if self.precompute_ref_log_probs and not self._precomputed_eval_ref_log_probs:\n            dataloader_params = {\n                \"batch_size\": self.args.per_device_eval_batch_size,\n                \"collate_fn\": self.data_collator,\n                \"num_workers\": self.args.dataloader_num_workers,\n                \"pin_memory\": self.args.dataloader_pin_memory,\n                \"shuffle\": False,\n            }\n\n            # prepare dataloader\n            data_loader = self.accelerator.prepare(DataLoader(eval_dataset, **dataloader_params))\n\n            reference_chosen_logps = []\n            reference_rejected_logps = []\n            for padded_batch in tqdm(iterable=data_loader, desc=\"Eval dataset reference log probs\"):\n                reference_chosen_logp, reference_rejected_logp = self.compute_reference_log_probs(padded_batch)\n                reference_chosen_logp, reference_rejected_logp = self.accelerator.gather_for_metrics(\n                    (reference_chosen_logp, reference_rejected_logp)\n                )\n                reference_chosen_logps.append(reference_chosen_logp.cpu())\n                reference_rejected_logps.append(reference_rejected_logp.cpu())\n\n            all_reference_chosen_logps = torch.cat(reference_chosen_logps).float().numpy()\n            all_reference_rejected_logps = torch.cat(reference_rejected_logps).float().numpy()\n\n            eval_dataset = eval_dataset.add_column(name=\"reference_chosen_logps\", column=all_reference_chosen_logps)\n            eval_dataset = eval_dataset.add_column(\n                name=\"reference_rejected_logps\", column=all_reference_rejected_logps\n            )\n\n            # Save calculated reference_chosen_logps and reference_rejected_logps to the eval_dataset for subsequent runs\n            if self.eval_dataset is not None:\n                self.eval_dataset = eval_dataset\n            self._precomputed_eval_ref_log_probs = True\n\n        return super().get_eval_dataloader(eval_dataset=eval_dataset)\n\n    def build_tokenized_answer(self, prompt, answer):\n        \"\"\"\n        Llama tokenizer does satisfy `enc(a + b) = enc(a) + enc(b)`.\n        It does ensure `enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]`.\n        Reference:\n            https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257\n        \"\"\"\n\n        full_tokenized = self.tokenizer(prompt + answer, add_special_tokens=False)\n        prompt_input_ids = self.tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n\n        answer_input_ids = full_tokenized[\"input_ids\"][len(prompt_input_ids) :]\n        answer_attention_mask = full_tokenized[\"attention_mask\"][len(prompt_input_ids) :]\n\n        # Concat tokens to form `enc(a) + enc(a + b)[len(enc(a)):]`\n        full_concat_input_ids = np.concatenate([prompt_input_ids, answer_input_ids])\n\n        # Prepare input tokens for token by token comparison\n        full_input_ids = np.array(full_tokenized[\"input_ids\"])\n\n        if len(full_input_ids) != len(full_concat_input_ids):\n            raise ValueError(\"Prompt input ids and answer input ids should have the same length.\")\n\n        # On some tokenizers, like Llama-2 tokenizer, there are occasions where tokens\n        # can be merged together when tokenizing prompt+answer. This could result\n        # on the last token from the prompt being different when tokenized on its own\n        # vs when done as prompt+answer.\n        response_token_ids_start_idx = len(prompt_input_ids)\n\n        # If tokenized prompt is different than both prompt+answer, then it means the\n        # last token has changed due to merging.\n        if prompt_input_ids != full_tokenized[\"input_ids\"][:response_token_ids_start_idx]:\n            response_token_ids_start_idx -= 1\n\n        prompt_input_ids = full_tokenized[\"input_ids\"][:response_token_ids_start_idx]\n        prompt_attention_mask = full_tokenized[\"attention_mask\"][:response_token_ids_start_idx]\n\n        if len(prompt_input_ids) != len(prompt_attention_mask):\n            raise ValueError(\"Prompt input ids and attention mask should have the same length.\")\n\n        answer_input_ids = full_tokenized[\"input_ids\"][response_token_ids_start_idx:]\n        answer_attention_mask = full_tokenized[\"attention_mask\"][response_token_ids_start_idx:]\n\n        return dict(\n            prompt_input_ids=prompt_input_ids,\n            prompt_attention_mask=prompt_attention_mask,\n            input_ids=answer_input_ids,\n            attention_mask=answer_attention_mask,\n        )\n\n    def tokenize_row(self, feature, model: Union[PreTrainedModel, nn.Module] = None) -> Dict:\n        \"\"\"Tokenize a single row from a DPO specific dataset.\n\n        At this stage, we don't convert to PyTorch tensors yet; we just handle the truncation\n        in case the prompt + chosen or prompt + rejected responses is/are too long. First\n            we truncate the prompt; if we're still too long, we truncate the chosen/rejected.\n\n        We also create the labels for the chosen/rejected responses, which are of length equal to\n            the sum of the length of the prompt and the chosen/rejected response, with\n            label_pad_token_id  for the prompt tokens.\n        \"\"\"\n        batch = {}\n        prompt = feature[\"prompt\"]\n        chosen = feature[\"chosen\"]\n        rejected = feature[\"rejected\"]\n\n        if not self.is_encoder_decoder:\n            # Check issues below for more details\n            #  1. https://github.com/huggingface/trl/issues/907\n            #  2. https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257\n            #  3. https://github.com/LianjiaTech/BELLE/issues/337\n\n            if not isinstance(prompt, str):\n                raise ValueError(f\"prompt should be an str but got {type(prompt)}\")\n            prompt_tokens = self.tokenizer(prompt, add_special_tokens=False)\n            prompt_tokens = {f\"prompt_{k}\": v for k, v in prompt_tokens.items()}\n\n            if not isinstance(chosen, str):\n                raise ValueError(f\"chosen should be an str but got {type(chosen)}\")\n            chosen_tokens = self.build_tokenized_answer(prompt, chosen)\n\n            if not isinstance(rejected, str):\n                raise ValueError(f\"rejected should be an str but got {type(rejected)}\")\n            rejected_tokens = self.build_tokenized_answer(prompt, rejected)\n\n            # Last prompt token might get merged by tokenizer and\n            # it should not be included for generation if that happens\n            prompt_len_input_ids = len(prompt_tokens[\"prompt_input_ids\"])\n\n            chosen_prompt_len_input_ids = len(chosen_tokens[\"prompt_input_ids\"])\n            rejected_prompt_len_input_ids = len(rejected_tokens[\"prompt_input_ids\"])\n            prompt_len_input_ids = min(chosen_prompt_len_input_ids, rejected_prompt_len_input_ids)\n\n            for k, v in prompt_tokens.items():\n                prompt_tokens[k] = v[:prompt_len_input_ids]\n\n            # Make sure prompts only have one different token at most an\n            # and length only differs by 1 at most\n            num_diff_tokens = sum(\n                [a != b for a, b in zip(chosen_tokens[\"prompt_input_ids\"], rejected_tokens[\"prompt_input_ids\"])]\n            )\n            num_diff_len = abs(chosen_prompt_len_input_ids - rejected_prompt_len_input_ids)\n            if num_diff_tokens > 1 or num_diff_len > 1:\n                raise ValueError(\n                    \"Chosen and rejected prompt_input_ids might only differ on the \"\n                    \"last token due to tokenizer merge ops.\"\n                )\n\n            # add BOS token to head of prompt\n            prompt_tokens[\"prompt_input_ids\"] = [self.tokenizer.bos_token_id] + prompt_tokens[\"prompt_input_ids\"]\n            chosen_tokens[\"prompt_input_ids\"] = [self.tokenizer.bos_token_id] + chosen_tokens[\"prompt_input_ids\"]\n            rejected_tokens[\"prompt_input_ids\"] = [self.tokenizer.bos_token_id] + rejected_tokens[\"prompt_input_ids\"]\n\n            prompt_tokens[\"prompt_attention_mask\"] = [1] + prompt_tokens[\"prompt_attention_mask\"]\n            chosen_tokens[\"prompt_attention_mask\"] = [1] + chosen_tokens[\"prompt_attention_mask\"]\n            rejected_tokens[\"prompt_attention_mask\"] = [1] + rejected_tokens[\"prompt_attention_mask\"]\n\n            # add EOS token to end of answer\n            chosen_tokens[\"input_ids\"].append(self.tokenizer.eos_token_id)\n            chosen_tokens[\"attention_mask\"].append(1)\n\n            rejected_tokens[\"input_ids\"].append(self.tokenizer.eos_token_id)\n            rejected_tokens[\"attention_mask\"].append(1)\n\n            longer_response_length = max(len(chosen_tokens[\"input_ids\"]), len(rejected_tokens[\"input_ids\"]))\n\n            # if combined sequence is too long, truncate the prompt\n            for answer_tokens in [chosen_tokens, rejected_tokens, prompt_tokens]:\n                if len(answer_tokens[\"prompt_input_ids\"]) + longer_response_length > self.max_length:\n                    if self.truncation_mode == \"keep_start\":\n                        for k in [\"prompt_input_ids\", \"prompt_attention_mask\"]:\n                            answer_tokens[k] = answer_tokens[k][: self.max_prompt_length]\n                    elif self.truncation_mode == \"keep_end\":\n                        for k in [\"prompt_input_ids\", \"prompt_attention_mask\"]:\n                            answer_tokens[k] = answer_tokens[k][-self.max_prompt_length :]\n                    else:\n                        raise ValueError(f\"Unknown truncation mode: {self.truncation_mode}\")\n\n            # if that's still too long, truncate the response\n            for answer_tokens in [chosen_tokens, rejected_tokens]:\n                if len(answer_tokens[\"prompt_input_ids\"]) + longer_response_length > self.max_length:\n                    for k in [\"input_ids\", \"attention_mask\"]:\n                        answer_tokens[k] = answer_tokens[k][: self.max_length - self.max_prompt_length]\n\n            # Create labels\n            chosen_sequence_tokens = {\n                k: chosen_tokens[f\"prompt_{k}\"] + chosen_tokens[k] for k in [\"input_ids\", \"attention_mask\"]\n            }\n            rejected_sequence_tokens = {\n                k: rejected_tokens[f\"prompt_{k}\"] + rejected_tokens[k] for k in [\"input_ids\", \"attention_mask\"]\n            }\n            chosen_sequence_tokens[\"labels\"] = chosen_sequence_tokens[\"input_ids\"][:]\n            chosen_sequence_tokens[\"labels\"][: len(chosen_tokens[\"prompt_input_ids\"])] = [\n                self.label_pad_token_id\n            ] * len(chosen_tokens[\"prompt_input_ids\"])\n            rejected_sequence_tokens[\"labels\"] = rejected_sequence_tokens[\"input_ids\"][:]\n            rejected_sequence_tokens[\"labels\"][: len(rejected_tokens[\"prompt_input_ids\"])] = [\n                self.label_pad_token_id\n            ] * len(rejected_tokens[\"prompt_input_ids\"])\n\n            for k, toks in {\n                \"chosen_\": chosen_sequence_tokens,\n                \"rejected_\": rejected_sequence_tokens,\n                \"\": prompt_tokens,\n            }.items():\n                for type_key, tokens in toks.items():\n                    if type_key == \"token_type_ids\":\n                        continue\n                    batch[f\"{k}{type_key}\"] = tokens\n\n        else:\n            chosen_tokens = self.tokenizer(\n                chosen, truncation=True, max_length=self.max_target_length, add_special_tokens=True\n            )\n            rejected_tokens = self.tokenizer(\n                rejected, truncation=True, max_length=self.max_target_length, add_special_tokens=True\n            )\n            prompt_tokens = self.tokenizer(\n                prompt, truncation=True, max_length=self.max_prompt_length, add_special_tokens=True\n            )\n\n            batch[\"chosen_labels\"] = chosen_tokens[\"input_ids\"]\n            batch[\"rejected_labels\"] = rejected_tokens[\"input_ids\"]\n            batch[\"prompt_input_ids\"] = prompt_tokens[\"input_ids\"]\n            batch[\"prompt_attention_mask\"] = prompt_tokens[\"attention_mask\"]\n\n            if model is not None and hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n                batch[\"rejected_decoder_input_ids\"] = model.prepare_decoder_input_ids_from_labels(\n                    labels=batch[\"rejected_labels\"]\n                )\n                batch[\"chosen_decoder_input_ids\"] = model.prepare_decoder_input_ids_from_labels(\n                    labels=batch[\"chosen_labels\"]\n                )\n\n        return batch\n\n    @contextmanager\n    def null_ref_context(self):\n        \"\"\"Context manager for handling null reference model (that is, peft adapter manipulation).\"\"\"\n        with self.accelerator.unwrap_model(\n            self.model\n        ).disable_adapter() if self.is_peft_model and not self.ref_adapter_name else nullcontext():\n            if self.ref_adapter_name:\n                self.model.set_adapter(self.ref_adapter_name)\n            yield\n            if self.ref_adapter_name:\n                self.model.set_adapter(self.model_adapter_name or \"default\")\n\n    def compute_reference_log_probs(self, padded_batch: Dict) -> Dict:\n        \"\"\"Computes log probabilities of the reference model for a single padded batch of a DPO specific dataset.\"\"\"\n        compte_ref_context_manager = torch.cuda.amp.autocast if self._peft_has_been_casted_to_bf16 else nullcontext\n\n        # compute reference logps\n        with torch.no_grad(), compte_ref_context_manager():\n            if self.ref_model is None:\n                with self.null_ref_context():\n                    (\n                        reference_chosen_logps,\n                        reference_rejected_logps,\n                        _,\n                        _,\n                    ) = self.concatenated_forward(self.model, padded_batch)\n            else:\n                (\n                    reference_chosen_logps,\n                    reference_rejected_logps,\n                    _,\n                    _,\n                ) = self.concatenated_forward(self.ref_model, padded_batch)\n\n        return reference_chosen_logps, reference_rejected_logps\n\n    @staticmethod\n    def concatenated_inputs(\n        batch: Dict[str, Union[List, torch.LongTensor]],\n        is_encoder_decoder: bool = False,\n        label_pad_token_id: int = -100,\n        padding_value: int = 0,\n        device: Optional[torch.device] = None,\n    ) -> Dict[str, torch.LongTensor]:\n        \"\"\"Concatenate the chosen and rejected inputs into a single tensor.\n\n        Args:\n            batch: A batch of data. Must contain the keys 'chosen_input_ids' and 'rejected_input_ids', which are tensors of shape (batch_size, sequence_length).\n            is_encoder_decoder: Whether the model is an encoder-decoder model.\n            label_pad_token_id: The label pad token id.\n            padding_value: The padding value to use for the concatenated inputs_ids.\n            device: The device for the concatenated inputs.\n\n        Returns:\n            A dictionary containing the concatenated inputs under the key 'concatenated_input_ids'.\n        \"\"\"\n        concatenated_batch = {}\n\n        if is_encoder_decoder:\n            max_length = max(batch[\"chosen_labels\"].shape[1], batch[\"rejected_labels\"].shape[1])\n        else:\n            max_length = max(batch[\"chosen_input_ids\"].shape[1], batch[\"rejected_input_ids\"].shape[1])\n\n        for k in batch:\n            if k.startswith(\"chosen\") and isinstance(batch[k], torch.Tensor):\n                if \"labels\" in k or is_encoder_decoder:\n                    pad_value = label_pad_token_id\n                elif k.endswith(\"_input_ids\"):\n                    pad_value = padding_value\n                elif k.endswith(\"_attention_mask\"):\n                    pad_value = 0\n                concatenated_key = k.replace(\"chosen\", \"concatenated\")\n                concatenated_batch[concatenated_key] = pad_to_length(batch[k], max_length, pad_value=pad_value)\n        for k in batch:\n            if k.startswith(\"rejected\") and isinstance(batch[k], torch.Tensor):\n                if \"labels\" in k or is_encoder_decoder:\n                    pad_value = label_pad_token_id\n                elif k.endswith(\"_input_ids\"):\n                    pad_value = padding_value\n                elif k.endswith(\"_attention_mask\"):\n                    pad_value = 0\n                concatenated_key = k.replace(\"rejected\", \"concatenated\")\n                concatenated_batch[concatenated_key] = torch.cat(\n                    (\n                        concatenated_batch[concatenated_key],\n                        pad_to_length(batch[k], max_length, pad_value=pad_value),\n                    ),\n                    dim=0,\n                ).to(device=device)\n\n        if is_encoder_decoder:\n            concatenated_batch[\"concatenated_input_ids\"] = batch[\"prompt_input_ids\"].repeat(2, 1).to(device=device)\n            concatenated_batch[\"concatenated_attention_mask\"] = (\n                batch[\"prompt_attention_mask\"].repeat(2, 1).to(device=device)\n            )\n\n        return concatenated_batch\n\n    def dpo_loss(\n        self,\n        policy_chosen_logps: torch.FloatTensor,\n        policy_rejected_logps: torch.FloatTensor,\n        reference_chosen_logps: torch.FloatTensor,\n        reference_rejected_logps: torch.FloatTensor,\n        reference_free: bool = False,\n    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n        \"\"\"Compute the DPO loss for a batch of policy and reference model log probabilities.\n\n        Args:\n            policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)\n            policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)\n            reference_chosen_logps: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)\n            reference_rejected_logps: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)\n            reference_free: If True, we ignore the _provided_ reference model and implicitly use a reference model that assigns equal probability to all responses.\n\n        Returns:\n            A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).\n            The losses tensor contains the DPO loss for each example in the batch.\n            The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.\n        \"\"\"\n        pi_logratios = policy_chosen_logps - policy_rejected_logps\n        if reference_free:\n            ref_logratios = 0\n        else:\n            ref_logratios = reference_chosen_logps - reference_rejected_logps\n\n        pi_logratios = pi_logratios.to(self.accelerator.device)\n        ref_logratios = ref_logratios.to(self.accelerator.device)\n        logits = pi_logratios - ref_logratios\n\n        # The beta is a temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5.\n        # We ignore the reference model as beta -> 0. The label_smoothing parameter encodes our uncertainty about the labels and\n        # calculates a conservative DPO loss.\n        if self.loss_type == \"sigmoid\":\n            losses = (\n                -F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing)\n                - F.logsigmoid(-self.beta * logits) * self.label_smoothing\n            )\n        elif self.loss_type == \"hinge\":\n            losses = torch.relu(1 - self.beta * logits)\n        elif self.loss_type == \"ipo\":\n            # eqn (17) of the paper where beta is the regularization parameter for the IPO loss, denoted by tau in the paper.\n            losses = (logits - 1 / (2 * self.beta)) ** 2\n        elif self.loss_type == \"kto_pair\":\n            # eqn (7) of the HALOs paper\n            chosen_KL = (policy_chosen_logps - reference_chosen_logps).mean().clamp(min=0)\n            rejected_KL = (policy_rejected_logps - reference_rejected_logps).mean().clamp(min=0)\n\n            chosen_logratios = policy_chosen_logps - reference_chosen_logps\n            rejected_logratios = policy_rejected_logps - reference_rejected_logps\n            # As described in the KTO report, the KL term for chosen (rejected) is estimated using the rejected (chosen) half.\n            losses = torch.cat(\n                (\n                    1 - F.sigmoid(self.beta * (chosen_logratios - rejected_KL)),\n                    1 - F.sigmoid(self.beta * (chosen_KL - rejected_logratios)),\n                ),\n                0,\n            )\n        else:\n            raise ValueError(\n                f\"Unknown loss type: {self.loss_type}. Should be one of ['sigmoid', 'hinge', 'ipo', 'kto_pair']\"\n            )\n\n        chosen_rewards = (\n            self.beta\n            * (\n                policy_chosen_logps.to(self.accelerator.device) - reference_chosen_logps.to(self.accelerator.device)\n            ).detach()\n        )\n        rejected_rewards = (\n            self.beta\n            * (\n                policy_rejected_logps.to(self.accelerator.device)\n                - reference_rejected_logps.to(self.accelerator.device)\n            ).detach()\n        )\n\n        return losses, chosen_rewards, rejected_rewards\n\n    @staticmethod\n    def get_batch_logps(\n        logits: torch.FloatTensor,\n        labels: torch.LongTensor,\n        average_log_prob: bool = False,\n        label_pad_token_id: int = -100,\n        is_encoder_decoder: bool = False,\n    ) -> torch.FloatTensor:\n        \"\"\"Compute the log probabilities of the given labels under the given logits.\n\n        Args:\n            logits: Logits of the model (unnormalized). Shape: (batch_size, sequence_length, vocab_size)\n            labels: Labels for which to compute the log probabilities. Label tokens with a value of label_pad_token_id are ignored. Shape: (batch_size, sequence_length)\n            average_log_prob: If True, return the average log probability per (non-masked) token. Otherwise, return the sum of the log probabilities of the (non-masked) tokens.\n\n        Returns:\n            A tensor of shape (batch_size,) containing the average/sum log probabilities of the given labels under the given logits.\n        \"\"\"\n        if logits.shape[:-1] != labels.shape:\n            raise ValueError(\"Logits (batch and sequence length dim) and labels must have the same shape.\")\n\n        if not is_encoder_decoder:\n            labels = labels[:, 1:].clone()\n            logits = logits[:, :-1, :]\n        loss_mask = labels != label_pad_token_id\n\n        # dummy token; we'll ignore the losses on these tokens later\n        labels[labels == label_pad_token_id] = 0\n\n        per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n\n        if average_log_prob:\n            return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n        else:\n            return (per_token_logps * loss_mask).sum(-1)\n\n    def concatenated_forward(\n        self, model: nn.Module, batch: Dict[str, Union[List, torch.LongTensor]]\n    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n        \"\"\"Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.\n\n        We do this to avoid doing two forward passes, because it's faster for FSDP.\n        \"\"\"\n        concatenated_batch = self.concatenated_inputs(\n            batch,\n            is_encoder_decoder=self.is_encoder_decoder,\n            label_pad_token_id=self.label_pad_token_id,\n            padding_value=self.padding_value,\n            device=self.accelerator.device,\n        )\n        len_chosen = batch[\"chosen_labels\"].shape[0]\n\n        model_kwargs = (\n            {\n                \"labels\": concatenated_batch[\"concatenated_labels\"],\n                \"decoder_input_ids\": concatenated_batch.pop(\"concatenated_decoder_input_ids\", None),\n            }\n            if self.is_encoder_decoder\n            else {}\n        )\n        all_logits = model(\n            concatenated_batch[\"concatenated_input_ids\"],\n            attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n            **model_kwargs,\n        ).logits\n\n        all_logps = self.get_batch_logps(\n            all_logits,\n            concatenated_batch[\"concatenated_labels\"],\n            average_log_prob=False,\n            is_encoder_decoder=self.is_encoder_decoder,\n            label_pad_token_id=self.label_pad_token_id,\n        )\n\n        chosen_logps = all_logps[:len_chosen]\n        rejected_logps = all_logps[len_chosen:]\n\n        chosen_logits = all_logits[:len_chosen]\n        rejected_logits = all_logits[len_chosen:]\n\n        return (chosen_logps, rejected_logps, chosen_logits, rejected_logits)\n\n    def get_batch_loss_metrics(\n        self,\n        model,\n        batch: Dict[str, Union[List, torch.LongTensor]],\n        train_eval: Literal[\"train\", \"eval\"] = \"train\",\n    ):\n        \"\"\"Compute the DPO loss and other metrics for the given batch of inputs for train or test.\"\"\"\n        metrics = {}\n\n        (\n            policy_chosen_logps,\n            policy_rejected_logps,\n            policy_chosen_logits,\n            policy_rejected_logits,\n        ) = self.concatenated_forward(model, batch)\n\n        # if reference_chosen_logps and reference_rejected_logps in batch use them, otherwise use the reference model\n        if \"reference_chosen_logps\" in batch and \"reference_rejected_logps\" in batch:\n            reference_chosen_logps = batch[\"reference_chosen_logps\"]\n            reference_rejected_logps = batch[\"reference_rejected_logps\"]\n        else:\n            with torch.no_grad():\n                if self.ref_model is None:\n                    with self.null_ref_context():\n                        (\n                            reference_chosen_logps,\n                            reference_rejected_logps,\n                            _,\n                            _,\n                        ) = self.concatenated_forward(self.model, batch)\n                else:\n                    (\n                        reference_chosen_logps,\n                        reference_rejected_logps,\n                        _,\n                        _,\n                    ) = self.concatenated_forward(self.ref_model, batch)\n\n        losses, chosen_rewards, rejected_rewards = self.dpo_loss(\n            policy_chosen_logps,\n            policy_rejected_logps,\n            reference_chosen_logps,\n            reference_rejected_logps,\n        )\n        reward_accuracies = (chosen_rewards > rejected_rewards).float()\n\n        prefix = \"eval_\" if train_eval == \"eval\" else \"\"\n        metrics[f\"{prefix}rewards/chosen\"] = chosen_rewards.mean().cpu()\n        metrics[f\"{prefix}rewards/rejected\"] = rejected_rewards.mean().cpu()\n        metrics[f\"{prefix}rewards/accuracies\"] = reward_accuracies.mean().cpu()\n        metrics[f\"{prefix}rewards/margins\"] = (chosen_rewards - rejected_rewards).mean().cpu()\n        metrics[f\"{prefix}logps/rejected\"] = policy_rejected_logps.detach().mean().cpu()\n        metrics[f\"{prefix}logps/chosen\"] = policy_chosen_logps.detach().mean().cpu()\n        metrics[f\"{prefix}logits/rejected\"] = policy_rejected_logits.detach().mean().cpu()\n        metrics[f\"{prefix}logits/chosen\"] = policy_chosen_logits.detach().mean().cpu()\n\n        return losses.mean(), metrics\n\n    def compute_loss(\n        self,\n        model: Union[PreTrainedModel, nn.Module],\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        return_outputs=False,\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict[str, torch.Tensor]]]:\n        if not self.use_dpo_data_collator:\n            warnings.warn(\n                \"compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than \"\n                \"DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator\"\n            )\n\n        compute_loss_context_manager = torch.cuda.amp.autocast if self._peft_has_been_casted_to_bf16 else nullcontext\n\n        with compute_loss_context_manager():\n            loss, metrics = self.get_batch_loss_metrics(model, inputs, train_eval=\"train\")\n\n        # force log the metrics\n        if self.accelerator.is_main_process:\n            self.store_metrics(metrics, train_eval=\"train\")\n\n        if return_outputs:\n            return (loss, metrics)\n        return loss\n\n    def get_batch_samples(self, model, batch: Dict[str, torch.LongTensor]) -> Tuple[str, str]:\n        \"\"\"Generate samples from the model and reference model for the given batch of inputs.\"\"\"\n\n        # If one uses `generate_during_eval` with peft + bf16, we need to explictly call generate with\n        # the torch cuda amp context manager as some hidden states are silently casted to full precision.\n        generate_context_manager = nullcontext if not self._peft_has_been_casted_to_bf16 else torch.cuda.amp.autocast\n\n        with generate_context_manager():\n            policy_output = model.generate(\n                input_ids=batch[\"prompt_input_ids\"],\n                attention_mask=batch[\"prompt_attention_mask\"],\n                max_length=self.max_length,\n                do_sample=True,\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n\n            # if reference_output in batch use that otherwise use the reference model\n            if \"reference_output\" in batch:\n                reference_output = batch[\"reference_output\"]\n            else:\n                if self.ref_model is None:\n                    with self.null_ref_context():\n                        reference_output = self.model.generate(\n                            input_ids=batch[\"prompt_input_ids\"],\n                            attention_mask=batch[\"prompt_attention_mask\"],\n                            max_length=self.max_length,\n                            do_sample=True,\n                            pad_token_id=self.tokenizer.pad_token_id,\n                        )\n                else:\n                    reference_output = self.ref_model.generate(\n                        input_ids=batch[\"prompt_input_ids\"],\n                        attention_mask=batch[\"prompt_attention_mask\"],\n                        max_length=self.max_length,\n                        do_sample=True,\n                        pad_token_id=self.tokenizer.pad_token_id,\n                    )\n\n        policy_output = pad_to_length(policy_output, self.max_length, self.tokenizer.pad_token_id)\n        policy_output_decoded = self.tokenizer.batch_decode(policy_output, skip_special_tokens=True)\n\n        reference_output = pad_to_length(reference_output, self.max_length, self.tokenizer.pad_token_id)\n        reference_output_decoded = self.tokenizer.batch_decode(reference_output, skip_special_tokens=True)\n\n        return policy_output_decoded, reference_output_decoded\n\n    def prediction_step(\n        self,\n        model: Union[PreTrainedModel, nn.Module],\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        prediction_loss_only: bool,\n        ignore_keys: Optional[List[str]] = None,\n    ):\n        if not self.use_dpo_data_collator:\n            warnings.warn(\n                \"prediction_step is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than \"\n                \"DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator\"\n            )\n        if ignore_keys is None:\n            if hasattr(model, \"config\"):\n                ignore_keys = getattr(model.config, \"keys_to_ignore_at_inference\", [])\n            else:\n                ignore_keys = []\n\n        prediction_context_manager = torch.cuda.amp.autocast if self._peft_has_been_casted_to_bf16 else nullcontext\n\n        with torch.no_grad(), prediction_context_manager():\n            loss, metrics = self.get_batch_loss_metrics(model, inputs, train_eval=\"eval\")\n\n        # force log the metrics\n        if self.accelerator.is_main_process:\n            self.store_metrics(metrics, train_eval=\"eval\")\n\n        if prediction_loss_only:\n            return (loss.detach(), None, None)\n\n        # logits for the chosen and rejected samples from model\n        logits_dict = {\n            \"eval_logits/chosen\": metrics[\"eval_logits/chosen\"],\n            \"eval_logits/rejected\": metrics[\"eval_logits/rejected\"],\n        }\n        logits = tuple(v.unsqueeze(dim=0) for k, v in logits_dict.items() if k not in ignore_keys)\n        logits = torch.stack(logits).mean(axis=1).to(self.accelerator.device)\n        labels = torch.zeros(logits.shape[0], device=self.accelerator.device)\n\n        return (loss.detach(), logits, labels)\n\n    def store_metrics(self, metrics: Dict[str, float], train_eval: Literal[\"train\", \"eval\"] = \"train\") -> None:\n        for key, value in metrics.items():\n            self._stored_metrics[train_eval][key].append(value)\n\n    def evaluation_loop(\n        self,\n        dataloader: DataLoader,\n        description: str,\n        prediction_loss_only: Optional[bool] = None,\n        ignore_keys: Optional[List[str]] = None,\n        metric_key_prefix: str = \"eval\",\n    ) -> EvalLoopOutput:\n        \"\"\"\n        Overriding built-in evaluation loop to store metrics for each batch.\n        Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n\n        Works both with or without labels.\n        \"\"\"\n\n        # Sample and save to game log if requested (for one batch to save time)\n        if self.generate_during_eval:\n            # Generate random indices within the range of the total number of samples\n            num_samples = len(dataloader.dataset)\n            random_indices = random.sample(range(num_samples), k=self.args.eval_batch_size)\n\n            # Use dataloader.dataset.select to get the random batch without iterating over the DataLoader\n            random_batch_dataset = dataloader.dataset.select(random_indices)\n            random_batch = self.data_collator(random_batch_dataset)\n            random_batch = self._prepare_inputs(random_batch)\n\n            policy_output_decoded, ref_output_decoded = self.get_batch_samples(self.model, random_batch)\n\n            self.log(\n                {\n                    \"game_log\": wandb.Table(\n                        columns=[\"Prompt\", \"Policy\", \"Ref Model\"],\n                        rows=[\n                            [prompt, pol[len(prompt) :], ref[len(prompt) :]]\n                            for prompt, pol, ref in zip(\n                                random_batch[\"prompt\"], policy_output_decoded, ref_output_decoded\n                            )\n                        ],\n                    )\n                }\n            )\n            self.state.log_history.pop()\n\n        # Base evaluation\n        initial_output = super().evaluation_loop(\n            dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix\n        )\n\n        return initial_output\n\n    def log(self, logs: Dict[str, float]) -> None:\n        \"\"\"\n        Log `logs` on the various objects watching training, including stored metrics.\n\n        Args:\n            logs (`Dict[str, float]`):\n                The values to log.\n        \"\"\"\n        # logs either has 'loss' or 'eval_loss'\n        train_eval = \"train\" if \"loss\" in logs else \"eval\"\n        # Add averaged stored metrics to logs\n        for key, metrics in self._stored_metrics[train_eval].items():\n            logs[key] = torch.tensor(metrics).mean().item()\n        del self._stored_metrics[train_eval]\n        return super().log(logs)\n\n    @wraps(Trainer.push_to_hub)\n    def push_to_hub(self, commit_message: Optional[str] = \"End of training\", blocking: bool = True, **kwargs) -> str:\n        \"\"\"\n        Overwrite the `push_to_hub` method in order to force-add the tag \"sft\" when pushing the\n        model on the Hub. Please refer to `~transformers.Trainer.push_to_hub` for more details.\n        \"\"\"\n        kwargs = trl_sanitze_kwargs_for_tagging(model=self.model, tag_names=self._tag_names, kwargs=kwargs)\n\n        return super().push_to_hub(commit_message=commit_message, blocking=blocking, **kwargs)\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/trainer/ppo_trainer.py",
        "recommendation": "The code file is too long to analyze. Please select a shorter file.",
        "code_snippet": "\nimport inspect\nimport math\nimport os\nimport time\nimport typing\nimport warnings\nfrom contextlib import nullcontext\nfrom typing import Callable, List, Optional, Union\n\nimport datasets\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom accelerate import Accelerator\nfrom accelerate.utils import ProjectConfiguration, gather_object, is_deepspeed_available\nfrom datasets import Dataset\nfrom huggingface_hub import whoami\nfrom packaging import version\nfrom torch.optim import Adam\nfrom transformers import (\n    DataCollatorForLanguageModeling,\n    PreTrainedTokenizer,\n    PreTrainedTokenizerBase,\n    PreTrainedTokenizerFast,\n)\n\nfrom ..core import (\n    WANDB_PADDING,\n    PPODecorators,\n    clip_by_value,\n    convert_to_scalar,\n    entropy_from_logits,\n    flatten_dict,\n    logprobs_from_logits,\n    masked_mean,\n    masked_var,\n    masked_whiten,\n    set_seed,\n    stack_dicts,\n    stats_to_np,\n)\nfrom ..import_utils import is_npu_available, is_torch_greater_2_0, is_xpu_available\nfrom ..models import SUPPORTED_ARCHITECTURES, PreTrainedModelWrapper, create_reference_model\nfrom . import AdaptiveKLController, BaseTrainer, FixedKLController, PPOConfig, RunningMoments\n\n\nif is_deepspeed_available():\n    import deepspeed\n\nMODEL_CARD_TEMPLATE = \"\"\"---\nlicense: apache-2.0\ntags:\n- trl\n- ppo\n- transformers\n- reinforcement-learning\n---\n\n# {model_name}\n\nThis is a [TRL language model](https://github.com/huggingface/trl) that has been fine-tuned with reinforcement learning to\n guide the model outputs according to a value, function, or human feedback. The model can be used for text generation.\n\n## Usage\n\nTo use this model for inference, first install the TRL library:\n\n```bash\npython -m pip install trl\n```\n\nYou can then generate text as follows:\n\n```python\nfrom transformers import pipeline\n\ngenerator = pipeline(\"text-generation\", model=\"{model_id}\")\noutputs = generator(\"Hello, my llama is cute\")\n```\n\nIf you want to use the model for training or to obtain the outputs from the value head, load the model as follows:\n\n```python\nfrom transformers import AutoTokenizer\nfrom trl import AutoModelForCausalLMWithValueHead\n\ntokenizer = AutoTokenizer.from_pretrained(\"{model_id}\")\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\"{model_id}\")\n\ninputs = tokenizer(\"Hello, my llama is cute\", return_tensors=\"pt\")\noutputs = model(**inputs, labels=inputs[\"input_ids\"])\n```\n\"\"\"\n\n\nclass PPOTrainer(BaseTrainer):\n    \"\"\"\n    The PPOTrainer uses Proximal Policy Optimization to optimise language models.\n    Note, this trainer is heavily inspired by the original OpenAI learning to summarize work here:\n    https://github.com/openai/summarize-from-feedback\n\n    Attributes:\n        **config** (`PPOConfig`) -- Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more\n            details.\n        **model** (`PreTrainedModelWrapper`) -- Model to be optimized, Hugging Face transformer model with a value head.\n            Check the documentation of `PreTrainedModelWrapper` for more details.\n        **ref_model** (`PreTrainedModelWrapper`, *optional*) -- Reference model to be used for KL penalty, Hugging Face\n            transformer model with a casual language modelling head. Check the documentation of `PreTrainedModelWrapper`\n            for more details. If no reference model is provided, the trainer will create a reference model with the same\n             architecture as the model to be optimized with shared layers.\n        **tokenizer** (`PreTrainedTokenizerBase`) -- Tokenizer to be used for encoding the\n            data. Check the documentation of `transformers.PreTrainedTokenizer` and\n            `transformers.PreTrainedTokenizerFast` for more details.\n        **dataset** (Union[`torch.utils.data.Dataset`, `datasets.Dataset`], *optional*) -- PyTorch dataset or Hugging\n            Face dataset. This is used to create a PyTorch dataloader. If no dataset is provided, the dataloader must be\n             created outside the trainer users needs to design their own dataloader and make sure the batch\n            size that is used is the same as the one specified in the configuration object.\n        **optimizer** (`torch.optim.Optimizer`, *optional*) -- Optimizer to be used for training. If no optimizer is\n            provided, the trainer will create an Adam optimizer with the learning rate specified in the configuration\n            object.\n        **data_collator** (DataCollatorForLanguageModeling, *optional*) -- Data collator to be used for training and\n            passed along the dataloader\n        **num_shared_layers** (int, *optional*) -- Number of layers to be shared between the model and the reference\n            model, if no reference model is passed. If no number is provided, all the layers will be shared.\n        **lr_scheduler** (`torch.optim.lr_scheduler`, *optional*) -- Learning rate scheduler to be used for training.\n    \"\"\"\n\n    _tag_names = [\"trl\", \"ppo\"]\n\n    def __init__(\n        self,\n        config: PPOConfig = None,\n        model: PreTrainedModelWrapper = None,\n        ref_model: Optional[PreTrainedModelWrapper] = None,\n        tokenizer: PreTrainedTokenizerBase = None,\n        dataset: Optional[Union[torch.utils.data.Dataset, Dataset]] = None,\n        optimizer: Optional[torch.optim.Optimizer] = None,\n        data_collator: Optional[typing.Callable] = None,\n        num_shared_layers: Optional[int] = None,\n        lr_scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n    ):\n        \"\"\"\n        Initialize PPOTrainer.\n\n        Args:\n            config (`PPOConfig`):\n                Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more details.\n            model (`PreTrainedModelWrapper`):\n                Hugging Face transformer model with a value head.\n            ref_model (`PreTrainedModelWrapper`):\n                Hugging Face transformer model with a casual language modelling head. Used for KL penalty\n            tokenizer (`transformers.PreTrainedTokenizerBase`):\n                Hugging Face tokenizer\n            dataset (Optional[Union[`torch.utils.data.Dataset`, `datasets.Dataset`]]):\n                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset\n                will be preprocessed by removing the columns that are not used by the model. If none is passed,\n                a warning will be raised in a multi-GPU setting.\n            optimizer (Optional[`torch.optim.Optimizer`]):\n                Optimizer used for training. If `None`, the `Adam` is used as default.\n            data_collator (Optional[function]):\n                Data collator function.\n            num_shared_layers (Optional[int]):\n                Number of shared layers between the model and the reference model. If `None`, all layers are shared.\n                used only if `ref_model` is `None`.\n            lr_scheduler (Optional[`torch.optim.lr_scheduler`]):\n                Learning rate scheduler used for training.\n        \"\"\"\n        super().__init__(config)\n\n        # initial seed for reproducible experiments\n        set_seed(config.seed)\n\n        # Step 0: check positional arguments validity\n        if not isinstance(config, PPOConfig):\n            raise ValueError(f\"config must be a PPOConfig, got {type(config)}\")\n        if not isinstance(tokenizer, (PreTrainedTokenizerBase)):\n            raise ValueError(\n                f\"tokenizer must be a PreTrainedTokenizerBase like a PreTrainedTokenizer or a PreTrainedTokenizerFast, got {type(tokenizer)}\"\n            )\n        if not isinstance(model, (SUPPORTED_ARCHITECTURES)):\n            raise ValueError(\n                f\"model must be a PreTrainedModelWrapper, got {type(model)} - supported architectures are: {SUPPORTED_ARCHITECTURES}\"\n            )\n        # Step 1: Initialize Accelerator\n        self.accelerator = Accelerator(\n            log_with=config.log_with,\n            gradient_accumulation_steps=config.gradient_accumulation_steps,\n            project_config=ProjectConfiguration(**config.project_kwargs),\n            **config.accelerator_kwargs,\n        )\n\n        # Step 1.1 Runtime variables filled by the accelerator\n        config.world_size = self.accelerator.num_processes\n        config.global_backward_batch_size = config.backward_batch_size * config.world_size\n        config.global_batch_size = config.batch_size * config.world_size\n\n        self.model = model\n        self.model_params = filter(lambda p: p.requires_grad, self.model.parameters())\n        self.is_encoder_decoder = hasattr(self.model, \"is_encoder_decoder\")\n        self.is_peft_model = getattr(self.model, \"is_peft_model\", False)\n        config.is_encoder_decoder = self.is_encoder_decoder\n        config.is_peft_model = self.is_peft_model\n\n        is_using_tensorboard = config.log_with is not None and config.log_with == \"tensorboard\"\n        self.accelerator.init_trackers(\n            config.tracker_project_name,\n            config=dict(trl_ppo_trainer_config=config.to_dict()) if not is_using_tensorboard else config.to_dict(),\n            init_kwargs=config.tracker_kwargs,\n        )\n        self.is_using_text_environment = getattr(config, \"use_text_environment\", False)\n\n        if isinstance(ref_model, SUPPORTED_ARCHITECTURES):\n            self.ref_model = ref_model\n            if num_shared_layers is not None:\n                warnings.warn(\n                    \"num_shared_layers is ignored when ref_model is provided. Two different models are used for the \"\n                    \"model and the reference model and no layers are shared.\",\n                    UserWarning,\n                )\n        elif ref_model is None and not self.is_peft_model:\n            self.ref_model = create_reference_model(self.model, num_shared_layers=num_shared_layers)\n        elif self.is_peft_model:\n            self.ref_model = None\n        else:\n            raise ValueError(\n                f\"ref_model must be a PreTrainedModelWrapper or `None`, got {type(ref_model)} - supported \"\n                f\"architectures are: {SUPPORTED_ARCHITECTURES} \"\n            )\n        self.optional_peft_ctx = (\n            self.accelerator.unwrap_model(self.model).pretrained_model.disable_adapter\n            if self.is_peft_model\n            else nullcontext\n        )\n\n        if not (isinstance(tokenizer, PreTrainedTokenizer) or isinstance(tokenizer, PreTrainedTokenizerFast)):\n            raise ValueError(\n                \"tokenizer must be a transformers.PreTrainedTokenizer or transformers.PreTrainedTokenizerFast\"\n            )\n        self.tokenizer = tokenizer\n\n        if dataset is not None and not (isinstance(dataset, torch.utils.data.Dataset) or isinstance(dataset, Dataset)):\n            raise ValueError(\"dataset must be a torch.utils.data.Dataset or datasets.Dataset\")\n        elif dataset is None:\n            warnings.warn(\n                \"No dataset is provided. Make sure to set config.batch_size to the correct value before training.\",\n                UserWarning,\n            )\n        self.dataset = dataset\n        self._signature_columns = None\n        if self.dataset is not None:\n            self.dataloader = self.prepare_dataloader(self.dataset, data_collator)\n        elif self.dataset is None and self.accelerator.num_processes > 1:\n            warnings.warn(\n                \"No dataset is provided. In a multi-GPU setting, this will lead to an error. You should\"\n                \" prepare your dataloader yourself with `dataloader = ppo_trainer.accelerator.prepare(dataloader)`\"\n                \" and using `torch.utils.data.DataLoader`, or pass a dataset to the `PPOTrainer`. Please \"\n                \" refer to the documentation for more details.\",\n                UserWarning,\n            )\n            self.dataloader = None\n        else:\n            self.dataloader = None\n\n        # Step 3: Initialize optimizer and data collator\n        self.data_collator = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n        if optimizer is None:\n            self.optimizer = Adam(\n                filter(lambda p: p.requires_grad, self.model.parameters()),\n                lr=self.config.learning_rate,\n            )\n        else:\n            self.optimizer = optimizer\n\n        self.lr_scheduler = lr_scheduler\n        if self.lr_scheduler is not None:\n            lr_scheduler_class = (\n                torch.optim.lr_scheduler._LRScheduler\n                if not is_torch_greater_2_0()\n                else torch.optim.lr_scheduler.LRScheduler\n            )\n\n            if not isinstance(self.lr_scheduler, lr_scheduler_class):\n                raise ValueError(\n                    \"lr_scheduler must be a torch.optim.lr_scheduler._LRScheduler or torch.optim.lr_scheduler.LRScheduler (for torch >= 2.0)\"\n                )\n\n        if self.config.adap_kl_ctrl:\n            self.kl_ctl = AdaptiveKLController(self.config.init_kl_coef, self.config.target, self.config.horizon)\n        else:\n            self.kl_ctl = FixedKLController(self.config.init_kl_coef)\n\n        # Safety checkers for DS integration\n        is_deepspeed_used = self.accelerator.distributed_type == \"DEEPSPEED\" and hasattr(\n            self.accelerator.state, \"deepspeed_plugin\"\n        )\n\n        (\n            self.model,\n            self.optimizer,\n            self.data_collator,\n            self.dataloader,\n            self.lr_scheduler,\n        ) = self.accelerator.prepare(\n            self.model,\n            self.optimizer,\n            self.data_collator,\n            self.dataloader,\n            self.lr_scheduler,\n        )\n        if is_deepspeed_used:\n            # Quantized models are already set on the correct device\n            if not self.is_peft_model and not (\n                getattr(self.ref_model.pretrained_model, \"is_loaded_in_8bit\", False)\n                or getattr(self.ref_model.pretrained_model, \"is_loaded_in_4bit\", False)\n            ):\n                self.ref_model = self._prepare_deepspeed(self.ref_model)\n        else:\n            self.ref_model = self.accelerator.prepare(self.ref_model)\n\n        # In a distributed setup, only logging needs to be performed on the main process\n        # check: https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html\n        # or: https://discuss.pytorch.org/t/use-distributed-data-parallel-correctly/82500/11\n        self.is_distributed = self.accelerator.num_processes > 1\n\n        # init the current step\n        self.current_step = 0\n\n        # init variables for pushing model to hub\n        if config.push_to_hub_if_best_kwargs:\n            if \"repo_id\" not in config.push_to_hub_if_best_kwargs:\n                raise ValueError(\"You have to specify repo_id in order to push the model to the hub!\")\n            self.push_to_hub_kwargs = config.push_to_hub_if_best_kwargs\n            self.compare_step = 0\n            self.highest_reward = torch.tensor(-float(\"inf\"))\n\n        # post process for PP\n        if not getattr(self.model, \"is_sequential_parallel\", False):\n            self.current_device = self.accelerator.device\n        else:\n            if is_xpu_available():\n                self.current_device = torch.device(\"xpu:0\")\n            elif is_npu_available():\n                self.current_device = torch.device(\"npu:0\")\n            else:\n                self.current_device = torch.device(\"cuda:0\")\n\n        PPODecorators.optimize_device_cache = self.config.optimize_device_cache\n\n        self.running = RunningMoments(self.accelerator)\n\n    def _filter_kwargs(self, kwargs, target_func):\n        \"\"\"\n        filter the keyword arguments that are supported by the target function.\n\n        Args:\n            kwargs (dict):\n                Keyword arguments\n            target_func (function):\n                Target function\n        \"\"\"\n        return {k: v for k, v in kwargs.items() if k in inspect.signature(target_func).parameters.keys()}\n\n    def prepare_dataloader(self, dataset: Union[torch.utils.data.Dataset, Dataset], data_collator=None):\n        \"\"\"\n        Prepare the dataloader for training.\n\n        Args:\n            dataset (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]):\n                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset\n                will be preprocessed by removing the columns that are not used by the model.\n            data_collator (Optional[function]):\n                Data collator function.\n\n        Returns:\n            `torch.utils.data.DataLoader`: PyTorch dataloader\n        \"\"\"\n        if isinstance(dataset, Dataset):\n            dataset = self._remove_unused_columns(dataset)\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=self.config.batch_size,\n            collate_fn=data_collator,\n            shuffle=True,\n            drop_last=True,\n        )\n        return dataloader\n\n    # Adapted from transformers.Trainer._set_signature_columns_if_needed\n    def _set_signature_columns_if_needed(self):\n        if self._signature_columns is None:\n            # Inspect model forward signature to keep only the arguments it accepts.\n            signature = inspect.signature(self.model.forward)\n            self._signature_columns = list(signature.parameters.keys())\n            # label => sentiment | we need query and response for logging purpose\n            self._signature_columns += [\"label\", \"query\", \"response\"]\n\n    # Adapted from transformers.Trainer._remove_unused_columns\n    def _remove_unused_columns(self, dataset: \"Dataset\"):\n        if not self.config.remove_unused_columns:\n            return dataset\n        self._set_signature_columns_if_needed()\n        signature_columns = self._signature_columns\n\n        ignored_columns = list(set(dataset.column_names) - set(signature_columns))\n\n        columns = [k for k in signature_columns if k in dataset.column_names]\n\n        if version.parse(datasets.__version__) < version.parse(\"1.4.0\"):\n            dataset.set_format(\n                type=dataset.format[\"type\"],\n                columns=columns,\n                format_kwargs=dataset.format[\"format_kwargs\"],\n            )\n            return dataset\n        else:\n            return dataset.remove_columns(ignored_columns)\n\n    def generate(\n        self,\n        query_tensor: Union[torch.Tensor, List[torch.Tensor]],\n        length_sampler: Callable = None,\n        batch_size: int = 4,\n        return_prompt: bool = True,\n        generate_ref_response: bool = False,\n        **generation_kwargs,\n    ):\n        \"\"\"\n        Generate response with the model given the query tensor.\n        call the `generate` method of the model.\n\n        Args:\n            query_tensor (`torch.LongTensor`):\n                A tensor of shape (`seq_len`) containing query tokens or a list of tensors of shape (`seq_len`).\n            generation_kwargs (dict[str, Any]):\n                Keyword arguments for generation.\n            length_sampler (`Callable`, *optional*):\n                Callable that returns the number of newly generated tokens.\n            batch_size (`int`, *optional):\n                Batch size used for generation, defaults to `4`.\n            return_prompt (`bool`, *optional*):\n                If set to `False` the prompt is not returned but only the newly generated tokens, defaults to `True`.\n            generate_ref_response (`bool`, *optional*):\n                If set to `True` the reference response is also generated, defaults to `False`.\n\n        Returns:\n            `torch.LongTensor`: A tensor of shape (`batch_size`, `gen_len`) containing response tokens.\n        \"\"\"\n        if generate_ref_response:\n            ref_model = self.model if self.is_peft_model else self.ref_model\n        if isinstance(query_tensor, List):\n            response = self._generate_batched(\n                self.model,\n                query_tensor,\n                length_sampler=length_sampler,\n                batch_size=batch_size,\n                return_prompt=return_prompt,\n                **generation_kwargs,\n            )\n            if generate_ref_response:\n                with self.optional_peft_ctx():\n                    ref_response = self._generate_batched(\n                        ref_model,\n                        query_tensor,\n                        length_sampler=length_sampler,\n                        batch_size=batch_size,\n                        return_prompt=return_prompt,\n                        **generation_kwargs,\n                    )\n\n        else:\n            if len(query_tensor.shape) == 2:\n                raise ValueError(\n                    \"query_tensor must be a tensor of shape (`seq_len`) or a list of tensors of shape (`seq_len`)\"\n                )\n\n            if length_sampler is not None:\n                generation_kwargs[\"max_new_tokens\"] = length_sampler()\n            response = self.accelerator.unwrap_model(self.model).generate(\n                input_ids=query_tensor.unsqueeze(dim=0), **generation_kwargs\n            )\n            if generate_ref_response:\n                with self.optional_peft_ctx():\n                    ref_response = ref_model.generate(input_ids=query_tensor.unsqueeze(dim=0), **generation_kwargs)\n\n            if not return_prompt and not self.is_encoder_decoder:\n                response = response[:, query_tensor.shape[0] :]\n                if generate_ref_response:\n                    ref_response = ref_response[:, query_tensor.shape[0] :]\n\n        if generate_ref_response:\n            return response, ref_response\n        return response\n\n    def _generate_batched(\n        self,\n        model: PreTrainedModelWrapper,\n        query_tensors: List[torch.Tensor],\n        length_sampler: Callable = None,\n        batch_size: int = 4,\n        return_prompt: bool = True,\n        pad_to_multiple_of: int = None,\n        remove_padding: bool = True,\n        **generation_kwargs,\n    ):\n        outputs = []\n\n        padding_side_default = self.tokenizer.padding_side\n        if not self.is_encoder_decoder:\n            self.tokenizer.padding_side = \"left\"\n\n        # in case we have fewer examples than bs\n        batch_size = min(len(query_tensors), batch_size)\n\n        for i in range(0, len(query_tensors), batch_size):\n            if length_sampler is not None:\n                generation_kwargs[\"max_new_tokens\"] = length_sampler()\n\n            # prevent overflow if query tensors are not even multiple of bs\n            end_index = min(len(query_tensors), i + batch_size)\n\n            batch = query_tensors[i:end_index]\n            batch_mask = [torch.ones_like(element) for element in batch]\n            inputs = {\"input_ids\": batch, \"attention_mask\": batch_mask}\n\n            padded_inputs = self.tokenizer.pad(\n                inputs,\n                padding=True,\n                max_length=None,\n                pad_to_multiple_of=pad_to_multiple_of,\n                return_tensors=\"pt\",\n            ).to(self.current_device)\n\n            generations = self.accelerator.unwrap_model(model).generate(**padded_inputs, **generation_kwargs)\n\n            for generation, mask in zip(generations, padded_inputs[\"attention_mask\"]):\n                if not self.is_encoder_decoder:\n                    output = generation[(1 - mask).sum() :]  # remove padding\n                else:\n                    output = generation\n\n                if not return_prompt and not self.is_encoder_decoder:\n                    output = output[(mask).sum() :]  # remove prompt\n\n                if remove_padding and self.tokenizer.eos_token_id in output:\n                    pad_mask = output == self.tokenizer.eos_token_id\n                    pad_start = torch.nonzero(pad_mask, as_tuple=False)[0, 0].item()\n                    output = output[: pad_start + 1]  # keep the eos token at the end\n\n                outputs.append(output)\n\n        self.tokenizer.padding_side = padding_side_default\n        return outputs\n\n    def _step_safety_checker(\n        self,\n        batch_size: int,\n        queries: List[torch.LongTensor],\n        responses: List[torch.LongTensor],\n        scores: List[torch.FloatTensor],\n        masks: Optional[List[torch.LongTensor]] = None,\n    ):\n        \"\"\"\n        Check if the input data is valid for training.\n\n        Args:\n            batch_size (int):\n                Batch size from the config file.\n            queries (List[`torch.LongTensor`]):\n                List of tensors containing the encoded queries of shape (`query_length`)\n            responses (List[`torch.LongTensor`]):\n                List of tensors containing the encoded responses of shape (`response_length`)\n            scores (List[`torch.FloatTensor`]):\n                List of tensors containing the scores.\n            masks (List[`torch.LongTensor`], *optional*):\n                list of optional tensors containing the masks of shape (`query_length` + `response_length`)\n        Returns:\n            `tuple`: The input processed data.\n        \"\"\"\n        for name, tensor_list in zip([\"queries\", \"responses\", \"scores\"], [queries, responses, scores]):\n            if not isinstance(tensor_list, list):\n                raise ValueError(f\"{name} must be a list of tensors - got {type(tensor_list)}\")\n            if not isinstance(tensor_list[0], torch.Tensor):\n                raise ValueError(f\"Elements in {name} must be tensors - got {type(tensor_list[0])}\")\n            if batch_size is not None and len(tensor_list) != batch_size:\n                raise ValueError(\n                    f\"Batch size ({batch_size}) does not match number of examples - but got {len(tensor_list)} for: {name}\"\n                )\n\n        # add queries, scores and responses on the correct device\n        queries = [tensor.to(self.current_device) for tensor in queries]\n        responses = [tensor.to(self.current_device) for tensor in responses]\n        scores = [tensor.to(self.current_device) for tensor in scores]\n        masks = [tensor.to(self.current_device) for tensor in masks] if masks is not None else None\n\n        # squeeze scores if needed\n        for i, score in enumerate(scores):\n            if score.dim() > 1:\n                raise ValueError(f\"Scores must be 1-dimensional - got {score.dim()} for {score}\")\n            elif score.dim() == 1:\n                scores[i] = score.squeeze()\n\n        return queries, responses, scores, masks\n\n    @PPODecorators.empty_device_cache()\n    def step(\n        self,\n        queries: List[torch.LongTensor],\n        responses: List[torch.LongTensor],\n        scores: List[torch.FloatTensor],\n        response_masks: Optional[List[torch.LongTensor]] = None,\n    ):\n        \"\"\"\n        Run a PPO optimisation step given a list of queries, model responses, and rewards.\n\n        Args:\n            queries (List[`torch.LongTensor`]):\n                List of tensors containing the encoded queries of shape (`query_length`)\n            responses (List[`torch.LongTensor`]):\n                List of tensors containing the encoded responses of shape (`response_length`)\n            scores (List[`torch.FloatTensor`]):\n                List of tensors containing the scores.\n            response_masks (List[`torch.FloatTensor`], *optional*)):\n                List of tensors containing masks of the response tokens.\n\n        Returns:\n            `dict[str, Any]`: A summary of the training statistics\n        \"\"\"\n        bs = self.config.batch_size\n\n        queries, responses, scores, response_masks = self._step_safety_checker(\n            bs, queries, responses, scores, response_masks\n        )\n        scores = torch.tensor(scores, device=self.current_device)\n        if self.config.use_score_scaling:\n            # Score scaling\n            scores_mean, scores_std = self.running.update(scores)\n            tensor_to_kwargs = dict(dtype=scores.dtype, device=scores.device)\n            score_scaling_factor = self.running.std.to(**tensor_to_kwargs) + torch.finfo(scores.dtype).eps\n            if self.config.use_score_norm:\n                scores = (scores - self.running.mean.to(**tensor_to_kwargs)) / score_scaling_factor\n            else:\n                scores /= score_scaling_factor\n\n        if self.config.score_clip is not None:\n            # Score clipping\n            scores_dtype = scores.dtype\n            scores = torch.clip(scores.float(), -self.config.score_clip, self.config.score_clip).to(dtype=scores_dtype)\n\n        # if we want to push best model to the hub\n        if hasattr(self, \"highest_reward\"):\n            if self.compare_step % self.config.compare_steps == 0:\n                curr_mean_reward = scores.mean()\n                # if the best reward ever seen\n                if curr_mean_reward > self.highest_reward:\n                    self.highest_reward = curr_mean_reward\n                    # push model to hub\n                    self.push_to_hub(**self.push_to_hub_kwargs)\n            self.compare_step += 1\n\n        timing = dict()\n        t0 = time.time()\n\n        t = time.time()\n\n        model_inputs = self.prepare_model_inputs(queries, responses)\n\n        if self.is_distributed:\n            pad_first = self.tokenizer.padding_side == \"left\"\n\n            model_inputs[\"input_ids\"] = self.accelerator.pad_across_processes(\n                model_inputs[\"input_ids\"],\n                dim=1,\n                pad_index=self.tokenizer.pad_token_id,\n                pad_first=pad_first,\n            )\n            model_inputs[\"attention_mask\"] = self.accelerator.pad_across_processes(\n                model_inputs[\"attention_mask\"], dim=1, pad_index=0, pad_first=pad_first\n            )\n            if self.is_encoder_decoder:\n                model_inputs[\"decoder_input_ids\"] = self.accelerator.pad_across_processes(\n                    model_inputs[\"decoder_input_ids\"],\n                    dim=1,\n                    pad_index=self.tokenizer.pad_token_id,\n                    pad_first=pad_first,\n                )\n                model_inputs[\"decoder_attention_mask\"] = self.accelerator.pad_across_processes(\n                    model_inputs[\"decoder_attention_mask\"],\n                    dim=1,\n                    pad_index=0,\n                    pad_first=pad_first,\n                )\n\n        model_inputs_names = list(model_inputs.keys())\n\n        full_kl_penalty = self.config.kl_penalty == \"full\"\n\n        with torch.no_grad():\n            all_logprobs, logits_or_none, values, masks = self.batched_forward_pass(\n                self.model,\n                queries,\n                responses,\n                model_inputs,\n                response_masks=response_masks,\n                return_logits=full_kl_penalty,\n            )\n            with self.optional_peft_ctx():\n                ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(\n                    self.model if self.is_peft_model else self.ref_model,\n                    queries,\n                    responses,\n                    model_inputs,\n                    return_logits=full_kl_penalty,\n                )\n\n        timing[\"time/ppo/forward_pass\"] = time.time() - t\n\n        with torch.no_grad():\n            t = time.time()\n            if full_kl_penalty:\n                active_full_logprobs = logprobs_from_logits(logits_or_none, None, gather=False)\n                ref_full_logprobs = logprobs_from_logits(ref_logits_or_none, None, gather=False)\n\n                rewards, non_score_reward, kls = self.compute_rewards(\n                    scores, active_full_logprobs, ref_full_logprobs, masks\n                )\n            else:\n                rewards, non_score_reward, kls = self.compute_rewards(scores, all_logprobs, ref_logprobs, masks)\n            timing[\"time/ppo/compute_rewards\"] = time.time() - t\n\n            t = time.time()\n            values, advantages, returns = self.compute_advantages(values, rewards, masks)\n            timing[\"time/ppo/compute_advantages\"] = time.time() - t\n\n        # upcast to float32 to avoid dataset issues\n        batch_dict = {\n            \"queries\": queries,\n            \"responses\": responses,\n            \"logprobs\": all_logprobs.to(torch.float32),\n            \"values\": values.to(torch.float32),\n            \"masks\": masks,\n            \"advantages\": advantages,\n            \"returns\": returns,\n        }\n        batch_dict.update(model_inputs)\n\n        t = time.time()\n        all_stats = []\n        early_stop = False\n        for _ in range(self.config.ppo_epochs):\n            if early_stop:\n                break\n            b_inds = np.random.permutation(bs)\n            for backward_batch_start in range(0, bs, self.config.backward_batch_size):\n                backward_batch_end = backward_batch_start + self.config.backward_batch_size\n                backward_batch_inds = b_inds[backward_batch_start:backward_batch_end]\n\n                for mini_batch_start in range(0, self.config.backward_batch_size, self.config.mini_batch_size):\n                    mini_batch_end = mini_batch_start + self.config.mini_batch_size\n                    mini_batch_inds = backward_batch_inds[mini_batch_start:mini_batch_end]\n                    mini_batch_dict = {\n                        \"logprobs\": batch_dict[\"logprobs\"][mini_batch_inds],\n                        \"values\": batch_dict[\"values\"][mini_batch_inds],\n                        \"masks\": batch_dict[\"masks\"][mini_batch_inds],\n                        # hacks: the queries and responses are ragged.\n                        \"queries\": [batch_dict[\"queries\"][i] for i in mini_batch_inds],\n                        \"responses\": [batch_dict[\"responses\"][i] for i in mini_batch_inds],\n                        \"advantages\": batch_dict[\"advantages\"][mini_batch_inds],\n                        \"returns\": batch_dict[\"returns\"][mini_batch_inds],\n                    }\n                    for k in model_inputs_names:\n                        mini_batch_dict[k] = batch_dict[k][mini_batch_inds]\n                    with self.accelerator.accumulate(self.model):\n                        model_inputs = {k: mini_batch_dict[k] for k in model_inputs_names}\n\n                        logprobs, logits, vpreds, _ = self.batched_forward_pass(\n                            self.model,\n                            mini_batch_dict[\"queries\"],\n                            mini_batch_dict[\"responses\"],\n                            model_inputs,\n                            return_logits=True,\n                        )\n                        train_stats = self.train_minibatch(\n                            mini_batch_dict[\"logprobs\"],\n                            mini_batch_dict[\"values\"],\n                            logprobs,\n                            logits,\n                            vpreds,\n                            mini_batch_dict[\"masks\"],\n                            mini_batch_dict[\"advantages\"],\n                            mini_batch_dict[\"returns\"],\n                        )\n                        all_stats.append(train_stats)\n\n            # typically, early stopping is done at the epoch level\n            if self.config.early_stopping:\n                policykl = train_stats[\"policy/policykl\"]\n                early_stop = self._early_stop(policykl)\n                if early_stop:\n                    break\n\n        timing[\"time/ppo/optimize_step\"] = time.time() - t\n\n        t = time.time()\n        train_stats = stack_dicts(all_stats)\n\n        # reshape advantages/ratios such that they are not averaged.\n        train_stats[\"policy/advantages\"] = torch.flatten(train_stats[\"policy/advantages\"]).unsqueeze(0)\n        train_stats[\"policy/advantages\"] = torch.nan_to_num(train_stats[\"policy/advantages\"], WANDB_PADDING)\n        train_stats[\"policy/ratio\"] = torch.flatten(train_stats[\"policy/ratio\"]).unsqueeze(0)\n\n        stats = self.record_step_stats(\n            scores=scores,\n            logprobs=all_logprobs,\n            ref_logprobs=ref_logprobs,\n            non_score_reward=non_score_reward,\n            train_stats=train_stats,\n            kl_coef=self.kl_ctl.value,\n            masks=masks,\n            queries=queries,\n            responses=responses,\n            kls=kls,\n        )\n        # Gather/Reduce stats from all processes\n        if self.is_distributed:\n            stats = self.gather_stats(stats)\n        stats = stats_to_np(stats)\n        timing[\"time/ppo/calc_stats\"] = time.time() - t\n        stats[\"ppo/learning_rate\"] = self.optimizer.param_groups[0][\"lr\"]\n\n        # Update the KL control - multiply the batch_size by the number of processes\n        self.kl_ctl.update(\n            stats[\"objective/kl\"],\n            self.config.batch_size * self.accelerator.num_processes,\n        )\n\n        # Log the total ppo time\n        timing[\"time/ppo/total\"] = time.time() - t0\n        stats.update(timing)\n\n        # post-process stats for tensorboard and other loggers\n        if self.config.log_with != \"wandb\":\n            stats = convert_to_scalar(stats)\n\n        if self.lr_scheduler is not None:\n            self.lr_scheduler.step()\n\n        return stats\n\n    def _early_stop(self, policykl):\n        r\"\"\"\n        Handles the early stopping logic. If the policy KL is greater than the target KL, then the gradient is zeroed and\n        the optimization step is skipped.\n        This also handles the multi-gpu case where the policy KL is averaged across all processes.\n\n        Args:\n            policy_kl (torch.Tensor):\n                the policy KL\n\n        Returns:\n            `bool`: whether to early stop or not\n        \"\"\"\n        early_stop = False\n        if not self.config.early_stopping:\n            return early_stop\n\n        if not self.is_distributed and policykl > 1.5 * self.config.target_kl:\n            self.optimizer.zero_grad()\n            early_stop = True\n        elif self.is_distributed:\n            import torch.distributed as dist\n\n            # Wait for all processes to finish\n            dist.barrier()\n\n            # all gather the policykl\n            dist.all_reduce(policykl, dist.ReduceOp.SUM)\n            policykl /= self.accelerator.num_processes\n\n            if policykl > 1.5 * self.config.target_kl:\n                self.optimizer.zero_grad()\n                early_stop = True\n        return early_stop\n\n    def gather_stats(self, stats):\n        \"\"\"\n        Gather stats from all processes. Useful in the context of distributed training.\n\n        Args:\n            stats (dict[str, Any]):\n            a dictionary of stats to be gathered. The stats should contain torch tensors.\n\n        Returns:\n            `dict[str, Any]`: A dictionary of stats with the tensors gathered.\n        \"\"\"\n        import torch.distributed as dist\n\n        # Wait for all processes to finish\n        dist.barrier()\n\n        for k, v in stats.items():\n            if isinstance(v, torch.Tensor):\n                dist.all_reduce(v.to(self.accelerator.device), dist.ReduceOp.SUM)\n                v /= self.accelerator.num_processes\n            stats[k] = v\n        return stats\n\n    def prepare_model_inputs(self, queries: torch.Tensor, responses: torch.Tensor):\n        if self.is_encoder_decoder:\n            input_data = self.data_collator(\n                [{\"input_ids\": q, \"attention_mask\": torch.ones_like(q)} for q in queries]\n            ).to(self.current_device)\n\n            decoder_inputs = self.data_collator(\n                [{\"input_ids\": r, \"attention_mask\": torch.ones_like(r)} for r in responses]\n            ).to(self.current_device)\n\n            input_data[\"decoder_input_ids\"] = decoder_inputs[\"input_ids\"]\n            input_data[\"decoder_attention_mask\"] = decoder_inputs[\"attention_mask\"]\n        else:\n            input_ids = [torch.cat([q, r]) for q, r in zip(queries, responses)]\n            input_data = self.data_collator(\n                [{\"input_ids\": ids, \"attention_mask\": torch.ones_like(ids)} for ids in input_ids]\n            ).to(self.current_device)\n\n        input_data.pop(\"labels\", None)  # we don't want to compute LM losses\n        return input_data\n\n    @PPODecorators.empty_device_cache()\n    def batched_forward_pass(\n        self,\n        model: PreTrainedModelWrapper,\n        queries: torch.Tensor,\n        responses: torch.Tensor,\n        model_inputs: dict,\n        return_logits: bool = False,\n        response_masks: Optional[torch.Tensor] = None,\n    ):\n        \"\"\"\n        Calculate model outputs in multiple batches.\n\n        Args:\n            queries (`torch.LongTensor`):\n                List of tensors containing the encoded queries, shape (`batch_size`, `query_length`)\n            responses (`torch.LongTensor`):\n                List of tensors containing the encoded responses, shape (`batch_size`, `response_length`)\n            return_logits (`bool`, *optional*, defaults to `False`):\n                Whether to return all_logits. Set to `False` if logits are not needed to reduce memory consumption.\n        Returns:\n            (tuple):\n                - all_logprobs (`torch.FloatTensor`): Log probabilities of the responses,\n                    shape (`batch_size`, `response_length`)\n                - all_ref_logprobs (`torch.FloatTensor`): Log probabilities of the responses,\n                    shape (`batch_size`, `response_length`)\n                - all_values (`torch.FloatTensor`): Values of the responses, shape (`batch_size`, `response_length`)\n        \"\"\"\n        bs = len(queries)\n        fbs = self.config.mini_batch_size\n        all_logprobs = []\n        all_logits = []\n        all_masks = []\n        all_values = []\n\n        model.eval()\n\n        for i in range(math.ceil(bs / fbs)):\n            input_kwargs = {key: value[i * fbs : (i + 1) * fbs] for key, value in model_inputs.items()}\n            query_batch = queries[i * fbs : (i + 1) * fbs]\n            response_batch = responses[i * fbs : (i + 1) * fbs]\n            if response_masks is not None:\n                response_masks_batch = response_masks[i * fbs : (i + 1) * fbs]\n            logits, _, values = model(**input_kwargs)\n\n            if self.is_encoder_decoder:\n                input_ids = input_kwargs[\"decoder_input_ids\"]\n                attention_mask = input_kwargs[\"decoder_attention_mask\"]\n            else:\n                input_ids = input_kwargs[\"input_ids\"]\n                attention_mask = input_kwargs[\"attention_mask\"]\n\n            logprobs = logprobs_from_logits(logits[:, :-1, :], input_ids[:, 1:])\n            masks = torch.zeros_like(attention_mask)\n            masks[:, :-1] = attention_mask[:, 1:]\n\n            for j in range(len(query_batch)):\n                if self.is_encoder_decoder:\n                    # Decoder sentence starts always in the index 1 after padding in the Enc-Dec Models\n                    start = 1\n                    end = attention_mask[j, :].sum() - 1\n                else:\n                    start = len(query_batch[j]) - 1  # logprobs starts from the second query token\n                    if attention_mask[j, 0] == 0:  # offset left padding\n                        start += attention_mask[j, :].nonzero()[0]\n                    end = start + len(response_batch[j])\n                    if response_masks is not None:\n                        response_masks_batch[j] = torch.cat(\n                            (torch.zeros_like(query_batch[j]), response_masks_batch[j])\n                        )[1:]\n\n                masks[j, :start] = 0\n                masks[j, end:] = 0\n                if response_masks is not None:\n                    masks[j, start:end] = masks[j, start:end] * response_masks_batch[j][start:end]\n\n            if return_logits:\n                all_logits.append(logits)\n            else:\n                del logits\n            all_values.append(values)\n            all_logprobs.append(logprobs)\n            all_masks.append(masks)\n\n        return (\n            torch.cat(all_logprobs),\n            torch.cat(all_logits)[:, :-1] if return_logits else None,\n            torch.cat(all_values)[:, :-1],\n            torch.cat(all_masks)[:, :-1],\n        )\n\n    @PPODecorators.empty_device_cache()\n    def train_minibatch(\n        self,\n        old_logprobs: torch.FloatTensor,\n        values: torch.FloatTensor,\n        logprobs: torch.FloatTensor,\n        logits: torch.FloatTensor,\n        vpreds: torch.FloatTensor,\n        mask: torch.LongTensor,\n        advantages: torch.FloatTensor,\n        returns: torch.FloatTensor,\n    ):\n        \"\"\"\n        Train one PPO minibatch\n\n        Args:\n            logprobs (`torch.FloatTensor`):\n                Log probabilities of the model, shape [mini_batch_size, response_length]\n            values (`torch.FloatTensor`):\n                Values of the value head, shape [mini_batch_size, response_length]\n            query (`torch.LongTensor`):\n                Encoded queries, shape [mini_batch_size, query_length]\n            response (`torch.LongTensor`):\n                Encoded responses, shape [mini_batch_size, response_length]\n            model_input (`torch.LongTensor`):\n                Concatenated queries and responses, shape [mini_batch_size, query_length+response_length]\n\n        Returns:\n            train_stats (dict[str, `torch.Tensor`]):\n                Dictionary of training statistics\n        \"\"\"\n        self.model.train()\n        loss_p, loss_v, train_stats = self.loss(\n            old_logprobs, values, logits, vpreds, logprobs, mask, advantages, returns\n        )\n        loss = loss_p + loss_v\n        self.accelerator.backward(loss)\n        if self.config.max_grad_norm is not None:\n            if self.accelerator.sync_gradients:\n                self.accelerator.clip_grad_norm_(self.model_params, self.config.max_grad_norm)\n        self.optimizer.step()\n        # we call optimizer.zero_grad() every time and let `accelerator` handle accumulation\n        # see https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation#the-finished-code\n        self.optimizer.zero_grad()\n        return train_stats\n\n    def compute_rewards(\n        self,\n        scores: torch.FloatTensor,\n        logprobs: torch.FloatTensor,\n        ref_logprobs: torch.FloatTensor,\n        masks: torch.LongTensor,\n    ):\n        \"\"\"\n        Compute per token rewards from scores and KL-penalty.\n\n        Args:\n            scores (`torch.FloatTensor`):\n                Scores from the reward model, shape (`batch_size`)\n            logprobs (`torch.FloatTensor`):\n                Log probabilities of the model, shape (`batch_size`, `response_length`)\n            ref_logprobs (`torch.FloatTensor`):\n                Log probabilities of the reference model, shape (`batch_size`, `response_length`)\n\n        Returns:\n            `torch.FloatTensor`: Per token rewards, shape (`batch_size`, `response_length`)\n            `torch.FloatTensor`: Non score rewards, shape (`batch_size`, `response_length`)\n            `torch.FloatTensor`: KL penalty, shape (`batch_size`, `response_length`)\n        \"\"\"\n        rewards, non_score_rewards, kls = [], [], []\n        for score, logprob, ref_logprob, mask in zip(scores, logprobs, ref_logprobs, masks):\n            # compute KL penalty (from difference in logprobs)\n            kl = self._kl_penalty(logprob, ref_logprob)\n            kls.append(kl)\n            non_score_reward = -self.kl_ctl.value * kl\n            non_score_rewards.append(non_score_reward)\n            reward = non_score_reward.clone()\n            last_non_masked_index = mask.nonzero()[-1]\n\n            # reward is preference model score + KL penalty\n            reward[last_non_masked_index] += score\n            rewards.append(reward)\n        return torch.stack(rewards), torch.stack(non_score_rewards), torch.stack(kls)\n\n    def _kl_penalty(self, logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor) -> torch.FloatTensor:\n        if self.config.kl_penalty == \"kl\":\n            return logprob - ref_logprob\n\n        if self.config.kl_penalty == \"abs\":\n            return (logprob - ref_logprob).abs()\n\n        if self.config.kl_penalty == \"mse\":\n            return 0.5 * (logprob - ref_logprob).square()\n\n        if self.config.kl_penalty == \"full\":\n            # Flip is required due to this issue? :https://github.com/pytorch/pytorch/issues/57459\n            return F.kl_div(ref_logprob, logprob, log_target=True, reduction=\"none\").sum(-1)\n\n        raise NotImplementedError\n\n    def compute_advantages(\n        self,\n        values: torch.FloatTensor,\n        rewards: torch.FloatTensor,\n        mask: torch.FloatTensor,\n    ):\n        lastgaelam = 0\n        advantages_reversed = []\n        gen_len = rewards.shape[-1]\n\n        values = values * mask\n        rewards = rewards * mask\n\n        if self.config.whiten_rewards:\n            rewards = masked_whiten(rewards, mask, shift_mean=False)\n\n        for t in reversed(range(gen_len)):\n            nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0\n            delta = rewards[:, t] + self.config.gamma * nextvalues - values[:, t]\n            lastgaelam = delta + self.config.gamma * self.config.lam * lastgaelam\n            advantages_reversed.append(lastgaelam)\n        advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)\n\n        returns = advantages + values\n        advantages = masked_whiten(advantages, mask)\n        advantages = advantages.detach()\n        return values, advantages, returns\n\n    def loss(\n        self,\n        old_logprobs: torch.FloatTensor,\n        values: torch.FloatTensor,\n        logits: torch.FloatTensor,\n        vpreds: torch.FloatTensor,\n        logprobs: torch.FloatTensor,\n        mask: torch.LongTensor,\n        advantages: torch.FloatTensor,\n        returns: torch.FloatTensor,\n    ):\n        \"\"\"\n        Calculate policy and value losses.\n\n        Args:\n            old_logprobs (`torch.FloatTensor`):\n                Log probabilities of the model, shape (`batch_size`, `response_length`)\n            values (`torch.FloatTensor`):\n                Values of the value head, shape (`batch_size`, `response_length`)\n            rewards (`torch.FloatTensor`):\n                Rewards from the reward model, shape (`batch_size`, `response_length`)\n            logits (`torch.FloatTensor`):\n                Logits of the model, shape (`batch_size`, `response_length`, `vocab_size`)\n            v_pred (`torch.FloatTensor`):\n                Values of the value head, shape (`batch_size`, `response_length`)\n            logprobs (`torch.FloatTensor`):\n                Log probabilities of the model, shape (`batch_size`, `response_length`)\n        \"\"\"\n\n        vpredclipped = clip_by_value(\n            vpreds,\n            values - self.config.cliprange_value,\n            values + self.config.cliprange_value,\n        )\n\n        vf_losses1 = (vpreds - returns) ** 2\n        vf_losses2 = (vpredclipped - returns) ** 2\n        vf_loss = 0.5 * masked_mean(torch.max(vf_losses1, vf_losses2), mask)\n        vf_clipfrac = masked_mean(torch.gt(vf_losses2, vf_losses1).float(), mask)\n\n        ratio = torch.exp(logprobs - old_logprobs)\n\n        pg_losses = -advantages * ratio\n        pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - self.config.cliprange, 1.0 + self.config.cliprange)\n\n        pg_loss = masked_mean(torch.max(pg_losses, pg_losses2), mask)\n        pg_clipfrac = masked_mean(torch.gt(pg_losses2, pg_losses).float(), mask)\n\n        loss = pg_loss + self.config.vf_coef * vf_loss\n\n        avg_ratio = masked_mean(ratio, mask).item()\n        if avg_ratio > self.config.ratio_threshold:\n            warnings.warn(\n                f\"The average ratio of batch ({avg_ratio:.2f}) exceeds threshold {self.config.ratio_threshold:.2f}. Skipping batch.\"\n            )\n            pg_loss = pg_loss * 0.0\n            vf_loss = vf_loss * 0.0\n            loss = loss * 0.0\n\n        entropy = masked_mean(entropy_from_logits(logits), mask)\n\n        approxkl = 0.5 * masked_mean((logprobs - old_logprobs) ** 2, mask)\n        policykl = masked_mean(old_logprobs - logprobs, mask)\n\n        return_mean, return_var = masked_mean(returns, mask), masked_var(returns, mask)\n        value_mean, value_var = masked_mean(values, mask), masked_var(values, mask)\n\n        stats = dict(\n            loss=dict(policy=pg_loss.detach(), value=vf_loss.detach(), total=loss.detach()),\n            policy=dict(\n                entropy=entropy.detach(),\n                approxkl=approxkl.detach(),\n                policykl=policykl.detach(),\n                clipfrac=pg_clipfrac.detach(),\n                advantages=advantages.detach(),\n                advantages_mean=masked_mean(advantages, mask).detach(),\n                ratio=ratio.detach(),\n            ),\n            returns=dict(mean=return_mean.detach(), var=return_var.detach()),\n            val=dict(\n                vpred=masked_mean(vpreds, mask).detach(),\n                error=masked_mean((vpreds - returns) ** 2, mask).detach(),\n                clipfrac=vf_clipfrac.detach(),\n                mean=value_mean.detach(),\n                var=value_var.detach(),\n            ),\n        )\n        return pg_loss, self.config.vf_coef * vf_loss, flatten_dict(stats)\n\n    def record_step_stats(self, kl_coef: float, **data):\n        \"\"\"\n        Record training step statistics.\n\n\n        Args:\n            kl_coef (`float`):\n                KL coefficient\n            data (`dict`):\n                Dictionary of training step data\n\n        Returns:\n            stats (`dict`):\n                Dictionary of training step statistics\n        \"\"\"\n        mask = data.pop(\"masks\")\n\n        kls = data.pop(\"kls\")\n        kl_list = ((kls) * mask).sum(axis=-1)\n        mean_kl = kl_list.mean()\n        mean_entropy = (-data[\"logprobs\"] * mask).sum(axis=-1).mean()\n\n        mean_non_score_reward = masked_mean(\n            data[\"non_score_reward\"], mask\n        )  # non_score_reward is size `batch_size`, `response_length`\n        mean_scores = data[\"scores\"].mean()  # scores is size `batch_size`\n        std_scores = data[\"scores\"].std()\n\n        if mean_kl.item() < -1.0:\n            # warn users\n            warnings.warn(\n                f\"KL divergence is starting to become negative: {mean_kl.item():.2f} - this might be a precursor for failed training.\"\n                \" sometimes this happens because the generation kwargs are not correctly set. Please make sure\"\n                \" that the generation kwargs are set correctly, or review your training hyperparameters.\"\n            )\n\n        stats = {\n            \"objective/kl\": mean_kl,\n            \"objective/kl_dist\": kl_list,\n            \"objective/logprobs\": data[\"logprobs\"],\n            \"objective/ref_logprobs\": data[\"ref_logprobs\"],\n            \"objective/kl_coef\": kl_coef,\n            \"objective/entropy\": mean_entropy,\n            \"ppo/mean_non_score_reward\": mean_non_score_reward,\n            \"ppo/mean_scores\": mean_scores,\n            \"ppo/std_scores\": std_scores,\n        }\n\n        # Log text properties\n        query_lens = torch.tensor([len(query) for query in data[\"queries\"]], dtype=torch.float)\n        response_lens = torch.tensor([len(response) for response in data[\"responses\"]], dtype=torch.float)\n\n        stats[\"tokens/queries_len_mean\"] = torch.mean(query_lens).cpu().numpy().item()\n        stats[\"tokens/queries_len_std\"] = torch.std(query_lens).cpu().numpy().item()\n        stats[\"tokens/queries_dist\"] = query_lens.cpu().numpy()\n        stats[\"tokens/responses_len_mean\"] = torch.mean(response_lens).cpu().numpy().item()\n        stats[\"tokens/responses_len_std\"] = torch.std(response_lens).cpu().numpy().item()\n        stats[\"tokens/responses_dist\"] = response_lens.cpu().numpy()\n\n        for k, v in data[\"train_stats\"].items():\n            stats[f\"ppo/{k}\"] = torch.mean(v, axis=0)\n        stats[\"ppo/val/var_explained\"] = 1 - stats[\"ppo/val/error\"] / stats[\"ppo/returns/var\"]\n        return stats\n\n    def log_stats(\n        self,\n        stats: dict,\n        batch: dict,\n        rewards: List[torch.FloatTensor],\n        columns_to_log: List[str] = [\"query\", \"response\"],\n    ):\n        \"\"\"\n        A function that logs all the training stats. Call it at the end of each epoch.\n\n        Args:\n            stats (dict[str, Any]):\n                A dictionary of training stats.\n            batch (dict[str, Any]):\n                A dictionary of batch data, this contains the queries and responses.\n            rewards (`List[torch.FloatTensor]`):\n                A tensor of rewards.\n        \"\"\"\n\n        # all gather stats\n        if not isinstance(rewards, torch.Tensor):\n            rewards = torch.tensor(rewards).to(self.current_device)\n        rewards = self.accelerator.gather(rewards).flatten()\n\n        if self.config.log_with == \"wandb\":\n            import wandb\n\n            if any([column_to_log not in batch.keys() for column_to_log in columns_to_log]):\n                raise ValueError(f\"Columns to log {columns_to_log} are not present in the batch {batch.keys()}.\")\n\n            batch_list = [batch[column_to_log] for column_to_log in columns_to_log]\n            if self.is_distributed:\n                gathered_batch_list = []\n                for b in batch_list:\n                    flattened = gather_object(b)\n                    gathered_batch_list.append(flattened)\n                batch_list = gathered_batch_list\n\n        # Log only if we are in the main process\n        if self.accelerator.is_main_process:\n            logs = {}\n\n            # Log stats\n            if \"query\" not in batch.keys() and \"response\" not in batch.keys():\n                # warn the user that the game logs will not be logged\n                warnings.warn(\n                    \"The game logs will not be logged because the batch does not contain the keys 'query' and \"\n                    \"'response'. \"\n                )\n            elif self.config.log_with == \"wandb\":\n                table_rows = [list(r) for r in zip(*batch_list, rewards.cpu().tolist())]\n                logs.update({\"game_log\": wandb.Table(columns=[*columns_to_log, \"reward\"], rows=table_rows)})\n\n            logs.update(stats)\n\n            # manually cast in fp32 for bf16 torch tensors\n            for k, v in logs.items():\n                if isinstance(v, torch.Tensor) and v.dtype == torch.bfloat16:\n                    logs[k] = v.float()\n\n            logs[\"env/reward_mean\"] = torch.mean(rewards).cpu().numpy().item()\n            logs[\"env/reward_std\"] = torch.std(rewards).cpu().numpy().item()\n            logs[\"env/reward_dist\"] = rewards.cpu().numpy()\n\n            if self.config.log_with == \"tensorboard\":\n                # update the current step\n                self.current_step += 1\n\n            self.accelerator.log(\n                logs,\n                step=self.current_step if self.config.log_with == \"tensorboard\" else None,\n            )\n\n    def create_model_card(self, path: str, model_name: Optional[str] = \"TRL Model\") -> None:\n        \"\"\"Creates and saves a model card for a TRL model.\n\n        Args:\n            path (`str`): The path to save the model card to.\n            model_name (`str`, *optional*): The name of the model, defaults to `TRL Model`.\n        \"\"\"\n        try:\n            user = whoami()[\"name\"]\n        # handle the offline case\n        except:  # noqa\n            warnings.warn(\"Cannot retrieve user information assuming you are running in offline mode.\")\n            return\n\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        model_card_content = MODEL_CARD_TEMPLATE.format(model_name=model_name, model_id=f\"{user}/{path}\")\n        with open(os.path.join(path, \"README.md\"), \"w\", encoding=\"utf-8\") as f:\n            f.write(model_card_content)\n\n    def _save_pretrained(self, save_directory: str) -> None:\n        self.accelerator.unwrap_model(self.model).save_pretrained(save_directory)\n        self.tokenizer.save_pretrained(save_directory)\n        self.create_model_card(save_directory)\n\n    def _show_tokens(self, tokens, masks):\n        from rich import print\n        from rich.text import Text\n\n        text = Text()\n\n        for i, (token, mask) in enumerate(zip(tokens, masks)):\n            if mask == 1:\n                text.append(self.tokenizer.decode(token.item()), style=\"black on deep_sky_blue1\")\n                text.append(\" \")\n            else:\n                text.append(self.tokenizer.decode(token.item()), style=\"black on cyan3\")\n                text.append(\" \")\n        print(text)\n\n    def _prepare_deepspeed(self, model: PreTrainedModelWrapper):\n        # Adapted from accelerate: https://github.com/huggingface/accelerate/blob/739b135f8367becb67ffaada12fe76e3aa60fefd/src/accelerate/accelerator.py#L1473\n        deepspeed_plugin = self.accelerator.state.deepspeed_plugin\n        config_kwargs = deepspeed_plugin.deepspeed_config\n        if model is not None:\n            if hasattr(model, \"config\"):\n                hidden_size = (\n                    max(model.config.hidden_sizes)\n                    if getattr(model.config, \"hidden_sizes\", None)\n                    else getattr(model.config, \"hidden_size\", None)\n                )\n                if hidden_size is not None and config_kwargs[\"zero_optimization\"][\"stage\"] == 3:\n                    # Note that `stage3_prefetch_bucket_size` can produce DeepSpeed messages like: `Invalidate trace cache @ step 0: expected module 1, but got module 0`\n                    # This is expected and is not an error, see: https://github.com/microsoft/DeepSpeed/discussions/4081\n                    config_kwargs.update(\n                        {\n                            \"zero_optimization.reduce_bucket_size\": hidden_size * hidden_size,\n                            \"zero_optimization.stage3_param_persistence_threshold\": 10 * hidden_size,\n                            \"zero_optimization.stage3_prefetch_bucket_size\": 0.9 * hidden_size * hidden_size,\n                        }\n                    )\n\n        # If ZeRO-3 is used, we shard both the active and reference model.\n        # Otherwise, we assume the reference model fits in memory and is initialized on each device with ZeRO disabled (stage 0)\n        if config_kwargs[\"zero_optimization\"][\"stage\"] != 3:\n            config_kwargs[\"zero_optimization\"][\"stage\"] = 0\n        model, *_ = deepspeed.initialize(model=model, config=config_kwargs)\n        model.eval()\n        return model\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/extras/best_of_n_sampler.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from typing import Any, Callable, List, Optional, Union\n\nimport torch\nfrom transformers import GenerationConfig, PreTrainedTokenizer, PreTrainedTokenizerFast\n\nfrom ..core import set_seed\nfrom ..models import SUPPORTED_ARCHITECTURES, PreTrainedModelWrapper\n\n\nclass BestOfNSampler(object):\n    def __init__(\n        self,\n        model: PreTrainedModelWrapper,\n        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n        queries_to_scores: Callable[[List[str]], List[float]],\n        length_sampler: Any,\n        sample_size: int = 4,\n        seed: Optional[int] = None,\n        n_candidates: int = 1,\n        generation_config: Optional[GenerationConfig] = None,\n    ) -> None:\n        r\"\"\"\n        Initialize the sampler for best-of-n generation\n\n        Args:\n            model (`PreTrainedModelWrapper`):\n                The pretrained model to use for generation\n            tokenizer (`PreTrainedTokenizer` or `PreTrainedTokenizerFast`):\n                Tokenizer associated with the pretrained model\n            queries_to_scores (`Callable[[List[str]], List[float]]`):\n                Callable that takes a list of generated texts and returns the associated reward scores\n            length_sampler (`Any`):\n                Sampler used to sample the length of the generated text\n            sample_size (`int`):\n                Number of samples to generate for each query\n            seed (`int`, *optional*):\n                Random seed used to control generation\n            n_candidates (`int`):\n                Number of candidates to return for each query\n            generation_config (`GenerationConfig`, *optional*):\n                Generation config passed to the underlying model's `generate` method.\n                See `GenerationConfig` (https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig) for more details\n        \"\"\"\n        if seed is not None:\n            set_seed(seed)\n\n        if not isinstance(tokenizer, (PreTrainedTokenizer, PreTrainedTokenizerFast)):\n            raise ValueError(\n                f\"tokenizer must be a PreTrainedTokenizer or PreTrainedTokenizerFast, got {type(tokenizer)}\"\n            )\n        if not isinstance(model, (SUPPORTED_ARCHITECTURES)):\n            raise ValueError(\n                f\"model must be a PreTrainedModelWrapper, got {type(model)} - supported architectures are: {SUPPORTED_ARCHITECTURES}\"\n            )\n\n        self.model = model\n        self.tokenizer = tokenizer\n\n        self.queries_to_scores = queries_to_scores\n        self.length_sampler = length_sampler\n        self.gen_config = generation_config\n        self.sample_size = sample_size\n        self.n_candidates = n_candidates\n\n    def generate(\n        self,\n        tokenized_query: Union[List[int], torch.Tensor, List[torch.Tensor], List[List[int]]],\n        skip_special_tokens: bool = True,\n        device: Optional[Union[str, torch.device]] = None,\n        **generation_kwargs,\n    ) -> List[List[str]]:\n        r\"\"\"\n        Generate the best of n samples for input queries\n\n        Args:\n            tokenized_query (`List[int]` or `torch.Tensor` or `List[torch.Tensor]` or `List[int]`):\n                represents either a single tokenized query (a single tensor or a list of integers) or a batch of tokenized queries (a list of tensors or a list of lists of integers)\n            skip_special_tokens (`bool`):\n                Whether to remove the special tokens from the output\n            device (`str` or `torch.device`, *optional*):\n                The device on which the model will be loaded\n            **generation_kwargs (`dict`, *optional*):\n                Additional keyword arguments passed along to the underlying model's `generate` method.\n                This is used to override generation config\n\n        Returns:\n            List[List[str]]: A list of lists of generated texts\n        \"\"\"\n        queries = None\n\n        if isinstance(tokenized_query, torch.Tensor) and tokenized_query.ndim == 1:\n            queries = tokenized_query.unsqueeze(0)\n        elif isinstance(tokenized_query, List):\n            element_type = type(tokenized_query[0])\n            if element_type == int:\n                queries = torch.tensor(tokenized_query).unsqueeze(0)\n            elif element_type == torch.Tensor:\n                queries = [tensor.reshape((1, -1)) for tensor in tokenized_query]\n            else:\n                queries = [torch.tensor(query).reshape((1, -1)) for query in tokenized_query]\n\n        result = []\n\n        for query in queries:\n            queries = query.repeat((self.sample_size, 1))\n            output = self.model.generate(\n                queries.to(device),\n                max_new_tokens=self.length_sampler(),\n                generation_config=self.gen_config,\n                **generation_kwargs,\n            ).squeeze()\n            output = self.tokenizer.batch_decode(output, skip_special_tokens=skip_special_tokens)\n            scores = torch.tensor(self.queries_to_scores(output))\n            output = [output[i] for i in scores.topk(self.n_candidates).indices]\n            result.append(output)\n\n        return result\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/extras/__init__.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\nfrom .best_of_n_sampler import BestOfNSampler\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/extras/dataset_formatting.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import logging\nfrom typing import Callable, Literal, Optional, Union\n\nfrom datasets import Dataset, Value\nfrom transformers import AutoTokenizer\n\nfrom ..trainer.utils import ConstantLengthDataset\n\n\nFORMAT_MAPPING = {\n    \"chatml\": [{\"content\": Value(dtype=\"string\", id=None), \"role\": Value(dtype=\"string\", id=None)}],\n    \"instruction\": {\"completion\": Value(dtype=\"string\", id=None), \"prompt\": Value(dtype=\"string\", id=None)},\n}\n\n\ndef conversations_formatting_function(tokenizer: AutoTokenizer, messages_field: Literal[\"messages\", \"conversations\"]):\n    r\"\"\"\n    return a callable function that takes in a \"messages\" dataset and returns a formatted dataset, based on the tokenizer\n    apply chat template to the dataset\n    \"\"\"\n\n    def format_dataset(examples):\n        if isinstance(examples[messages_field][0], list):\n            output_texts = []\n            for i in range(len(examples[messages_field])):\n                output_texts.append(tokenizer.apply_chat_template(examples[messages_field][i], tokenize=False))\n            return output_texts\n        else:\n            return tokenizer.apply_chat_template(examples[messages_field], tokenize=False)\n\n    return format_dataset\n\n\ndef instructions_formatting_function(tokenizer: AutoTokenizer):\n    r\"\"\"\n    return a callable function that takes in an \"instructions\" dataset and returns a formatted dataset, based on the tokenizer\n    apply chat template to the dataset\n    \"\"\"\n\n    def format_dataset(examples):\n        if isinstance(examples[\"prompt\"], list):\n            output_texts = []\n            for i in range(len(examples[\"prompt\"])):\n                converted_sample = [\n                    {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n                    {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n                ]\n                output_texts.append(tokenizer.apply_chat_template(converted_sample, tokenize=False))\n            return output_texts\n        else:\n            converted_sample = [\n                {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n                {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n            ]\n            return tokenizer.apply_chat_template(converted_sample, tokenize=False)\n\n    return format_dataset\n\n\ndef get_formatting_func_from_dataset(\n    dataset: Union[Dataset, ConstantLengthDataset], tokenizer: AutoTokenizer\n) -> Optional[Callable]:\n    r\"\"\"\n    Finds the correct formatting function based on the dataset structure. Currently supported datasets are:\n    - `ChatML` with [{\"role\": str, \"content\": str}]\n    - `instruction` with [{\"prompt\": str, \"completion\": str}]\n\n    Args:\n        dataset (Dataset): User dataset\n        tokenizer (AutoTokenizer): Tokenizer used for formatting\n\n    Returns:\n        Callable: Formatting function if the dataset format is supported else None\n    \"\"\"\n    if isinstance(dataset, Dataset):\n        if \"messages\" in dataset.features:\n            if dataset.features[\"messages\"] == FORMAT_MAPPING[\"chatml\"]:\n                logging.info(\"Formatting dataset with chatml format\")\n                return conversations_formatting_function(tokenizer, \"messages\")\n        if \"conversations\" in dataset.features:\n            if dataset.features[\"conversations\"] == FORMAT_MAPPING[\"chatml\"]:\n                logging.info(\"Formatting dataset with chatml format\")\n                return conversations_formatting_function(tokenizer, \"conversations\")\n        elif dataset.features == FORMAT_MAPPING[\"instruction\"]:\n            logging.info(\"Formatting dataset with instruction format\")\n            return instructions_formatting_function(tokenizer)\n\n    return None\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/models/modeling_value_head.py",
        "recommendation": "The code file is too long to analyze. Please select a shorter file.",
        "code_snippet": "\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n\nfrom .modeling_base import PreTrainedModelWrapper\n\n\nclass ValueHead(nn.Module):\n    r\"\"\"\n    The ValueHead class implements a head for GPT2 that returns a scalar for each output token.\n    \"\"\"\n\n    def __init__(self, config, **kwargs):\n        super().__init__()\n        if not hasattr(config, \"summary_dropout_prob\"):\n            summary_dropout_prob = kwargs.pop(\"summary_dropout_prob\", 0.1)\n        else:\n            summary_dropout_prob = config.summary_dropout_prob\n\n        self.dropout = nn.Dropout(summary_dropout_prob) if summary_dropout_prob else nn.Identity()\n\n        # some models such as OPT have a projection layer before the word embeddings - e.g. OPT-350m\n        if hasattr(config, \"hidden_size\"):\n            hidden_size = config.hidden_size\n        if hasattr(config, \"word_embed_proj_dim\"):\n            hidden_size = config.word_embed_proj_dim\n        elif hasattr(config, \"is_encoder_decoder\"):\n            if config.is_encoder_decoder and hasattr(config, \"decoder\"):\n                if hasattr(config.decoder, \"hidden_size\"):\n                    hidden_size = config.decoder.hidden_size\n\n        self.summary = nn.Linear(hidden_size, 1)\n\n        self.flatten = nn.Flatten()\n\n    def forward(self, hidden_states):\n        output = self.dropout(hidden_states)\n\n        # For now force upcast in fp32 if needed. Let's keep the\n        # output in fp32 for numerical stability.\n        if output.dtype != self.summary.weight.dtype:\n            output = output.to(self.summary.weight.dtype)\n\n        output = self.summary(output)\n        return output\n\n\nclass AutoModelForCausalLMWithValueHead(PreTrainedModelWrapper):\n    r\"\"\"\n    An autoregressive model with a value head in addition to the language model head.\n    This class inherits from `~trl.PreTrainedModelWrapper` and wraps a\n    `transformers.PreTrainedModel` class. The wrapper class supports classic functions\n    such as `from_pretrained`, `push_to_hub` and `generate`. To call a method of the wrapped\n    model, simply manipulate the `pretrained_model` attribute of this class.\n\n    Class attributes:\n        - **transformers_parent_class** (`transformers.PreTrainedModel`) -- The parent class of the wrapped model. This\n            should be set to `transformers.AutoModelForCausalLM` for this class.\n        - **lm_head_namings** (`tuple`) -- A tuple of strings that are used to identify the language model head of the\n            wrapped model. This is set to `(\"lm_head\", \"embed_out\")` for this class but can be changed for other models\n            in the future\n        - **supported_args** (`tuple`) -- A tuple of strings that are used to identify the arguments that are supported\n            by the `ValueHead` class. Currently, the supported args are:\n            - **summary_dropout_prob** (`float`, `optional`, defaults to `None`) -- The dropout probability for the\n                `ValueHead` class.\n            - **v_head_initializer_range** (`float`, `optional`, defaults to `0.2`) -- The initializer range for the\n                `ValueHead` if a specific initialization strategy is selected.\n            - **v_head_init_strategy** (`str`, `optional`, defaults to `None`) -- The initialization strategy for the\n                `ValueHead`. Currently, the supported strategies are:\n                - **`None`** -- Initializes the weights of the `ValueHead` with a random distribution. This is the default\n                    strategy.\n                - **\"normal\"** -- Initializes the weights of the `ValueHead` with a normal distribution.\n\n    \"\"\"\n    transformers_parent_class = AutoModelForCausalLM\n    lm_head_namings = [\"lm_head\", \"embed_out\"]\n    supported_args = (\n        \"summary_dropout_prob\",\n        \"v_head_initializer_range\",\n        \"v_head_init_strategy\",\n    )\n\n    def __init__(self, pretrained_model, **kwargs):\n        r\"\"\"\n        Initializes the model.\n\n        Args:\n            pretrained_model (`transformers.PreTrainedModel`):\n                The model to wrap. It should be a causal language model such as GPT2.\n                or any model mapped inside the `AutoModelForCausalLM` class.\n            kwargs (`dict`, `optional`):\n                Additional keyword arguments, that are passed to the `ValueHead` class.\n        \"\"\"\n        super().__init__(pretrained_model, **kwargs)\n        v_head_kwargs, _, _ = self._split_kwargs(kwargs)\n\n        if not any(hasattr(self.pretrained_model, attribute) for attribute in self.lm_head_namings):\n            raise ValueError(\"The model does not have a language model head, please use a model that has one.\")\n\n        self.v_head = ValueHead(self.pretrained_model.config, **v_head_kwargs)\n\n        self._init_weights(**v_head_kwargs)\n\n    def _init_weights(self, **kwargs):\n        r\"\"\"\n        Initializes the weights of the value head. The default initialization strategy is random.\n        Users can pass a different initialization strategy by passing the `v_head_init_strategy` argument\n        when calling `.from_pretrained`. Supported strategies are:\n        - `normal`: initializes the weights with a normal distribution.\n\n        Args:\n            **kwargs (`dict`, `optional`):\n                Additional keyword arguments, that are passed to the `ValueHead` class. These arguments\n                can contain the `v_head_init_strategy` argument as well as the `v_head_initializer_range`\n                argument.\n        \"\"\"\n        initializer_range = kwargs.pop(\"v_head_initializer_range\", 0.2)\n        # random init by default\n        init_strategy = kwargs.pop(\"v_head_init_strategy\", None)\n        if init_strategy is None:\n            # do nothing\n            pass\n        elif init_strategy == \"normal\":\n            self.v_head.summary.weight.data.normal_(mean=0.0, std=initializer_range)\n            self.v_head.summary.bias.data.zero_()\n\n    def forward(\n        self,\n        input_ids=None,\n        past_key_values=None,\n        attention_mask=None,\n        **kwargs,\n    ):\n        r\"\"\"\n        Applies a forward pass to the wrapped model and returns the logits of the value head.\n\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary.\n            past_key_values (`tuple(tuple(torch.FloatTensor))`, `optional`):\n                Contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n                (see `past_key_values` input) to speed up sequential decoding.\n            attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, `optional`):\n                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n            kwargs (`dict`, `optional`):\n                Additional keyword arguments, that are passed to the wrapped model.\n        \"\"\"\n        kwargs[\"output_hidden_states\"] = True  # this had already been set in the LORA / PEFT examples\n        kwargs[\"past_key_values\"] = past_key_values\n\n        if self.is_peft_model and self.pretrained_model.active_peft_config.peft_type == \"PREFIX_TUNING\":\n            kwargs.pop(\"past_key_values\")\n\n        base_model_output = self.pretrained_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            **kwargs,\n        )\n\n        last_hidden_state = base_model_output.hidden_states[-1]\n        lm_logits = base_model_output.logits\n        loss = base_model_output.loss\n\n        if last_hidden_state.device != self.v_head.summary.weight.device:\n            last_hidden_state = last_hidden_state.to(self.v_head.summary.weight.device)\n\n        value = self.v_head(last_hidden_state).squeeze(-1)\n\n        # force upcast in fp32 if logits are in half-precision\n        if lm_logits.dtype != torch.float32:\n            lm_logits = lm_logits.float()\n\n        return (lm_logits, loss, value)\n\n    def generate(self, *args, **kwargs):\n        r\"\"\"\n        A simple wrapper around the `generate` method of the wrapped model.\n        Please refer to the [`generate`](https://huggingface.co/docs/transformers/internal/generation_utils)\n        method of the wrapped model for more information about the supported arguments.\n\n        Args:\n            *args (`list`, *optional*):\n                Positional arguments passed to the `generate` method of the wrapped model.\n            **kwargs (`dict`, *optional*):\n                Keyword arguments passed to the `generate` method of the wrapped model.\n        \"\"\"\n        return self.pretrained_model.generate(*args, **kwargs)\n\n    def state_dict(self, *args, **kwargs):\n        r\"\"\"\n        Returns the state dictionary of the model. We add the state dictionary of the value head\n        to the state dictionary of the wrapped model by prepending the key with `v_head.`.\n        \"\"\"\n        if not self.is_peft_model:\n            pretrained_model_state_dict = self.pretrained_model.state_dict(*args, **kwargs)\n        else:\n            # if it is a peft model, only save the v_head\n            pretrained_model_state_dict = {}\n\n        v_head_state_dict = self.v_head.state_dict(*args, **kwargs)\n        for k, v in v_head_state_dict.items():\n            pretrained_model_state_dict[f\"v_head.{k}\"] = v\n        return pretrained_model_state_dict\n\n    def push_to_hub(self, *args, **kwargs):\n        setattr(self.pretrained_model, \"v_head\", self.v_head)\n\n        return self.pretrained_model.push_to_hub(*args, **kwargs)\n\n    def post_init(self, state_dict):\n        r\"\"\"\n        We add the state dictionary of the value head to the state dictionary of the wrapped model\n        by prepending the key with `v_head.`. This function removes the `v_head.` prefix from the\n        keys of the value head state dictionary.\n        \"\"\"\n        for k in list(state_dict.keys()):\n            if \"v_head.\" in k:\n                state_dict[k.replace(\"v_head.\", \"\")] = state_dict.pop(k)\n        self.v_head.load_state_dict(state_dict, strict=False)\n        del state_dict\n\n        if hasattr(self.pretrained_model, \"hf_device_map\"):\n            if (\n                \"cpu\" in self.pretrained_model.hf_device_map.values()\n                or \"disk\" in self.pretrained_model.hf_device_map.values()\n            ):\n                raise ValueError(\n                    \"The model is offloaded on CPU or disk - CPU & disk offloading is not supported for ValueHead models.\"\n                )\n\n            first_device = list(set(self.pretrained_model.hf_device_map.values()))[0]\n\n            self.v_head = self.v_head.to(first_device)\n\n            def set_device_hook(module, input, outputs):\n                new_output = ()\n                for output in outputs:\n                    if isinstance(output, torch.Tensor):\n                        new_output += (output.to(first_device),)\n                    else:\n                        new_output += (output,)\n                return new_output\n\n            self.register_forward_hook(set_device_hook)\n\n            self.is_sequential_parallel = True\n\n\nclass AutoModelForSeq2SeqLMWithValueHead(PreTrainedModelWrapper):\n    r\"\"\"\n    A seq2seq model with a value head in addition to the language model head.\n    This class inherits from `~trl.PreTrainedModelWrapper` and wraps a\n    `transformers.PreTrainedModel` class. The wrapper class supports classic functions\n    such as `from_pretrained` and `push_to_hub` and also provides some additional\n    functionalities such as `generate`.\n\n    Args:\n        pretrained_model (`transformers.PreTrainedModel`):\n            The model to wrap. It should be a causal language model such as GPT2.\n            or any model mapped inside the `AutoModelForSeq2SeqLM` class.\n        kwargs:\n            Additional keyword arguments passed along to the `ValueHead` class.\n    \"\"\"\n    transformers_parent_class = AutoModelForSeq2SeqLM\n    lm_head_namings = [\"lm_head\", \"embed_out\", \"output_projection\"]\n    supported_args = (\n        \"summary_dropout_prob\",\n        \"v_head_initializer_range\",\n        \"v_head_init_strategy\",\n    )\n\n    def __init__(self, pretrained_model, **kwargs):\n        super().__init__(pretrained_model, **kwargs)\n        v_head_kwargs, _, _ = self._split_kwargs(kwargs)\n        self.is_encoder_decoder = True\n\n        if not self._has_lm_head():\n            raise ValueError(\"The model does not have a language model head, please use a model that has one.\")\n\n        self.v_head = ValueHead(self.pretrained_model.config, **v_head_kwargs)\n\n        self._init_weights(**v_head_kwargs)\n\n    def _has_lm_head(self):\n        # check module names of all modules inside `pretrained_model` to find the language model head\n        for name, module in self.pretrained_model.named_modules():\n            if any(attribute in name for attribute in self.lm_head_namings):\n                return True\n        return False\n\n    def post_init(self, state_dict):\n        r\"\"\"\n        We add the state dictionary of the value head to the state dictionary of the wrapped model\n        by prepending the key with `v_head.`. This function removes the `v_head.` prefix from the\n        keys of the value head state dictionary.\n        \"\"\"\n        for k in list(state_dict.keys()):\n            if \"v_head.\" in k:\n                state_dict[k.replace(\"v_head.\", \"\")] = state_dict.pop(k)\n        self.v_head.load_state_dict(state_dict, strict=False)\n        del state_dict\n\n        if hasattr(self.pretrained_model, \"hf_device_map\"):\n            if (\n                \"cpu\" in self.pretrained_model.hf_device_map.values()\n                or \"disk\" in self.pretrained_model.hf_device_map.values()\n            ):\n                raise ValueError(\n                    \"The model is offloaded on CPU or disk - CPU & disk offloading is not supported for ValueHead models.\"\n                )\n\n            # get the lm_head device\n            for name, module in self.pretrained_model.named_modules():\n                if any(attribute in name for attribute in self.lm_head_namings):\n                    lm_head_device = module.weight.device\n                    break\n\n            # put v_head on the same device as the lm_head to avoid issues\n            self.v_head = self.v_head.to(lm_head_device)\n\n            def set_device_hook(module, input, outputs):\n                r\"\"\"\n                A hook that sets the device of the output of the model to the device of the first\n                parameter of the model.\n\n                Args:\n                    module (`nn.Module`):\n                        The module to which the hook is attached.\n                    input (`tuple`):\n                        The input to the module.\n                    outputs (`tuple`):\n                        The output of the module.\n                \"\"\"\n                new_output = ()\n                for output in outputs:\n                    if isinstance(output, torch.Tensor):\n                        new_output += (output.to(lm_head_device),)\n                    else:\n                        new_output += (output,)\n                return new_output\n\n            self.register_forward_hook(set_device_hook)\n            self.is_sequential_parallel = True\n\n    def state_dict(self, *args, **kwargs):\n        r\"\"\"\n        Returns the state dictionary of the model. We add the state dictionary of the value head\n        to the state dictionary of the wrapped model by prepending the key with `v_head.`.\n        \"\"\"\n        if not self.is_peft_model:\n            pretrained_model_state_dict = self.pretrained_model.state_dict(*args, **kwargs)\n        else:\n            # if it is a peft model, only save the v_head\n            pretrained_model_state_dict = {}\n\n        v_head_state_dict = self.v_head.state_dict(*args, **kwargs)\n        for k, v in v_head_state_dict.items():\n            pretrained_model_state_dict[f\"v_head.{k}\"] = v\n        return pretrained_model_state_dict\n\n    def push_to_hub(self, *args, **kwargs):\n        setattr(self.pretrained_model, \"v_head\", self.v_head)\n\n        return self.pretrained_model.push_to_hub(*args, **kwargs)\n\n    def _init_weights(self, **kwargs):\n        r\"\"\"\n        We initialize the weights of the value head.\n        \"\"\"\n        initializer_range = kwargs.pop(\"v_head_initializer_range\", 0.2)\n        # random init by default\n        init_strategy = kwargs.pop(\"v_head_init_strategy\", None)\n        if init_strategy is None:\n            # do nothing\n            pass\n        elif init_strategy == \"normal\":\n            self.v_head.summary.weight.data.normal_(mean=0.0, std=initializer_range)\n            self.v_head.summary.bias.data.zero_()\n\n    def forward(\n        self,\n        input_ids=None,\n        past_key_values=None,\n        attention_mask=None,\n        **kwargs,\n    ):\n        kwargs[\"past_key_values\"] = past_key_values\n        if self.is_peft_model and self.pretrained_model.active_peft_config.peft_type == \"PREFIX_TUNING\":\n            kwargs.pop(\"past_key_values\")\n\n        base_model_output = self.pretrained_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=True,  # We force the model to output hidden states\n            **kwargs,\n        )\n\n        last_hidden_state = base_model_output.decoder_hidden_states[-1]\n        lm_logits = base_model_output.logits\n        loss = base_model_output.loss\n\n        value = self.v_head(last_hidden_state).squeeze(-1)\n\n        # force upcast in fp32 if logits are in half-precision\n        if lm_logits.dtype != torch.float32:\n            lm_logits = lm_logits.float()\n\n        return (lm_logits, loss, value)\n\n    def generate(self, *args, **kwargs):\n        r\"\"\"\n        We call `generate` on the wrapped model.\n        \"\"\"\n        return self.pretrained_model.generate(*args, **kwargs)\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/models/modeling_base.py",
        "recommendation": "The code file is too long to analyze. Please select a shorter file.",
        "code_snippet": "\nimport json\nimport logging\nimport os\nfrom copy import deepcopy\n\nimport torch\nimport torch.nn as nn\nfrom accelerate import PartialState\nfrom huggingface_hub import hf_hub_download\nfrom huggingface_hub.utils import (\n    EntryNotFoundError,\n    HFValidationError,\n    LocalEntryNotFoundError,\n    RepositoryNotFoundError,\n)\nfrom safetensors.torch import load_file as safe_load_file\nfrom transformers import PreTrainedModel\n\nfrom ..import_utils import is_npu_available, is_peft_available, is_transformers_greater_than, is_xpu_available\n\n\nif is_peft_available():\n    from peft import (\n        PeftConfig,\n        PeftModel,\n        PeftModelForCausalLM,\n        PeftModelForSeq2SeqLM,\n        PromptLearningConfig,\n        get_peft_model,\n        prepare_model_for_kbit_training,\n    )\n\nif is_transformers_greater_than(\"4.33.0\"):\n    from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\nelse:\n    from transformers.deepspeed import is_deepspeed_zero3_enabled\n\nLAYER_PATTERNS = [\n    \"transformer.h.{layer}\",\n    \"model.decoder.layers.{layer}\",\n    \"gpt_neox.layers.{layer}\",\n    \"model.layers.{layer}\",\n]\n\n\nclass PreTrainedModelWrapper(nn.Module):\n    r\"\"\"\n    A wrapper class around a (`transformers.PreTrainedModel`) to be compatible with the\n    (`~transformers.PreTrained`) class in order to keep some attributes and methods of the\n    (`~transformers.PreTrainedModel`) class.\n\n    Attributes:\n        pretrained_model: (`transformers.PreTrainedModel`)\n            The model to be wrapped.\n        parent_class: (`transformers.PreTrainedModel`)\n            The parent class of the model to be wrapped.\n        supported_args: (`list`)\n            The list of arguments that are supported by the wrapper class.\n    \"\"\"\n    transformers_parent_class = None\n    supported_args = None\n    supported_modules = (\"v_head\",)\n    supported_rm_modules = (\"score\",)\n    supported_pretrained_model_architectures = (\n        (PreTrainedModel)\n        if not is_peft_available()\n        else (PreTrainedModel, PeftModelForCausalLM, PeftModelForSeq2SeqLM)\n    )\n\n    def __init__(\n        self, pretrained_model=None, score_module=None, supports_rm_adapter=False, rm_adapter_name=None, **kwargs\n    ):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n\n        self.config = pretrained_model.config\n        self.prepare_inputs_for_generation = pretrained_model.prepare_inputs_for_generation\n        self.is_loaded_in_8bit = getattr(pretrained_model, \"is_loaded_in_8bit\", False)\n        self.is_loaded_in_4bit = getattr(pretrained_model, \"is_loaded_in_4bit\", False)\n        self.is_sequential_parallel = False\n\n        if hasattr(pretrained_model, \"gradient_checkpointing_disable\"):\n            self.gradient_checkpointing_disable = pretrained_model.gradient_checkpointing_disable\n\n        if hasattr(pretrained_model, \"gradient_checkpointing_enable\"):\n            self.gradient_checkpointing_enable = pretrained_model.gradient_checkpointing_enable\n\n        self.supports_rm_adapter = supports_rm_adapter\n        self.rm_adapter_name = rm_adapter_name\n        self.policy_adapter_name = \"default\"\n        if score_module is not None:\n            self.score = score_module\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n        r\"\"\"\n        Instantiates a new model from a pretrained model from `transformers`. The\n        pretrained model is loaded using the `from_pretrained` method of the\n        `transformers.PreTrainedModel` class. The arguments that are specific to the\n        `transformers.PreTrainedModel` class are passed along this method and filtered\n        out from the `kwargs` argument.\n\n\n        Args:\n            pretrained_model_name_or_path (`str` or `transformers.PreTrainedModel`):\n                The path to the pretrained model or its name.\n            *model_args (`list`, *optional*)):\n                Additional positional arguments passed along to the underlying model's\n                `from_pretrained` method.\n            **kwargs (`dict`, *optional*):\n                Additional keyword arguments passed along to the underlying model's\n                `from_pretrained` method. We also pre-process the kwargs to extract\n                the arguments that are specific to the `transformers.PreTrainedModel`\n                class and the arguments that are specific to trl models. The kwargs\n                also support `prepare_model_for_kbit_training` arguments from\n                `peft` library.\n        \"\"\"\n        if kwargs is not None:\n            peft_config = kwargs.pop(\"peft_config\", None)\n            reward_adapter = kwargs.pop(\"reward_adapter\", None)\n            reward_adapter_name = kwargs.pop(\"reward_adapter_name\", \"reward_adapter\")\n            is_trainable = kwargs.pop(\"is_trainable\", False)\n            trl_model_args, pretrained_kwargs, peft_quantization_kwargs = cls._split_kwargs(kwargs)\n            token = pretrained_kwargs.get(\"token\", None)\n        else:\n            peft_config = None\n            is_trainable = False\n            trl_model_args = {}\n            pretrained_kwargs = {}\n            peft_quantization_kwargs = {}\n            token = None\n\n        if reward_adapter is not None and not isinstance(reward_adapter, str):\n            raise ValueError(\n                \"The `reward_adapter` argument should be a string representing the name of local path or the Hub id to the Reward Modeling adapter.\"\n            )\n\n        is_peft_model = False\n\n        current_device = cls._get_current_device()\n        if isinstance(pretrained_model_name_or_path, str):\n            is_loaded_in_8bit = pretrained_kwargs[\"load_in_8bit\"] if \"load_in_8bit\" in pretrained_kwargs else False\n            is_loaded_in_4bit = pretrained_kwargs[\"load_in_4bit\"] if \"load_in_4bit\" in pretrained_kwargs else False\n        else:\n            is_loaded_in_8bit = getattr(pretrained_model_name_or_path, \"is_loaded_in_8bit\", False)\n            is_loaded_in_4bit = getattr(pretrained_model_name_or_path, \"is_loaded_in_4bit\", False)\n\n        if (is_loaded_in_8bit or is_loaded_in_4bit) and \"device_map\" not in pretrained_kwargs:\n            # warn users\n            logging.warning(\n                \"The `device_map` argument is not provided. We will override the device_map argument.\"\n                \" to set the entire\"\n                \" model on the current device. If you want to set the model on multiple devices, please provide\"\n                \" a custom `device_map` argument.\"\n            )\n            pretrained_kwargs[\"device_map\"] = {\"\": current_device}\n\n        if is_peft_available() and peft_config is not None and not isinstance(peft_config, PeftConfig):\n            raise ValueError(\"The `peft_config` argument should be an instance of `peft.PeftConfig` class.\")\n\n        # First, load the pre-trained model using the parent-class\n        # either `AutoModelForCausalLM` or `AutoModelForSeq2SeqLM`\n        if isinstance(pretrained_model_name_or_path, str):\n            if is_peft_available():\n                try:\n                    # If there is a trained peft adapter in the hub, load its config.\n                    remote_adapter_config = hf_hub_download(\n                        pretrained_model_name_or_path,\n                        \"adapter_config.json\",\n                        token=token,\n                    )\n                except (EntryNotFoundError, LocalEntryNotFoundError, HFValidationError, RepositoryNotFoundError):\n                    remote_adapter_config = None\n            else:\n                remote_adapter_config = None\n\n            local_adapter_present = os.path.exists(os.path.join(pretrained_model_name_or_path, \"adapter_config.json\"))\n\n            if (local_adapter_present or remote_adapter_config is not None) and is_peft_available():\n                if peft_config is not None:\n                    logging.warning(\n                        \"`peft_config` argument ignored since a peft config file was found in \"\n                        f\"{pretrained_model_name_or_path}\"\n                    )\n\n                # Load the trained peft adapter config\n                if local_adapter_present:\n                    trained_adapter_config = PeftConfig.from_pretrained(pretrained_model_name_or_path)\n                else:\n                    remote_adapter_dir = os.path.dirname(remote_adapter_config)\n                    trained_adapter_config = PeftConfig.from_pretrained(remote_adapter_dir)\n\n                # Load the pretrained base model\n                pretrained_model = cls.transformers_parent_class.from_pretrained(\n                    trained_adapter_config.base_model_name_or_path, *model_args, **pretrained_kwargs\n                )\n\n                # Wrap the pretrained model with the trained peft adapter\n                pretrained_model = PeftModel.from_pretrained(\n                    pretrained_model, pretrained_model_name_or_path, is_trainable=is_trainable\n                )\n                logging.info(\"Trained peft adapter loaded\")\n            else:\n                pretrained_model = cls.transformers_parent_class.from_pretrained(\n                    pretrained_model_name_or_path, *model_args, **pretrained_kwargs\n                )\n\n                if peft_config is not None:\n                    # Initialize a new peft adapter with the given config\n                    if is_loaded_in_8bit or is_loaded_in_4bit:\n                        pretrained_model = prepare_model_for_kbit_training(\n                            pretrained_model,\n                            **peft_quantization_kwargs,\n                        )\n                    pretrained_model = get_peft_model(pretrained_model, peft_config)\n                    logging.info(\"peft adapter initialised\")\n\n        elif isinstance(pretrained_model_name_or_path, cls.supported_pretrained_model_architectures):\n            pretrained_model = pretrained_model_name_or_path\n\n            if peft_config is not None and isinstance(pretrained_model, PreTrainedModel):\n                # Initialize a new peft adapter with the given config\n                if is_loaded_in_8bit or is_loaded_in_4bit:\n                    pretrained_model = prepare_model_for_kbit_training(\n                        pretrained_model,\n                        **peft_quantization_kwargs,\n                    )\n                pretrained_model = get_peft_model(pretrained_model, peft_config)\n                logging.info(\"peft adapter initialised\")\n        else:\n            raise ValueError(\n                \"pretrained_model_name_or_path should be a string or a PreTrainedModel, \"\n                f\"but is {type(pretrained_model_name_or_path)}\"\n            )\n\n        if is_peft_available():\n            if isinstance(pretrained_model, PeftModel):\n                is_peft_model = True\n                # for backward compatibility\n                if hasattr(pretrained_model, \"active_peft_config\") and isinstance(\n                    pretrained_model.active_peft_config, PromptLearningConfig\n                ):\n                    raise ValueError(\"PromptLearningConfig is not supported for PPO training.\")\n\n        # Add reward modeling adapter if specified\n        if not is_peft_model and reward_adapter is not None:\n            raise ValueError(\"reward_adapter can only be used with a PeftModel. \")\n        elif is_peft_model and reward_adapter is not None:\n            score_module = cls.add_and_load_reward_modeling_adapter(\n                pretrained_model, reward_adapter, reward_adapter_name, token=token\n            )\n            multi_adapter_args = {\n                \"score_module\": score_module,\n                \"supports_rm_adapter\": True,\n                \"rm_adapter_name\": reward_adapter_name,\n            }\n        else:\n            multi_adapter_args = {\"supports_rm_adapter\": False}\n\n        # Then, create the full model by instantiating the wrapper class\n        model = cls(pretrained_model, **multi_adapter_args, **trl_model_args)\n\n        # if resume_training, load the state_dict again - this is ok since the\n        # state_dict is removed from the model after loading it.\n        is_resuming_training = True\n        if isinstance(pretrained_model_name_or_path, str):\n            safe_filename = os.path.join(pretrained_model_name_or_path, \"model.safetensors\")\n            filename = os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin\")\n\n            sharded_index_filename = os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin.index.json\")\n            safe_sharded_index_filename = os.path.join(pretrained_model_name_or_path, \"model.safetensors.index.json\")\n            is_sharded = False\n            use_safe = os.path.exists(safe_filename)\n\n            if not (os.path.exists(filename) or os.path.exists(safe_filename)):\n                # Try with `pytorch_model.bin`\n                filename, files_to_download, is_sharded, is_resuming_training = cls._get_checkpoint_from_hub(\n                    pretrained_model,\n                    pretrained_model_name_or_path,\n                    sharded_index_filename,\n                    token=token,\n                )\n                # Try with safetensors\n                if filename is None and files_to_download is None:\n                    safe_filename, files_to_download, is_sharded, is_resuming_training = cls._get_checkpoint_from_hub(\n                        pretrained_model,\n                        pretrained_model_name_or_path,\n                        safe_sharded_index_filename,\n                        token=token,\n                        model_name=\"model.safetensors\",\n                        model_index_name=\"model.safetensors.index.json\",\n                    )\n                    use_safe = True\n                else:\n                    use_safe = False\n\n            loading_func = safe_load_file if use_safe else torch.load\n            load_kwargs = {} if use_safe else {\"map_location\": \"cpu\"}\n\n            if is_resuming_training:\n                if is_sharded:\n                    # download each file and add it to the state_dict\n                    state_dict = {}\n\n                    for shard_file in files_to_download:\n                        filename = hf_hub_download(\n                            pretrained_model_name_or_path,\n                            shard_file,\n                            token=token,\n                        )\n                        state_dict.update(loading_func(filename, **load_kwargs))\n                else:\n                    state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n\n        else:\n            state_dict = pretrained_model_name_or_path.state_dict()\n\n        model.is_peft_model = is_peft_model\n        model.current_device = current_device\n\n        if is_resuming_training:\n            model.post_init(state_dict=state_dict)\n\n        return model\n\n    @classmethod\n    def _get_checkpoint_from_hub(\n        cls,\n        pretrained_model,\n        pretrained_model_name_or_path,\n        index_filename,\n        token=None,\n        model_name=\"pytorch_model.bin\",\n        model_index_name=\"pytorch_model.bin.index.json\",\n    ):\n        files_to_download = None\n        filename = None\n        is_resuming_training = True\n        is_sharded = False\n\n        try:\n            filename = hf_hub_download(\n                pretrained_model_name_or_path,\n                model_name,\n                token=token,\n            )\n        # sharded\n        except (EntryNotFoundError, LocalEntryNotFoundError, HFValidationError, RepositoryNotFoundError):\n            if os.path.exists(index_filename):\n                index_file_name = index_filename\n            else:\n                try:\n                    index_file_name = hf_hub_download(\n                        pretrained_model_name_or_path,\n                        model_index_name,\n                        token=token,\n                    )\n                except (EntryNotFoundError, LocalEntryNotFoundError, HFValidationError, RepositoryNotFoundError):\n                    # not continue training, do not have v_head weight\n                    is_resuming_training = False\n                    logging.warning(\n                        f\"A {type(pretrained_model)} model is loaded from '{pretrained_model_name_or_path}', \"\n                        f\"and no v_head weight is found. This IS expected if you are not resuming PPO training.\"\n                    )\n            # load json\n            if is_resuming_training:\n                with open(index_file_name, \"r\") as f:\n                    index = json.load(f)\n                # check filename with `v_head` or any known extra module:\n                files_to_download = set()\n                for k, v in index[\"weight_map\"].items():\n                    if any([module in k for module in cls.supported_modules]):\n                        files_to_download.add(v)\n                is_sharded = True\n\n        return filename, files_to_download, is_sharded, is_resuming_training\n\n    @classmethod\n    def _get_current_device(cls):\n        r\"\"\"\n        Get the current device. For GPU, we return the local process index using the `accelerate.PartialState`\n        object to handle corner cases when running scripts in distributed environments.\n\n        Returns:\n            current_device (`Union[int, str]`):\n                The current device.\n        \"\"\"\n        state = PartialState()\n        if is_xpu_available():\n            return f\"xpu:{state.local_process_index}\"\n        elif is_npu_available():\n            return f\"npu:{state.local_process_index}\"\n        else:\n            return state.local_process_index if torch.cuda.is_available() else \"cpu\"\n\n    @classmethod\n    def _split_kwargs(cls, kwargs):\n        \"\"\"\n        Separate the kwargs from the arguments that we support inside\n        `supported_args` and the ones that we don't.\n        \"\"\"\n        check_peft_kwargs = False\n\n        if is_peft_available():\n            from peft import prepare_model_for_kbit_training\n\n            check_peft_kwargs = True\n\n        supported_kwargs = {}\n        unsupported_kwargs = {}\n        peft_kwargs = {}\n\n        for key, value in kwargs.items():\n            if key in cls.supported_args:\n                supported_kwargs[key] = value\n            else:\n                unsupported_kwargs[key] = value\n\n            if check_peft_kwargs:\n                if key in prepare_model_for_kbit_training.__code__.co_varnames:\n                    peft_kwargs[key] = value\n                    if key in unsupported_kwargs:\n                        unsupported_kwargs.pop(key)\n\n        return supported_kwargs, unsupported_kwargs, peft_kwargs\n\n    @classmethod\n    def add_and_load_reward_modeling_adapter(\n        cls, pretrained_model, adapter_model_id, adapter_name=\"reward_model_adapter\", token=None\n    ):\n        r\"\"\"\n        Add and load a reward modeling adapter. This method can only be used if the\n        model is a `PeftModel` and if you have initialized the model with the `reward_modeling_adapter_id`\n        argument, pointing to the id of the reward modeling adapter. The latest needs also to contain the\n        score head in order to produce the reward.\n        \"\"\"\n        pretrained_model.load_adapter(adapter_model_id, adapter_name, is_trainable=False)\n        pretrained_model.train()\n\n        filename = os.path.join(adapter_model_id, \"adapter_model.bin\")\n        safe_loading = False\n        if not os.path.exists(filename):\n            try:\n                local_filename = hf_hub_download(\n                    adapter_model_id,\n                    \"adapter_model.bin\",\n                    token=token,\n                )\n            except:  # noqa\n                filename = os.path.join(adapter_model_id, \"adapter_model.safetensors\")\n                safe_loading = True\n                if not os.path.exists(filename):\n                    try:\n                        local_filename = hf_hub_download(\n                            adapter_model_id,\n                            \"adapter_model.safetensors\",\n                            token=token,\n                        )\n                    except:  # noqa\n                        raise ValueError(\n                            \"Could not find adapter model in the Hub, make sure you have the correct adapter model id.\"\n                        )\n                else:\n                    local_filename = filename\n        else:\n            local_filename = filename\n\n        loading_func = safe_load_file if safe_loading else torch.load\n        load_kwargs = {} if safe_loading else {\"map_location\": \"cpu\"}\n\n        adapter_state_dict = loading_func(local_filename, **load_kwargs)\n\n        for score_name_candidate in cls.supported_rm_modules:\n            if any([score_name_candidate in name for name in adapter_state_dict.keys()]):\n                score_name = score_name_candidate\n                # we have found the correct head name and can break\n                break\n\n        score_dict = {}\n\n        for name, param in adapter_state_dict.items():\n            if score_name in name:\n                key_name = \".\".join(name.split(\".\")[-1:])\n                score_dict[key_name] = param.to(cls._get_current_device())\n\n        num_labels, hidden_dim = score_dict[\"weight\"].shape\n        has_bias = any([\"bias\" in name for name in adapter_state_dict.keys()])\n\n        score = nn.Linear(hidden_dim, num_labels, bias=has_bias).to(\n            device=cls._get_current_device(),\n            dtype=pretrained_model.dtype,\n        )\n        score.load_state_dict(score_dict)\n        for param in score.parameters():\n            param.requires_grad = False\n\n        return score\n\n    def push_to_hub(self, *args, **kwargs):\n        r\"\"\"\n        Push the pretrained model to the hub. This method is a wrapper around\n        `transformers.PreTrainedModel.push_to_hub`. Please refer to the documentation\n        of `transformers.PreTrainedModel.push_to_hub` for more information.\n\n        Args:\n            *args (`list`, *optional*):\n                Positional arguments passed along to the underlying model's\n                `push_to_hub` method.\n            **kwargs (`dict`, *optional*):\n                Keyword arguments passed along to the underlying model's\n                `push_to_hub` method.\n        \"\"\"\n        raise NotImplementedError\n\n    def save_pretrained(self, *args, **kwargs):\n        r\"\"\"\n        Save the pretrained model to a directory. This method is a wrapper around\n        `transformers.PreTrainedModel.save_pretrained`. Please refer to the documentation\n        of `transformers.PreTrainedModel.save_pretrained` for more information.\n\n        Args:\n            *args (`list`, *optional*):\n                Positional arguments passed along to the underlying model's\n                `save_pretrained` method.\n            **kwargs (`dict`, *optional*):\n                Keyword arguments passed along to the underlying model's\n                `save_pretrained` method.\n        \"\"\"\n        state_dict = kwargs.get(\"state_dict\")\n        if state_dict is None:\n            state_dict = self.state_dict()\n            kwargs[\"state_dict\"] = state_dict\n\n        # if it is a peft model only save the `v_head` state_dict and\n        # pop the `state_dict` from the kwargs to avoid slient bugs with `peft`\n        if self.is_peft_model:\n            save_path = args[0]\n            save_path = os.path.join(save_path, \"pytorch_model.bin\")\n            torch.save(state_dict, save_path)\n            _ = kwargs.pop(\"state_dict\", None)\n\n        return self.pretrained_model.save_pretrained(*args, **kwargs)\n\n    def state_dict(self, *args, **kwargs):\n        r\"\"\"\n        Return the state_dict of the pretrained model.\n        \"\"\"\n        raise NotImplementedError\n\n    def post_init(self, *args, **kwargs):\n        r\"\"\"\n        Post initialization method. This method is called after the model is\n        instantiated and loaded from a checkpoint. It can be used to perform\n        additional operations such as loading the state_dict.\n        \"\"\"\n        raise NotImplementedError\n\n    def compute_reward_score(self, input_ids, attention_mask=None, **kwargs):\n        r\"\"\"\n        Computes the reward score for a given input. The method has first to enable the adapter\n        and then compute the reward score. After that the model disables the reward modeling\n        adapter and enables the default ppo adapter again.\n        \"\"\"\n        if not self.supports_rm_adapter:\n            raise ValueError(\"This model does not support reward modeling adapter.\")\n\n        # enable rm adapter\n        self.pretrained_model.set_adapter(self.rm_adapter_name)\n        self.pretrained_model.eval()\n\n        with torch.no_grad():\n            base_model_output = self.pretrained_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_hidden_states=True,\n                return_dict=True,\n                **kwargs,\n            )\n\n            last_hidden_states = base_model_output.hidden_states[-1]\n            scores = self.score(last_hidden_states)\n\n        self.pretrained_model.set_adapter(self.policy_adapter_name)\n        self.pretrained_model.eval()\n\n        return scores\n\n\ndef create_reference_model(\n    model: PreTrainedModelWrapper, num_shared_layers: int = None, pattern: str = None\n) -> PreTrainedModelWrapper:\n    \"\"\"\n    Creates a static reference copy of a model. Note that model will be in `.eval()` mode.\n\n    Args:\n        model (`PreTrainedModelWrapper`): The model to be copied.\n        num_shared_layers (`int`, *optional*): The number of initial layers that are shared between both models and kept frozen.\n        pattern (`str`, *optional*): The shared layers are selected with a string pattern\n            (e.g. \"transformer.h.{layer}\" for GPT2) and if a custom pattern is necessary it can be passed here.\n\n    Returns\n        `PreTrainedModelWrapper`\n    \"\"\"\n    if is_deepspeed_zero3_enabled():\n        raise ValueError(\n            \"DeepSpeed ZeRO-3 is enabled and is not compatible with `create_reference_model()`. Please instantiate your reference model directly with `AutoCausalLM.from_pretrained()`.\"\n        )\n\n    parameter_names = [n for n, _ in model.named_parameters()]\n    ref_model = deepcopy(model)\n\n    # if no layers are shared, return copy of model\n    if num_shared_layers is None:\n        for param_name in parameter_names:\n            param = ref_model.get_parameter(param_name)\n            param.requires_grad = False\n        return ref_model.eval()\n\n    # identify layer name pattern\n    if pattern is not None:\n        pattern = pattern.format(layer=num_shared_layers)\n    else:\n        for pattern_candidate in LAYER_PATTERNS:\n            pattern_candidate = pattern_candidate.format(layer=num_shared_layers)\n            if any([pattern_candidate in name for name in parameter_names]):\n                pattern = pattern_candidate\n                break\n\n    if pattern is None:\n        raise ValueError(\"Layer pattern could not be matched.\")\n\n    # divide parameters in shared and unshared parameter lists\n    shared_param_list = []\n    unshared_param_list = []\n\n    shared_parameter = True\n    for name, param in model.named_parameters():\n        if pattern in name:\n            shared_parameter = False\n        if shared_parameter:\n            shared_param_list.append(name)\n        else:\n            unshared_param_list.append(name)\n\n    # create reference of the original parameter if they are shared\n    for param_name in shared_param_list:\n        param = model.get_parameter(param_name)\n        param.requires_grad = False\n\n        ref_param = ref_model.get_parameter(param_name)  # noqa\n        ref_param = param  # noqa\n\n    # for all other parameters just make sure they don't use gradients\n    for param_name in unshared_param_list:\n        param = ref_model.get_parameter(param_name)\n        param.requires_grad = False\n\n    if pattern is not None and len(unshared_param_list) == 0:\n        logging.warning(\"Pattern passed or found, but no layers matched in the model. Check for a typo.\")\n\n    return ref_model.eval()\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/models/__init__.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# flake8: noqa\n\n\nfrom .modeling_base import PreTrainedModelWrapper, create_reference_model\nfrom .modeling_value_head import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead\n\n\nSUPPORTED_ARCHITECTURES = (\n    AutoModelForCausalLMWithValueHead,\n    AutoModelForSeq2SeqLMWithValueHead,\n)\n\nfrom ..import_utils import is_diffusers_available\n\n\nif is_diffusers_available():\n    from .modeling_sd_base import (\n        DDPOPipelineOutput,\n        DDPOSchedulerOutput,\n        DDPOStableDiffusionPipeline,\n        DefaultDDPOStableDiffusionPipeline,\n    )\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/models/modeling_sd_base.py",
        "recommendation": "The code file is too long to analyze. Please select a shorter file.",
        "code_snippet": "import contextlib\nimport os\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nimport numpy as np\nimport torch\nfrom diffusers import DDIMScheduler, StableDiffusionPipeline, UNet2DConditionModel\nfrom diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import rescale_noise_cfg\nfrom diffusers.utils import convert_state_dict_to_diffusers\n\nfrom ..core import randn_tensor\nfrom ..import_utils import is_peft_available\n\n\nif is_peft_available():\n    from peft import LoraConfig\n    from peft.utils import get_peft_model_state_dict\n\n\n@dataclass\nclass DDPOPipelineOutput(object):\n    \"\"\"\n    Output class for the diffusers pipeline to be finetuned with the DDPO trainer\n\n    Args:\n        images (`torch.Tensor`):\n            The generated images.\n        latents (`List[torch.Tensor]`):\n            The latents used to generate the images.\n        log_probs (`List[torch.Tensor]`):\n            The log probabilities of the latents.\n\n    \"\"\"\n\n    images: torch.Tensor\n    latents: torch.Tensor\n    log_probs: torch.Tensor\n\n\n@dataclass\nclass DDPOSchedulerOutput(object):\n    \"\"\"\n    Output class for the diffusers scheduler to be finetuned with the DDPO trainer\n\n    Args:\n        latents (`torch.Tensor`):\n            Predicted sample at the previous timestep. Shape: `(batch_size, num_channels, height, width)`\n        log_probs (`torch.Tensor`):\n            Log probability of the above mentioned sample. Shape: `(batch_size)`\n    \"\"\"\n\n    latents: torch.Tensor\n    log_probs: torch.Tensor\n\n\nclass DDPOStableDiffusionPipeline(object):\n    \"\"\"\n    Main class for the diffusers pipeline to be finetuned with the DDPO trainer\n    \"\"\"\n\n    def __call__(self, *args, **kwargs) -> DDPOPipelineOutput:\n        raise NotImplementedError\n\n    def scheduler_step(self, *args, **kwargs) -> DDPOSchedulerOutput:\n        raise NotImplementedError\n\n    @property\n    def unet(self):\n        \"\"\"\n        Returns the 2d U-Net model used for diffusion.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def vae(self):\n        \"\"\"\n        Returns the Variational Autoencoder model used from mapping images to and from the latent space\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def tokenizer(self):\n        \"\"\"\n        Returns the tokenizer used for tokenizing text inputs\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def scheduler(self):\n        \"\"\"\n        Returns the scheduler associated with the pipeline used for the diffusion process\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def text_encoder(self):\n        \"\"\"\n        Returns the text encoder used for encoding text inputs\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def autocast(self):\n        \"\"\"\n        Returns the autocast context manager\n        \"\"\"\n        raise NotImplementedError\n\n    def set_progress_bar_config(self, *args, **kwargs):\n        \"\"\"\n        Sets the progress bar config for the pipeline\n        \"\"\"\n        raise NotImplementedError\n\n    def save_pretrained(self, *args, **kwargs):\n        \"\"\"\n        Saves all of the model weights\n        \"\"\"\n        raise NotImplementedError\n\n    def get_trainable_layers(self, *args, **kwargs):\n        \"\"\"\n        Returns the trainable parameters of the pipeline\n        \"\"\"\n        raise NotImplementedError\n\n    def save_checkpoint(self, *args, **kwargs):\n        \"\"\"\n        Light wrapper around accelerate's register_save_state_pre_hook which is run before saving state\n        \"\"\"\n        raise NotImplementedError\n\n    def load_checkpoint(self, *args, **kwargs):\n        \"\"\"\n        Light wrapper around accelerate's register_lad_state_pre_hook which is run before loading state\n        \"\"\"\n        raise NotImplementedError\n\n\ndef _left_broadcast(input_tensor, shape):\n    \"\"\"\n    As opposed to the default direction of broadcasting (right to left), this function broadcasts\n    from left to right\n        Args:\n            input_tensor (`torch.FloatTensor`): is the tensor to broadcast\n            shape (`Tuple[int]`): is the shape to broadcast to\n    \"\"\"\n    input_ndim = input_tensor.ndim\n    if input_ndim > len(shape):\n        raise ValueError(\n            \"The number of dimensions of the tensor to broadcast cannot be greater than the length of the shape to broadcast to\"\n        )\n    return input_tensor.reshape(input_tensor.shape + (1,) * (len(shape) - input_ndim)).broadcast_to(shape)\n\n\ndef _get_variance(self, timestep, prev_timestep):\n    alpha_prod_t = torch.gather(self.alphas_cumprod, 0, timestep.cpu()).to(timestep.device)\n    alpha_prod_t_prev = torch.where(\n        prev_timestep.cpu() >= 0,\n        self.alphas_cumprod.gather(0, prev_timestep.cpu()),\n        self.final_alpha_cumprod,\n    ).to(timestep.device)\n    beta_prod_t = 1 - alpha_prod_t\n    beta_prod_t_prev = 1 - alpha_prod_t_prev\n\n    variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)\n\n    return variance\n\n\ndef scheduler_step(\n    self,\n    model_output: torch.FloatTensor,\n    timestep: int,\n    sample: torch.FloatTensor,\n    eta: float = 0.0,\n    use_clipped_model_output: bool = False,\n    generator=None,\n    prev_sample: Optional[torch.FloatTensor] = None,\n) -> DDPOSchedulerOutput:\n    \"\"\"\n\n    Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n    process from the learned model outputs (most often the predicted noise).\n\n    Args:\n        model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n        timestep (`int`): current discrete timestep in the diffusion chain.\n        sample (`torch.FloatTensor`):\n            current instance of sample being created by diffusion process.\n        eta (`float`): weight of noise for added noise in diffusion step.\n        use_clipped_model_output (`bool`): if `True`, compute \"corrected\" `model_output` from the clipped\n            predicted original sample. Necessary because predicted original sample is clipped to [-1, 1] when\n            `self.config.clip_sample` is `True`. If no clipping has happened, \"corrected\" `model_output` would\n            coincide with the one provided as input and `use_clipped_model_output` will have not effect.\n        generator: random number generator.\n        variance_noise (`torch.FloatTensor`): instead of generating noise for the variance using `generator`, we\n            can directly provide the noise for the variance itself. This is useful for methods such as\n            CycleDiffusion. (https://arxiv.org/abs/2210.05559)\n\n    Returns:\n        `DDPOSchedulerOutput`: the predicted sample at the previous timestep and the log probability of the sample\n    \"\"\"\n\n    if self.num_inference_steps is None:\n        raise ValueError(\n            \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n        )\n\n    # See formulas (12) and (16) of DDIM paper https://arxiv.org/pdf/2010.02502.pdf\n    # Ideally, read DDIM paper in-detail understanding\n\n    # Notation (<variable name> -> <name in paper>\n    # - pred_noise_t -> e_theta(x_t, t)\n    # - pred_original_sample -> f_theta(x_t, t) or x_0\n    # - std_dev_t -> sigma_t\n    # - eta -> \u03b7\n    # - pred_sample_direction -> \"direction pointing to x_t\"\n    # - pred_prev_sample -> \"x_t-1\"\n\n    # 1. get previous step value (=t-1)\n    prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps\n    # to prevent OOB on gather\n    prev_timestep = torch.clamp(prev_timestep, 0, self.config.num_train_timesteps - 1)\n\n    # 2. compute alphas, betas\n    alpha_prod_t = self.alphas_cumprod.gather(0, timestep.cpu())\n    alpha_prod_t_prev = torch.where(\n        prev_timestep.cpu() >= 0,\n        self.alphas_cumprod.gather(0, prev_timestep.cpu()),\n        self.final_alpha_cumprod,\n    )\n    alpha_prod_t = _left_broadcast(alpha_prod_t, sample.shape).to(sample.device)\n    alpha_prod_t_prev = _left_broadcast(alpha_prod_t_prev, sample.shape).to(sample.device)\n\n    beta_prod_t = 1 - alpha_prod_t\n\n    # 3. compute predicted original sample from predicted noise also called\n    # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n    if self.config.prediction_type == \"epsilon\":\n        pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n        pred_epsilon = model_output\n    elif self.config.prediction_type == \"sample\":\n        pred_original_sample = model_output\n        pred_epsilon = (sample - alpha_prod_t ** (0.5) * pred_original_sample) / beta_prod_t ** (0.5)\n    elif self.config.prediction_type == \"v_prediction\":\n        pred_original_sample = (alpha_prod_t**0.5) * sample - (beta_prod_t**0.5) * model_output\n        pred_epsilon = (alpha_prod_t**0.5) * model_output + (beta_prod_t**0.5) * sample\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or\"\n            \" `v_prediction`\"\n        )\n\n    # 4. Clip or threshold \"predicted x_0\"\n    if self.config.thresholding:\n        pred_original_sample = self._threshold_sample(pred_original_sample)\n    elif self.config.clip_sample:\n        pred_original_sample = pred_original_sample.clamp(\n            -self.config.clip_sample_range, self.config.clip_sample_range\n        )\n\n    # 5. compute variance: \"sigma_t(\u03b7)\" -> see formula (16)\n    # \u03c3_t = sqrt((1 \u2212 \u03b1_t\u22121)/(1 \u2212 \u03b1_t)) * sqrt(1 \u2212 \u03b1_t/\u03b1_t\u22121)\n    variance = _get_variance(self, timestep, prev_timestep)\n    std_dev_t = eta * variance ** (0.5)\n    std_dev_t = _left_broadcast(std_dev_t, sample.shape).to(sample.device)\n\n    if use_clipped_model_output:\n        # the pred_epsilon is always re-derived from the clipped x_0 in Glide\n        pred_epsilon = (sample - alpha_prod_t ** (0.5) * pred_original_sample) / beta_prod_t ** (0.5)\n\n    # 6. compute \"direction pointing to x_t\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n    pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * pred_epsilon\n\n    # 7. compute x_t without \"random noise\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n    prev_sample_mean = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n\n    if prev_sample is not None and generator is not None:\n        raise ValueError(\n            \"Cannot pass both generator and prev_sample. Please make sure that either `generator` or\"\n            \" `prev_sample` stays `None`.\"\n        )\n\n    if prev_sample is None:\n        variance_noise = randn_tensor(\n            model_output.shape,\n            generator=generator,\n            device=model_output.device,\n            dtype=model_output.dtype,\n        )\n        prev_sample = prev_sample_mean + std_dev_t * variance_noise\n\n    # log prob of prev_sample given prev_sample_mean and std_dev_t\n    log_prob = (\n        -((prev_sample.detach() - prev_sample_mean) ** 2) / (2 * (std_dev_t**2))\n        - torch.log(std_dev_t)\n        - torch.log(torch.sqrt(2 * torch.as_tensor(np.pi)))\n    )\n    # mean along all but batch dimension\n    log_prob = log_prob.mean(dim=tuple(range(1, log_prob.ndim)))\n\n    return DDPOSchedulerOutput(prev_sample.type(sample.dtype), log_prob)\n\n\n# 1. The output type for call is different as the logprobs are now returned\n# 2. An extra method called `scheduler_step` is added which is used to constraint the scheduler output\n@torch.no_grad()\ndef pipeline_step(\n    self,\n    prompt: Optional[Union[str, List[str]]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    guidance_rescale: float = 0.0,\n):\n    r\"\"\"\n    Function invoked when calling the pipeline for generation.  Args: prompt (`str` or `List[str]`, *optional*): The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.  instead.  height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor): The height in pixels of the generated image.\n        width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n            The width in pixels of the generated image.\n        num_inference_steps (`int`, *optional*, defaults to 50):\n            The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n            expense of slower inference.\n        guidance_scale (`float`, *optional*, defaults to 7.5):\n            Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n            `guidance_scale` is defined as `w` of equation 2. of [Imagen\n            Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n            1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n            usually at the expense of lower image quality.\n        negative_prompt (`str` or `List[str]`, *optional*):\n            The prompt or prompts not to guide the image generation. If not defined, one has to pass\n            `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n            less than `1`).\n        num_images_per_prompt (`int`, *optional*, defaults to 1):\n            The number of images to generate per prompt.\n        eta (`float`, *optional*, defaults to 0.0):\n            Corresponds to parameter eta (\u03b7) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n            [`schedulers.DDIMScheduler`], will be ignored for others.\n        generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n            One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n            to make generation deterministic.\n        latents (`torch.FloatTensor`, *optional*):\n            Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n            generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n            tensor will ge generated by sampling using the supplied random `generator`.\n        prompt_embeds (`torch.FloatTensor`, *optional*):\n            Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n            provided, text embeddings will be generated from `prompt` input argument.\n        negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n            Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n            weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n            argument.\n        output_type (`str`, *optional*, defaults to `\"pil\"`):\n            The output format of the generate image. Choose between\n            [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n        return_dict (`bool`, *optional*, defaults to `True`):\n            Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n            plain tuple.\n        callback (`Callable`, *optional*):\n            A function that will be called every `callback_steps` steps during inference. The function will be\n            called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n        callback_steps (`int`, *optional*, defaults to 1):\n            The frequency at which the `callback` function will be called. If not specified, the callback will be\n            called at every step.\n        cross_attention_kwargs (`dict`, *optional*):\n            A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n            `self.processor` in\n            [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n        guidance_rescale (`float`, *optional*, defaults to 0.7):\n            Guidance rescale factor proposed by [Common Diffusion Noise Schedules and Sample Steps are\n            Flawed](https://arxiv.org/pdf/2305.08891.pdf) `guidance_scale` is defined as `\u03c6` in equation 16. of\n            [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).\n            Guidance rescale factor should fix overexposure when using zero terminal SNR.\n\n    Examples:\n\n    Returns:\n        `DDPOPipelineOutput`: The generated image, the predicted latents used to generate the image and the associated log probabilities\n    \"\"\"\n    # 0. Default height and width to unet\n    height = height or self.unet.config.sample_size * self.vae_scale_factor\n    width = width or self.unet.config.sample_size * self.vae_scale_factor\n\n    # 1. Check inputs. Raise error if not correct\n    self.check_inputs(\n        prompt,\n        height,\n        width,\n        callback_steps,\n        negative_prompt,\n        prompt_embeds,\n        negative_prompt_embeds,\n    )\n\n    # 2. Define call parameters\n    if prompt is not None and isinstance(prompt, str):\n        batch_size = 1\n    elif prompt is not None and isinstance(prompt, list):\n        batch_size = len(prompt)\n    else:\n        batch_size = prompt_embeds.shape[0]\n\n    device = self._execution_device\n    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n    # corresponds to doing no classifier free guidance.\n    do_classifier_free_guidance = guidance_scale > 1.0\n\n    # 3. Encode input prompt\n    text_encoder_lora_scale = cross_attention_kwargs.get(\"scale\", None) if cross_attention_kwargs is not None else None\n    prompt_embeds = self._encode_prompt(\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt,\n        prompt_embeds=prompt_embeds,\n        negative_prompt_embeds=negative_prompt_embeds,\n        lora_scale=text_encoder_lora_scale,\n    )\n\n    # 4. Prepare timesteps\n    self.scheduler.set_timesteps(num_inference_steps, device=device)\n    timesteps = self.scheduler.timesteps\n\n    # 5. Prepare latent variables\n    num_channels_latents = self.unet.config.in_channels\n    latents = self.prepare_latents(\n        batch_size * num_images_per_prompt,\n        num_channels_latents,\n        height,\n        width,\n        prompt_embeds.dtype,\n        device,\n        generator,\n        latents,\n    )\n\n    # 6. Denoising loop\n    num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n    all_latents = [latents]\n    all_log_probs = []\n    with self.progress_bar(total=num_inference_steps) as progress_bar:\n        for i, t in enumerate(timesteps):\n            # expand the latents if we are doing classifier free guidance\n            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n            # predict the noise residual\n            noise_pred = self.unet(\n                latent_model_input,\n                t,\n                encoder_hidden_states=prompt_embeds,\n                cross_attention_kwargs=cross_attention_kwargs,\n                return_dict=False,\n            )[0]\n\n            # perform guidance\n            if do_classifier_free_guidance:\n                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n            if do_classifier_free_guidance and guidance_rescale > 0.0:\n                # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n                noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=guidance_rescale)\n\n            # compute the previous noisy sample x_t -> x_t-1\n            scheduler_output = scheduler_step(self.scheduler, noise_pred, t, latents, eta)\n            latents = scheduler_output.latents\n            log_prob = scheduler_output.log_probs\n\n            all_latents.append(latents)\n            all_log_probs.append(log_prob)\n\n            # call the callback, if provided\n            if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n                progress_bar.update()\n                if callback is not None and i % callback_steps == 0:\n                    callback(i, t, latents)\n\n    if not output_type == \"latent\":\n        image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n        image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)\n    else:\n        image = latents\n        has_nsfw_concept = None\n\n    if has_nsfw_concept is None:\n        do_denormalize = [True] * image.shape[0]\n    else:\n        do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n\n    image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n\n    # Offload last model to CPU\n    if hasattr(self, \"final_offload_hook\") and self.final_offload_hook is not None:\n        self.final_offload_hook.offload()\n\n    return DDPOPipelineOutput(image, all_latents, all_log_probs)\n\n\nclass DefaultDDPOStableDiffusionPipeline(DDPOStableDiffusionPipeline):\n    def __init__(self, pretrained_model_name: str, *, pretrained_model_revision: str = \"main\", use_lora: bool = True):\n        self.sd_pipeline = StableDiffusionPipeline.from_pretrained(\n            pretrained_model_name, revision=pretrained_model_revision\n        )\n\n        self.use_lora = use_lora\n        self.pretrained_model = pretrained_model_name\n        self.pretrained_revision = pretrained_model_revision\n\n        try:\n            self.sd_pipeline.load_lora_weights(\n                pretrained_model_name,\n                weight_name=\"pytorch_lora_weights.safetensors\",\n                revision=pretrained_model_revision,\n            )\n            self.use_lora = True\n        except OSError:\n            if use_lora:\n                warnings.warn(\n                    \"If you are aware that the pretrained model has no lora weights to it, ignore this message. \"\n                    \"Otherwise please check the if `pytorch_lora_weights.safetensors` exists in the model folder.\"\n                )\n\n        self.sd_pipeline.scheduler = DDIMScheduler.from_config(self.sd_pipeline.scheduler.config)\n        self.sd_pipeline.safety_checker = None\n\n        # memory optimization\n        self.sd_pipeline.vae.requires_grad_(False)\n        self.sd_pipeline.text_encoder.requires_grad_(False)\n        self.sd_pipeline.unet.requires_grad_(not self.use_lora)\n\n    def __call__(self, *args, **kwargs) -> DDPOPipelineOutput:\n        return pipeline_step(self.sd_pipeline, *args, **kwargs)\n\n    def scheduler_step(self, *args, **kwargs) -> DDPOSchedulerOutput:\n        return scheduler_step(self.sd_pipeline.scheduler, *args, **kwargs)\n\n    @property\n    def unet(self):\n        return self.sd_pipeline.unet\n\n    @property\n    def vae(self):\n        return self.sd_pipeline.vae\n\n    @property\n    def tokenizer(self):\n        return self.sd_pipeline.tokenizer\n\n    @property\n    def scheduler(self):\n        return self.sd_pipeline.scheduler\n\n    @property\n    def text_encoder(self):\n        return self.sd_pipeline.text_encoder\n\n    @property\n    def autocast(self):\n        return contextlib.nullcontext if self.use_lora else None\n\n    def save_pretrained(self, output_dir):\n        if self.use_lora:\n            state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(self.sd_pipeline.unet))\n            self.sd_pipeline.save_lora_weights(save_directory=output_dir, unet_lora_layers=state_dict)\n        self.sd_pipeline.save_pretrained(output_dir)\n\n    def set_progress_bar_config(self, *args, **kwargs):\n        self.sd_pipeline.set_progress_bar_config(*args, **kwargs)\n\n    def get_trainable_layers(self):\n        if self.use_lora:\n            lora_config = LoraConfig(\n                r=4,\n                lora_alpha=4,\n                init_lora_weights=\"gaussian\",\n                target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n            )\n            self.sd_pipeline.unet.add_adapter(lora_config)\n\n            # To avoid accelerate unscaling problems in FP16.\n            for param in self.sd_pipeline.unet.parameters():\n                # only upcast trainable parameters (LoRA) into fp32\n                if param.requires_grad:\n                    param.data = param.to(torch.float32)\n            return self.sd_pipeline.unet\n        else:\n            return self.sd_pipeline.unet\n\n    def save_checkpoint(self, models, weights, output_dir):\n        if len(models) != 1:\n            raise ValueError(\"Given how the trainable params were set, this should be of length 1\")\n        if self.use_lora and hasattr(models[0], \"peft_config\") and getattr(models[0], \"peft_config\", None) is not None:\n            state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(models[0]))\n            self.sd_pipeline.save_lora_weights(save_directory=output_dir, unet_lora_layers=state_dict)\n        elif not self.use_lora and isinstance(models[0], UNet2DConditionModel):\n            models[0].save_pretrained(os.path.join(output_dir, \"unet\"))\n        else:\n            raise ValueError(f\"Unknown model type {type(models[0])}\")\n\n    def load_checkpoint(self, models, input_dir):\n        if len(models) != 1:\n            raise ValueError(\"Given how the trainable params were set, this should be of length 1\")\n        if self.use_lora:\n            lora_state_dict, network_alphas = self.sd_pipeline.lora_state_dict(\n                input_dir, weight_name=\"pytorch_lora_weights.safetensors\"\n            )\n            self.sd_pipeline.load_lora_into_unet(lora_state_dict, network_alphas=network_alphas, unet=models[0])\n\n        elif not self.use_lora and isinstance(models[0], UNet2DConditionModel):\n            load_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder=\"unet\")\n            models[0].register_to_config(**load_model.config)\n            models[0].load_state_dict(load_model.state_dict())\n            del load_model\n        else:\n            raise ValueError(f\"Unknown model type {type(models[0])}\")\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/environment/__init__.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# flake8: noqa\n\nfrom .base_environment import TextEnvironment, TextHistory\n"
    },
    {
        "file": "kwaai-pai/app/trl/trl/environment/base_environment.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\n\nimport re\nimport warnings\n\nimport torch\nfrom accelerate.utils import extract_model_from_parallel\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\nfrom ..import_utils import is_rich_available\n\n\nif is_rich_available():\n    from rich import print\n    from rich.text import Text\n\n\nclass StringStoppingCriteria(StoppingCriteria):\n    \"\"\"Custom `StoppingCriteria` which checks if all generations in the batch are completed.\"\"\"\n\n    def __init__(self, stop_strings, tokenizer):\n        self.stop_strings = stop_strings\n        self.tokenizer = tokenizer\n        self.first_call = True\n\n    def __call__(self, input_ids, scores, **kwargs):\n        \"\"\"Returns true if all generated sequences contain any of the stop strings.\"\"\"\n        if self.first_call:\n            self.generated_tokens = [1 for _ in range(input_ids.shape[0])]\n            self.start_length = input_ids.shape[-1] - 1\n            self.first_call = False\n        decoded_generations = self.tokenizer.batch_decode(input_ids[:, self.start_length :])\n        done = []\n\n        for i, decoded_generation in enumerate(decoded_generations):\n            sequence_complete = any([stop_string in decoded_generation for stop_string in self.stop_strings])\n            done.append(sequence_complete)\n            if not sequence_complete:\n                self.generated_tokens[i] += 1\n\n        if all(done):\n            self.first_call = True\n\n        return all(done)\n\n\nclass TextHistory:\n    \"\"\"The TextHistory class keeps track of the history of an interaction between the language model and the environment.\"\"\"\n\n    def __init__(self, text, tokens, system=True):\n        \"\"\"\n        Initialize TextHistory.\n\n        args:\n            text (`str`): The text of the first segment.\n            tokens (`torch.LongTensor`): The tokens of the first segment.\n            system (`bool`, *optional*): Whether the first segment is a system or user segment.\n        \"\"\"\n        self.system_spans = []\n        self.text_spans = []\n        self.token_spans = []\n        self.token_masks = torch.tensor([], dtype=torch.long).to(tokens.device)\n        self.text = \"\"\n        self.tokens = torch.tensor([], dtype=torch.long).to(tokens.device)\n        self.completed = False\n        self.truncated = False\n        self.reward = 0.0\n\n        self.prompt_color = \"black on grey85\"\n        self.system_color = \"black on cyan3\"\n        self.model_color = \"black on deep_sky_blue1\"\n        self.reward_color = \"black on plum1\"\n\n        self.append_segment(text, tokens, system=system)\n\n    def append_segment(self, text, tokens, system=True):\n        \"\"\"\n        Append a new segment to the history.\n\n        args:\n            text (`str`): The text of the new segment.\n            tokens (`torch.LongTensor`): The tokens of the new segment.\n            system (`bool`, *optional*): Whether the new segment is a system or user segment.\n        \"\"\"\n\n        if len(text) == 0 or len(tokens) == 0:\n            raise ValueError(\"Can't append empty text or token list to history.\")\n\n        original_text_length = len(self.text)\n\n        self.text += text\n        self.text_spans.append((original_text_length, len(self.text)))\n        self.system_spans.append(system)\n\n        original_token_length = len(self.tokens)\n\n        self.tokens = torch.cat((self.tokens, tokens))\n        if system:\n            self.token_masks = torch.cat((self.token_masks, torch.zeros_like(tokens)))\n        else:\n            self.token_masks = torch.cat((self.token_masks, torch.ones_like(tokens)))\n        self.token_spans.append((original_token_length, len(self.tokens)))\n\n    def complete(self, truncated=False):\n        \"\"\"\n        Mark the history as completed.\n        \"\"\"\n        self.completed = True\n        self.truncated = truncated\n\n    @property\n    def last_text_segment(self):\n        \"\"\"\n        Get the last text segment.\n        \"\"\"\n        start, end = self.text_spans[-1]\n        return self.text[start:end]\n\n    def split_query_response_tokens(self):\n        \"\"\"\n        Split the tokens into query and response tokens.\n        \"\"\"\n        split_index = self.token_spans[0][1]\n        query = self.tokens[:split_index]\n        response = self.tokens[split_index:]\n        mask = self.token_masks[split_index:]\n\n        return query, response, mask\n\n    def show_text(self, show_legend=False):\n        \"\"\"\n        Print the text history.\n        \"\"\"\n        if not is_rich_available():\n            warnings.warn(\"install rich to display text\")\n            return\n\n        text = Text(self.text)\n        text.stylize(self.prompt_color, self.text_spans[0][0], self.text_spans[1][0])\n        for i, (start, end) in enumerate(self.text_spans[1:]):\n            if self.system_spans[i + 1]:\n                text.stylize(self.system_color, start, end)\n            else:\n                text.stylize(self.model_color, start, end)\n\n        text.append(f\"\\n\\nReward: {self.reward}\", style=self.reward_color)\n        print(text)\n\n        if show_legend:\n            self.show_colour_legend()\n\n    def show_tokens(self, tokenizer, show_legend=False):\n        \"\"\"\n        Print the history tokens.\n        \"\"\"\n        if not is_rich_available():\n            warnings.warn(\"install rich to display tokens\")\n            return\n\n        text = Text()\n        prompt_end = self.token_spans[0][1]\n        for i, (token, mask) in enumerate(zip(self.tokens, self.token_masks)):\n            if i < prompt_end:\n                text.append(tokenizer.convert_ids_to_tokens(token.item()), style=self.prompt_color)\n                text.append(\" \")\n            elif mask == 0:\n                text.append(tokenizer.convert_ids_to_tokens(token.item()), style=self.system_color)\n                text.append(\" \")\n            else:\n                text.append(tokenizer.convert_ids_to_tokens(token.item()), style=self.model_color)\n                text.append(\" \")\n        text.append(f\"\\n\\nReward: {self.reward}\", style=self.reward_color)\n        print(text)\n        if show_legend:\n            self.show_colour_legend()\n\n    def show_colour_legend(self):\n        \"\"\"\n        Print the colour legend.\n        \"\"\"\n        if not is_rich_available():\n            warnings.warn(\"install rich to display colour legend\")\n            return\n        text = Text(\"\\n\\n(Colour Legend: \")\n        text.append(\"Prompt\", style=self.prompt_color)\n        text.append(\"|\")\n        text.append(\"System\", style=self.system_color)\n        text.append(\"|\")\n        text.append(\"Model\", style=self.model_color)\n        text.append(\"|\")\n        text.append(\"Reward\", style=self.reward_color)\n        text.append(\")\")\n        print(text)\n\n\nclass TextEnvironment:\n    \"\"\"\n    The TextEnvironment enables interaction of a LLM with an environment using tools.\n    \"\"\"\n\n    def __init__(\n        self,\n        model=None,\n        tokenizer=None,\n        tools=None,\n        reward_fn=None,\n        prompt=None,\n        max_turns=4,\n        max_tool_reponse=100,\n        max_length=None,\n        generation_kwargs=None,\n    ):\n        \"\"\"\n        Initialize TextEnvironment.\n\n        Args:\n            model (`PreTrainedModelWrapper`): The model to use for generation.\n            tokenizer (`transformers.PreTrainedTokenizer`): The tokenizer to use for generation.\n            tools (list): A list of tools to use for interaction.\n            reward_fn (function): A function that takes a string and returns a reward.\n            prompt (str): The base prompt to use for generation. Is prepended to the tasks.\n            max_turns (Optional[int]): The maximum number of turns to allow.\n            max_tool_response (Optional[int]): The maximum number of characters to allow in a tool response.\n            max_length (Optional[int]): The maximum number of tokens to allow in an episode.\n            generation_kwargs (Optional[dict]): A dictionary of keyword arguments to pass to the model's generate method.\n        \"\"\"\n        self.model = model\n        self.tokenizer = tokenizer\n        self.prompt = prompt\n        if isinstance(tools, dict):\n            self.tools = tools\n        else:\n            self.tools = dict([(tool.__class__.__name__, tool) for tool in tools])\n        self.reward_fn = reward_fn\n        self.max_length = max_length\n        self.request_token = \"<request>\"\n        self.call_token = \"<call>\"\n        self.response_token = \"<response>\"\n        self.submit_token = \"<submit>\"\n        self.max_turns = max_turns\n        self.max_tool_response = max_tool_reponse\n\n        if generation_kwargs is None:\n            self.generation_kwargs = dict()\n        else:\n            self.generation_kwargs = generation_kwargs\n\n        self.is_encoder_decoder = hasattr(self.model, \"is_encoder_decoder\")\n        self.current_device = extract_model_from_parallel(self.model).pretrained_model.device\n\n    def run(self, queries, **rewards_kwargs):\n        \"\"\"\n        Run the environment on a list of queries.\n\n        Args:\n            queries (list[str]): A list of queries to run the model in the environment on.\n        \"\"\"\n        turns = 0\n\n        queries = [self.prompt + task for task in queries]\n        queries_tokens = [\n            self.tokenizer(query, return_tensors=\"pt\").input_ids[0].to(self.model.pretrained_model.device)\n            for query in queries\n        ]\n\n        histories = [TextHistory(q, qt, system=True) for q, qt in zip(queries, queries_tokens)]\n\n        while any([not history.completed for history in histories]) and turns < self.max_turns:\n            histories = self.generate(histories)\n            histories = self.tasks_end_check(histories)\n            # TODO: make this parallel rather than for-loop\n            for i in range(len(histories)):\n                histories[i] = self.step(histories[i])\n            histories = self.tasks_end_check(histories, model_turn=False)\n            turns += 1\n        self.compute_reward(histories, **rewards_kwargs)\n\n        # convert a list of (q, r, m) tuples to lists of all qs, rs, and ms respectively\n        queries, responses, masks = map(list, zip(*[history.split_query_response_tokens() for history in histories]))\n\n        rewards = [history.reward for history in histories]\n        return queries, responses, masks, rewards, histories\n\n    def step(self, history):\n        \"\"\"\n        Step the environment forward one turn.\n\n        Args:\n            history (`TextHistory`): The history to step forward.\n        \"\"\"\n        truncated, ended = self.task_end_check(history)\n        if ended:\n            history.complete(truncated=truncated)\n        if history.completed:\n            return history\n\n        tool, query = self.parse_tool_call(history.last_text_segment)\n        if tool is None or query is None:\n            response = f\"Unknown tool call: {history.last_text_segment}\"\n        else:\n            if tool not in self.tools:\n                response = f\"Unknown tool {tool}.\"\n            try:\n                response = self.tools[tool](query)\n            except Exception as error:\n                response = f\"Tool error: {str(error)}\"\n\n        if len(response) > self.max_tool_response:\n            response = response[: (self.max_tool_response - 3)] + \"...\"\n\n        history.append_segment(\n            response + self.response_token,\n            self.tokenizer(response + self.response_token, return_tensors=\"pt\")\n            .input_ids[0]\n            .to(self.model.pretrained_model.device),\n            system=True,\n        )\n\n        return history\n\n    def parse_tool_call(self, text):\n        \"\"\"\n        Parse request string. Expected format: <request><tool_name>query<call>\n        \"\"\"\n        result = re.search(f\"(?<={self.request_token}).*?(?={self.call_token})\", text, re.DOTALL)\n\n        # if we can't find a <request>/<call> span we return none\n        if result is None:\n            return None, None\n        else:\n            extracted_text = result.group()\n\n        result = re.search(r\"<(.*?)>\", extracted_text)\n\n        # if we can't find a tool name we return none\n        if result is None:\n            return None, None\n        else:\n            tool = result.group(1)\n\n        # split off the tool name\n        query = \">\".join(extracted_text.split(\">\")[1:])\n\n        return tool, query\n\n    def compute_reward(self, histories, **reward_kwargs):\n        \"\"\"\n        Compute the reward for a list of histories.\n        \"\"\"\n        rewards = self.reward_fn([history.last_text_segment for history in histories], **reward_kwargs)\n        for history, reward in zip(histories, rewards):\n            history.reward = reward\n        return histories\n\n    def generate(self, histories):\n        \"\"\"\n        Generate responses for a list of histories.\n        \"\"\"\n        active_histories = [i for i, history in enumerate(histories) if not history.completed]\n\n        query_tensors = [histories[i].tokens for i in active_histories]\n        response_tensors = self._generate_batched(query_tensors)\n        response_texts = self.tokenizer.batch_decode(response_tensors)\n\n        for i, response_text, response_tensor in zip(active_histories, response_texts, response_tensors):\n            histories[i].append_segment(response_text, response_tensor, system=False)\n\n        return histories\n\n    def tasks_end_check(self, histories, model_turn=True):\n        \"\"\"\n        Check if the current generation sequences have finished.\n        \"\"\"\n        for history in histories:\n            if not history.completed:\n                truncated, ended = self.task_end_check(history, model_turn=model_turn)\n                if ended:\n                    history.complete(truncated=truncated)\n        return histories\n\n    def task_end_check(self, history, model_turn=True):\n        \"\"\"\n        Check if the current generation sequence has finished.\n        \"\"\"\n        truncated = False\n        ended = False\n        if history.completed:\n            return truncated, ended\n        if self.max_length is not None and len(self.tokenizer(history.text).input_ids[0]) > self.max_length:\n            truncated = True\n            ended = True\n        elif self.tokenizer.eos_token in history.text:\n            ended = True\n        elif model_turn and not (\n            (self.request_token in history.last_text_segment and self.call_token in history.last_text_segment)\n            or self.submit_token in history.last_text_segment\n        ):\n            ended = True\n        elif self.submit_token in history.last_text_segment:\n            ended = True\n        return truncated, ended\n\n    def _generate_batched(\n        self,\n        query_tensors,\n        batch_size: int = 16,\n        pad_to_multiple_of: int = None,\n    ):\n        \"\"\"\n        Generate responses for a list of query tensors.\n\n        args:\n            query_tensors (list[torch.Tensor]): A list of query tensors to generate responses for.\n            batch_size (int): The batch size to use for generation.\n            pad_to_multiple_of (int): The padding length to use for generation.\n        \"\"\"\n        outputs = []\n        padding_side_default = self.tokenizer.padding_side\n        if not self.is_encoder_decoder:\n            self.tokenizer.padding_side = \"left\"\n\n        # in case we have fewer examples than bs\n        batch_size = min(len(query_tensors), batch_size)\n\n        for i in range(0, len(query_tensors), batch_size):\n            # prevent overflow if query tensors are not even multiple of bs\n            end_index = min(len(query_tensors), i + batch_size)\n\n            batch = query_tensors[i:end_index]\n            batch_mask = [torch.ones_like(element) for element in batch]\n            inputs = {\"input_ids\": batch, \"attention_mask\": batch_mask}\n\n            padded_inputs = self.tokenizer.pad(\n                inputs,\n                padding=True,\n                max_length=None,\n                pad_to_multiple_of=pad_to_multiple_of,\n                return_tensors=\"pt\",\n            ).to(self.current_device)\n\n            stopping_criteria = StringStoppingCriteria([self.call_token, self.submit_token], self.tokenizer)\n\n            self.generation_kwargs[\"stopping_criteria\"] = StoppingCriteriaList([stopping_criteria])\n\n            generations = extract_model_from_parallel(self.model).generate(**padded_inputs, **self.generation_kwargs)\n\n            for generation, mask, generated_tokens in zip(\n                generations, padded_inputs[\"attention_mask\"], stopping_criteria.generated_tokens\n            ):\n                if not self.is_encoder_decoder:\n                    output = generation[(1 - mask).sum() :]  # remove padding\n                else:\n                    output = generation\n\n                if not self.is_encoder_decoder:\n                    output = output[(mask).sum() :]  # remove prompt\n\n                # remove chunk generated after stopping criteria in batch mode\n                outputs.append(output[:generated_tokens])\n        self.tokenizer.padding_side = padding_side_default\n        return outputs\n"
    },
    {
        "file": "kwaai-pai/app/trl/scripts/stale.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"\nScript to close stale issue. Taken in part from the AllenNLP repository.\nhttps://github.com/allenai/allennlp.\n\"\"\"\nimport os\nfrom datetime import datetime as dt\nfrom datetime import timezone\n\nfrom github import Github\n\n\nLABELS_TO_EXEMPT = [\n    \"good first issue\",\n    \"good second issue\",\n    \"feature request\",\n]\n\n\ndef main():\n    g = Github(os.environ[\"GITHUB_TOKEN\"])\n    repo = g.get_repo(\"huggingface/trl\")\n    open_issues = repo.get_issues(state=\"open\")\n\n    for issue in open_issues:\n        comments = sorted([comment for comment in issue.get_comments()], key=lambda i: i.created_at, reverse=True)\n        last_comment = comments[0] if len(comments) > 0 else None\n        if (\n            last_comment is not None\n            and last_comment.user.login == \"github-actions[bot]\"\n            and (dt.now(timezone.utc) - issue.updated_at).days > 7\n            and (dt.now(timezone.utc) - issue.created_at).days >= 30\n            and not any(label.name.lower() in LABELS_TO_EXEMPT for label in issue.get_labels())\n        ):\n            issue.edit(state=\"closed\")\n        elif (\n            (dt.now(timezone.utc) - issue.updated_at).days > 23\n            and (dt.now(timezone.utc) - issue.created_at).days >= 30\n            and not any(label.name.lower() in LABELS_TO_EXEMPT for label in issue.get_labels())\n        ):\n            issue.create_comment(\n                \"This issue has been automatically marked as stale because it has not had \"\n                \"recent activity. If you think this still needs to be addressed \"\n                \"please comment on this thread.\\n\\n\"\n            )\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/hello_world.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# 0. imports\nimport torch\nfrom transformers import GPT2Tokenizer\n\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n\n\n# 1. load a pretrained model\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\nmodel_ref = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\n# 2. initialize trainer\nppo_config = {\"batch_size\": 1}\nconfig = PPOConfig(**ppo_config)\nppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)\n\n# 3. encode a query\nquery_txt = \"This morning I went to the \"\nquery_tensor = tokenizer.encode(query_txt, return_tensors=\"pt\").to(model.pretrained_model.device)\n\n# 4. generate model response\ngeneration_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n    \"max_new_tokens\": 20,\n}\nresponse_tensor = ppo_trainer.generate([item for item in query_tensor], return_prompt=False, **generation_kwargs)\nresponse_txt = tokenizer.decode(response_tensor[0])\n\n# 5. define a reward for response\n# (this could be any reward such as human feedback or output from another model)\nreward = [torch.tensor(1.0, device=model.pretrained_model.device)]\n\n# 6. train model with ppo\ntrain_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/scripts/ppo_multi_adapter.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# coding=utf-8\n\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, BitsAndBytesConfig, HfArgumentParser\n\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\nfrom trl.core import LengthSampler\nfrom trl.import_utils import is_npu_available, is_xpu_available\n\n\ninput_min_text_length = 6\ninput_max_text_length = 12\n\n\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    The name of the Casual LM model we wish to fine with PPO\n    \"\"\"\n\n    model_name: Optional[str] = field(default=\"huggyllama/llama-7b\", metadata={\"help\": \"the model name\"})\n    dataset_name: Optional[str] = field(default=\"Anthropic/hh-rlhf\", metadata={\"help\": \"the dataset name\"})\n    rm_adapter: Optional[str] = field(\n        default=\"trl-lib/llama-7b-hh-rm-adapter\", metadata={\"help\": \"the rm adapter name\"}\n    )\n    log_with: Optional[str] = field(default=None, metadata={\"help\": \"use 'wandb' to log with wandb\"})\n    use_safetensors: Optional[bool] = field(default=False, metadata={\"help\": \"Use safetensors\"})\n    seed: Optional[int] = field(default=0, metadata={\"help\": \"the random seed\"})\n    use_score_scaling: Optional[bool] = field(default=False, metadata={\"help\": \"Use score scaling\"})\n    use_score_norm: Optional[bool] = field(\n        default=False, metadata={\"help\": \"Use score normalization. Only applicable if use_score_scaling is True\"}\n    )\n    score_clip: Optional[float] = field(default=None, metadata={\"help\": \"Score clipping\"})\n\n\nparser = HfArgumentParser(ScriptArguments)\nscript_args = parser.parse_args_into_dataclasses()[0]\n\n\ndef create_and_prepare_dataset(tokenizer):\n    dataset = load_dataset(script_args.dataset_name, split=\"train[:1%]\")\n\n    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n\n    def tokenize(example):\n        text_size = input_size()\n        example[\"input_ids\"] = tokenizer.encode(example[\"chosen\"])[:text_size]\n        example[\"query\"] = tokenizer.decode(example[\"input_ids\"])\n        return example\n\n    dataset = dataset.map(tokenize, batched=False)\n    dataset.set_format(\"torch\")\n    return dataset\n\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nnf4_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16\n)\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\n    script_args.model_name,\n    device_map={\"\": \"xpu:0\"} if is_xpu_available() else {\"\": \"npu:0\"} if is_npu_available else {\"\": 0},\n    peft_config=lora_config,\n    quantization_config=nf4_config,\n    reward_adapter=script_args.rm_adapter,\n    use_safetensors=script_args.use_safetensors,\n)\ntokenizer = AutoTokenizer.from_pretrained(script_args.model_name)\n\ntokenizer.pad_token = tokenizer.eos_token\n\ndataset = create_and_prepare_dataset(tokenizer)\n\n\ndef collator(data):\n    return dict((key, [d[key] for d in data]) for key in data[0])\n\n\nconfig = PPOConfig(\n    model_name=script_args.model_name,\n    log_with=script_args.log_with,\n    learning_rate=1e-5,\n    batch_size=8,\n    mini_batch_size=2,\n    gradient_accumulation_steps=2,\n    optimize_cuda_cache=True,\n    seed=script_args.seed,\n    use_score_scaling=script_args.use_score_scaling,\n    use_score_norm=script_args.use_score_norm,\n    score_clip=script_args.score_clip,\n)\n\nppo_trainer = PPOTrainer(\n    config,\n    model,\n    ref_model=None,\n    tokenizer=tokenizer,\n    dataset=dataset,\n    data_collator=collator,\n)\n\ngeneration_kwargs = {\n    \"top_k\": 0.0,\n    \"top_p\": 0.9,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.pad_token_id,\n    \"max_new_tokens\": 32,\n}\n\nfor epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n    question_tensors = batch[\"input_ids\"]\n\n    response_tensors = ppo_trainer.generate(\n        question_tensors,\n        return_prompt=False,\n        **generation_kwargs,\n    )\n    batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n\n    # Compute reward score\n    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(ppo_trainer.accelerator.device)\n    raw_rewards = ppo_trainer.accelerator.unwrap_model(ppo_trainer.model).compute_reward_score(**inputs)\n    rewards = [raw_rewards[i, -1, 1] for i in range(len(raw_rewards))]  # take last token\n\n    # Run PPO step\n    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n    ppo_trainer.log_stats(stats, batch, rewards)\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/scripts/ddpo.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# Copyright 2023 metric-space, The HuggingFace Team. All rights reserved.\n\n\nimport os\nfrom dataclasses import dataclass, field\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport tyro\nfrom huggingface_hub import hf_hub_download\nfrom huggingface_hub.utils import EntryNotFoundError\nfrom transformers import CLIPModel, CLIPProcessor\n\nfrom trl import DDPOConfig, DDPOTrainer, DefaultDDPOStableDiffusionPipeline\nfrom trl.import_utils import is_npu_available, is_xpu_available\n\n\n@dataclass\nclass ScriptArguments:\n    hf_user_access_token: str\n    pretrained_model: str = \"runwayml/stable-diffusion-v1-5\"\n    \"\"\"the pretrained model to use\"\"\"\n    pretrained_revision: str = \"main\"\n    \"\"\"the pretrained model revision to use\"\"\"\n    hf_hub_model_id: str = \"ddpo-finetuned-stable-diffusion\"\n    \"\"\"HuggingFace repo to save model weights to\"\"\"\n    hf_hub_aesthetic_model_id: str = \"trl-lib/ddpo-aesthetic-predictor\"\n    \"\"\"HuggingFace model ID for aesthetic scorer model weights\"\"\"\n    hf_hub_aesthetic_model_filename: str = \"aesthetic-model.pth\"\n    \"\"\"HuggingFace model filename for aesthetic scorer model weights\"\"\"\n    use_lora: bool = True\n    \"\"\"Whether to use LoRA.\"\"\"\n\n    ddpo_config: DDPOConfig = field(\n        default_factory=lambda: DDPOConfig(\n            num_epochs=200,\n            train_gradient_accumulation_steps=1,\n            sample_num_steps=50,\n            sample_batch_size=6,\n            train_batch_size=3,\n            sample_num_batches_per_epoch=4,\n            per_prompt_stat_tracking=True,\n            per_prompt_stat_tracking_buffer_size=32,\n            tracker_project_name=\"stable_diffusion_training\",\n            log_with=\"wandb\",\n            project_kwargs={\n                \"logging_dir\": \"./logs\",\n                \"automatic_checkpoint_naming\": True,\n                \"total_limit\": 5,\n                \"project_dir\": \"./save\",\n            },\n        )\n    )\n\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(768, 1024),\n            nn.Dropout(0.2),\n            nn.Linear(1024, 128),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.Dropout(0.1),\n            nn.Linear(64, 16),\n            nn.Linear(16, 1),\n        )\n\n    @torch.no_grad()\n    def forward(self, embed):\n        return self.layers(embed)\n\n\nclass AestheticScorer(torch.nn.Module):\n    \"\"\"\n    This model attempts to predict the aesthetic score of an image. The aesthetic score\n    is a numerical approximation of how much a specific image is liked by humans on average.\n    This is from https://github.com/christophschuhmann/improved-aesthetic-predictor\n    \"\"\"\n\n    def __init__(self, *, dtype, model_id, model_filename):\n        super().__init__()\n        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n        self.mlp = MLP()\n        try:\n            cached_path = hf_hub_download(model_id, model_filename)\n        except EntryNotFoundError:\n            cached_path = os.path.join(model_id, model_filename)\n        state_dict = torch.load(cached_path, map_location=torch.device(\"cpu\"))\n        self.mlp.load_state_dict(state_dict)\n        self.dtype = dtype\n        self.eval()\n\n    @torch.no_grad()\n    def __call__(self, images):\n        device = next(self.parameters()).device\n        inputs = self.processor(images=images, return_tensors=\"pt\")\n        inputs = {k: v.to(self.dtype).to(device) for k, v in inputs.items()}\n        embed = self.clip.get_image_features(**inputs)\n        # normalize embedding\n        embed = embed / torch.linalg.vector_norm(embed, dim=-1, keepdim=True)\n        return self.mlp(embed).squeeze(1)\n\n\ndef aesthetic_scorer(hub_model_id, model_filename):\n    scorer = AestheticScorer(\n        model_id=hub_model_id,\n        model_filename=model_filename,\n        dtype=torch.float32,\n    )\n    if is_npu_available():\n        scorer = scorer.npu()\n    elif is_xpu_available():\n        scorer = scorer.xpu()\n    else:\n        scorer = scorer.cuda()\n\n    def _fn(images, prompts, metadata):\n        images = (images * 255).round().clamp(0, 255).to(torch.uint8)\n        scores = scorer(images)\n        return scores, {}\n\n    return _fn\n\n\n# list of example prompts to feed stable diffusion\nanimals = [\n    \"cat\",\n    \"dog\",\n    \"horse\",\n    \"monkey\",\n    \"rabbit\",\n    \"zebra\",\n    \"spider\",\n    \"bird\",\n    \"sheep\",\n    \"deer\",\n    \"cow\",\n    \"goat\",\n    \"lion\",\n    \"frog\",\n    \"chicken\",\n    \"duck\",\n    \"goose\",\n    \"bee\",\n    \"pig\",\n    \"turkey\",\n    \"fly\",\n    \"llama\",\n    \"camel\",\n    \"bat\",\n    \"gorilla\",\n    \"hedgehog\",\n    \"kangaroo\",\n]\n\n\ndef prompt_fn():\n    return np.random.choice(animals), {}\n\n\ndef image_outputs_logger(image_data, global_step, accelerate_logger):\n    # For the sake of this example, we will only log the last batch of images\n    # and associated data\n    result = {}\n    images, prompts, _, rewards, _ = image_data[-1]\n\n    for i, image in enumerate(images):\n        prompt = prompts[i]\n        reward = rewards[i].item()\n        result[f\"{prompt:.25} | {reward:.2f}\"] = image.unsqueeze(0)\n\n    accelerate_logger.log_images(\n        result,\n        step=global_step,\n    )\n\n\nif __name__ == \"__main__\":\n    args = tyro.cli(ScriptArguments)\n\n    pipeline = DefaultDDPOStableDiffusionPipeline(\n        args.pretrained_model, pretrained_model_revision=args.pretrained_revision, use_lora=args.use_lora\n    )\n\n    trainer = DDPOTrainer(\n        args.ddpo_config,\n        aesthetic_scorer(args.hf_hub_aesthetic_model_id, args.hf_hub_aesthetic_model_filename),\n        prompt_fn,\n        pipeline,\n        image_samples_hook=image_outputs_logger,\n    )\n\n    trainer.train()\n\n    trainer.push_to_hub(args.hf_hub_model_id, token=args.hf_user_access_token)\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/scripts/sft.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# coding=utf-8\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional\n\nimport torch\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nfrom peft import LoraConfig\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\nfrom trl import SFTTrainer, is_xpu_available\nfrom utils.config import MODEL_NAME\n\ntqdm.pandas()\n\n# Define and parse arguments.\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    The name of the Casual LM model we wish to fine with SFTTrainer\n    \"\"\"\n\n    model_name: Optional[str] = field(default=MODEL_NAME, metadata={\"help\": \"the model name\"})\n    dataset_name: Optional[str] = field(\n        default=\"timdettmers/openassistant-guanaco\", metadata={\"help\": \"the dataset name\"}\n    )\n    dataset_text_field: Optional[str] = field(default=\"text\", metadata={\"help\": \"the text field of the dataset\"})\n    report_to: Optional[str] = field(default=\"none\", metadata={\"help\": \"use 'wandb' to log with wandb\"})\n    learning_rate: Optional[float] = field(default=1.41e-5, metadata={\"help\": \"the learning rate\"})\n    batch_size: Optional[int] = field(default=16, metadata={\"help\": \"the batch size\"})  # Reducir el tama\u00f1o del lote\n    seq_length: Optional[int] = field(default=512, metadata={\"help\": \"Input sequence length\"})\n    gradient_accumulation_steps: Optional[int] = field(\n        default=16, metadata={\"help\": \"the number of gradient accumulation steps\"}\n    )\n    load_in_8bit: Optional[bool] = field(default=False, metadata={\"help\": \"load the model in 8 bits precision\"})\n    load_in_4bit: Optional[bool] = field(default=False, metadata={\"help\": \"load the model in 4 bits precision\"})\n    use_peft: Optional[bool] = field(default=False, metadata={\"help\": \"Wether to use PEFT or not to train adapters\"})\n    trust_remote_code: Optional[bool] = field(default=False, metadata={\"help\": \"Enable `trust_remote_code`\"})\n    output_dir: Optional[str] = field(default=\"output\", metadata={\"help\": \"the output directory\"})\n    peft_lora_r: Optional[int] = field(default=64, metadata={\"help\": \"the r parameter of the LoRA adapters\"})\n    peft_lora_alpha: Optional[int] = field(default=16, metadata={\"help\": \"the alpha parameter of the LoRA adapters\"})\n    logging_steps: Optional[int] = field(default=1, metadata={\"help\": \"the number of logging steps\"})\n    use_auth_token: Optional[bool] = field(default=True, metadata={\"help\": \"Use HF auth token to access the model\"})\n    num_train_epochs: Optional[int] = field(default=2, metadata={\"help\": \"the number of training epochs\"})  # Reducir el n\u00famero de \u00e9pocas\n    max_steps: Optional[int] = field(default=1000, metadata={\"help\": \"the number of training steps\"})  # Limitar el n\u00famero total de pasos\n    save_steps: Optional[int] = field(\n        default=100, metadata={\"help\": \"Number of updates steps before two checkpoint saves\"}\n    )\n    save_total_limit: Optional[int] = field(default=10, metadata={\"help\": \"Limits total number of checkpoints.\"})\n    push_to_hub: Optional[bool] = field(default=False, metadata={\"help\": \"Push the model to HF Hub\"})\n    gradient_checkpointing: Optional[bool] = field(\n        default=False, metadata={\"help\": \"Whether to use gradient checkpointing or no\"}\n    )\n    gradient_checkpointing_kwargs: Optional[dict] = field(\n        default=None,\n        metadata={\n            \"help\": \"key word arguments to be passed along `torch.utils.checkpoint.checkpoint` method - e.g. `use_reentrant=False`\"\n        },\n    )\n    hub_model_id: Optional[str] = field(default=None, metadata={\"help\": \"The name of the model on HF Hub\"})\n    mixed_precision: Optional[str] = field(default=None, metadata={\"help\": \"Mixed precision training\"})  # Desactivar aceleraci\u00f3n mixta\n    target_modules: Optional[List[str]] = field(default=None, metadata={\"help\": \"Target modules for LoRA adapters\"})\n\n\n# Parser y argumentos\nparser = HfArgumentParser(ScriptArguments)\nscript_args = parser.parse_args_into_dataclasses()[0]\n\n# Step 1: Load the model\nif is_xpu_available():\n    if script_args.load_in_8bit and script_args.load_in_4bit:\n        raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\n    elif script_args.load_in_8bit or script_args.load_in_4bit:\n        quantization_config = BitsAndBytesConfig(\n            load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit\n        )\n        device_map = {\"\": f\"xpu:{Accelerator().local_process_index}\"}\n        torch_dtype = torch.bfloat16\n    else:\n        device_map = None\n        quantization_config = None\n        torch_dtype = None\nelse:\n    device_map = None\n    quantization_config = None\n    torch_dtype = None\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    script_args.model_name,\n    quantization_config=quantization_config,\n    device_map=device_map,\n    trust_remote_code=script_args.trust_remote_code,\n    torch_dtype=torch_dtype,\n    use_auth_token=script_args.use_auth_token,\n)\n\n# Step 2: Load the dataset\ndataset = load_dataset(script_args.dataset_name, split=\"train\")\n\n# Step 3: Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=script_args.output_dir,\n    per_device_train_batch_size=script_args.batch_size,\n    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n    learning_rate=script_args.learning_rate,\n    logging_steps=script_args.logging_steps,\n    num_train_epochs=script_args.num_train_epochs,\n    max_steps=script_args.max_steps,\n    report_to=script_args.report_to,\n    save_steps=script_args.save_steps,\n    save_total_limit=script_args.save_total_limit,\n    push_to_hub=script_args.push_to_hub,\n    hub_model_id=script_args.hub_model_id,\n    gradient_checkpointing=script_args.gradient_checkpointing,\n)\n\n# Step 4: Define the LoraConfig\nif script_args.use_peft:\n    peft_config = LoraConfig(\n        r=script_args.peft_lora_r,\n        lora_alpha=script_args.peft_lora_alpha,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=script_args.target_modules,\n    )\nelse:\n    peft_config = None\n\n# Step 5: Define the Trainer\ntokenizer = AutoTokenizer.from_pretrained(script_args.model_name, use_fast=True)\n\ntokenizer.pad_token = tokenizer.eos_token \n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    max_seq_length=script_args.seq_length,\n    train_dataset=dataset,\n    dataset_text_field=script_args.dataset_text_field,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\n\ntrainer.save_model(script_args.output_dir)"
    },
    {
        "file": "kwaai-pai/app/trl/examples/scripts/reward_modeling.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# coding=utf-8\n\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport tyro\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nfrom peft import LoraConfig\nfrom tqdm import tqdm\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n\nfrom trl import RewardConfig, RewardTrainer, is_xpu_available\n\n\ntqdm.pandas()\n\n\n@dataclass\nclass ScriptArguments:\n    model_name: str = \"facebook/opt-350m\"\n    \"\"\"the model name\"\"\"\n    dataset_name: str = \"Anthropic/hh-rlhf\"\n    \"\"\"the dataset name\"\"\"\n    dataset_text_field: str = \"text\"\n    \"\"\"the text field of the dataset\"\"\"\n    eval_split: str = \"none\"\n    \"\"\"the dataset split to evaluate on; default to 'none' (no evaluation)\"\"\"\n    load_in_8bit: bool = False\n    \"\"\"load the model in 8 bits precision\"\"\"\n    load_in_4bit: bool = False\n    \"\"\"load the model in 4 bits precision\"\"\"\n    trust_remote_code: bool = True\n    \"\"\"Enable `trust_remote_code`\"\"\"\n    reward_config: RewardConfig = field(\n        default_factory=lambda: RewardConfig(\n            output_dir=\"output\",\n            per_device_train_batch_size=64,\n            num_train_epochs=1,\n            gradient_accumulation_steps=16,\n            gradient_checkpointing=True,\n            gradient_checkpointing_kwargs={\"use_reentrant\": False},\n            learning_rate=1.41e-5,\n            report_to=\"tensorboard\",\n            remove_unused_columns=False,\n            optim=\"adamw_torch\",\n            logging_steps=500,\n            evaluation_strategy=\"no\",\n            max_length=512,\n        )\n    )\n    use_peft: bool = False\n    \"\"\"whether to use peft\"\"\"\n    peft_config: Optional[LoraConfig] = field(\n        default_factory=lambda: LoraConfig(\n            r=16,\n            lora_alpha=16,\n            bias=\"none\",\n            task_type=\"SEQ_CLS\",\n            modules_to_save=[\"scores\"],\n        ),\n    )\n\n\nargs = tyro.cli(ScriptArguments)\nargs.reward_config.evaluation_strategy = \"steps\" if args.eval_split != \"none\" else \"no\"\n\n\n# Step 1: Load the model\nif args.load_in_8bit and args.load_in_4bit:\n    raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\nelif args.load_in_8bit or args.load_in_4bit:\n    quantization_config = BitsAndBytesConfig(load_in_8bit=args.load_in_8bit, load_in_4bit=args.load_in_4bit)\n    # Copy the model to each device\n    device_map = (\n        {\"\": f\"xpu:{Accelerator().local_process_index}\"}\n        if is_xpu_available()\n        else {\"\": Accelerator().local_process_index}\n    )\nelse:\n    device_map = None\n    quantization_config = None\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    args.model_name,\n    quantization_config=quantization_config,\n    device_map=device_map,\n    trust_remote_code=args.trust_remote_code,\n    num_labels=1,\n)\n\n# Step 2: Load the dataset and pre-process it\ntokenizer = AutoTokenizer.from_pretrained(args.model_name)\ntrain_dataset = load_dataset(args.dataset_name, split=\"train\")\n\n\n# Tokenize chosen/rejected pairs of inputs\n# Adapt this section to your needs for custom datasets\ndef preprocess_function(examples):\n    new_examples = {\n        \"input_ids_chosen\": [],\n        \"attention_mask_chosen\": [],\n        \"input_ids_rejected\": [],\n        \"attention_mask_rejected\": [],\n    }\n    for chosen, rejected in zip(examples[\"chosen\"], examples[\"rejected\"]):\n        tokenized_chosen = tokenizer(chosen)\n        tokenized_rejected = tokenizer(rejected)\n\n        new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n        new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n        new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n        new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n\n    return new_examples\n\n\n# Preprocess the dataset and filter out examples that are longer than args.max_length\ntrain_dataset = train_dataset.map(\n    preprocess_function,\n    batched=True,\n    num_proc=4,\n)\ntrain_dataset = train_dataset.filter(\n    lambda x: len(x[\"input_ids_chosen\"]) <= args.reward_config.max_length\n    and len(x[\"input_ids_rejected\"]) <= args.reward_config.max_length\n)\n\nif args.eval_split == \"none\":\n    eval_dataset = None\nelse:\n    eval_dataset = load_dataset(args.dataset_name, split=args.eval_split)\n\n    eval_dataset = eval_dataset.map(\n        preprocess_function,\n        batched=True,\n        num_proc=4,\n    )\n    eval_dataset = eval_dataset.filter(\n        lambda x: len(x[\"input_ids_chosen\"]) <= args.reward_config.max_length\n        and len(x[\"input_ids_rejected\"]) <= args.reward_config.max_length\n    )\n\n\n# Step 4: Define the LoraConfig\nif args.use_peft:\n    peft_config = args.peft_config\nelse:\n    peft_config = None\n\n# Step 5: Define the Trainer\ntrainer = RewardTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=args.reward_config,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n)\n\ntrainer.train()\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/scripts/dpo.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# coding=utf-8\n\n# Note: you need to install transformers from main to run this script. See https://huggingface.co/docs/transformers/installation#install-from-source\n# TODO: bump transformers version in requirements at next release.\n\n# 0. imports\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Optional\n\nimport torch\nfrom datasets import Dataset, load_dataset\nfrom peft import LoraConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, TrainingArguments\n\nfrom trl import DPOTrainer\n\n\n# Define and parse arguments.\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    The arguments for the DPO training script.\n    \"\"\"\n\n    # data parameters\n    beta: Optional[float] = field(default=0.1, metadata={\"help\": \"the beta parameter for DPO loss\"})\n\n    # training parameters\n    model_name_or_path: Optional[str] = field(default=\"gpt2\", metadata={\"help\": \"the model name\"})\n    learning_rate: Optional[float] = field(default=1e-3, metadata={\"help\": \"optimizer learning rate\"})\n    per_device_train_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"batch size per device\"})\n    gradient_accumulation_steps: Optional[int] = field(\n        default=1, metadata={\"help\": \"the number of gradient accumulation steps\"}\n    )\n    max_length: Optional[int] = field(default=512, metadata={\"help\": \"max length of each sample\"})\n    max_prompt_length: Optional[int] = field(default=128, metadata={\"help\": \"max length of each sample's prompt\"})\n    max_target_length: Optional[int] = field(\n        default=128, metadata={\"help\": \"Only used for encoder decoder model. Max target of each sample's prompt\"}\n    )\n    label_pad_token_id: Optional[int] = field(default=-100, metadata={\"help\": \"label for non response tokens\"})\n    max_steps: Optional[int] = field(default=1000, metadata={\"help\": \"max number of training steps\"})\n    # lora parameters\n    use_peft: Optional[bool] = field(default=True, metadata={\"help\": \"Wether to use PEFT or not to train adapters\"})\n    peft_lora_r: Optional[int] = field(default=64, metadata={\"help\": \"the r parameter of the LoRA adapters\"})\n    peft_lora_alpha: Optional[int] = field(default=16, metadata={\"help\": \"the alpha parameter of the LoRA adapters\"})\n    # instrumentation\n    sanity_check: Optional[bool] = field(default=True, metadata={\"help\": \"only train on 1000 samples\"})\n    report_to: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": 'The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,'\n            '`\"comet_ml\"`, `\"mlflow\"`, `\"neptune\"`, `\"tensorboard\"`,`\"clearml\"` and `\"wandb\"`. '\n            'Use `\"all\"` to report to all integrations installed, `\"none\"` for no integrations.'\n        },\n    )\n    # debug argument for distributed training\n    ignore_bias_buffers: Optional[bool] = field(\n        default=False,\n        metadata={\n            \"help\": \"fix for DDP issues with LM bias/mask buffers - invalid scalar type,`inplace operation. See\"\n            \"https://github.com/huggingface/transformers/issues/22482#issuecomment-1595790992\"\n        },\n    )\n    gradient_checkpointing: Optional[bool] = field(\n        default=False, metadata={\"help\": \"Whether to use gradient checkpointing or no\"}\n    )\n    gradient_checkpointing_kwargs: Optional[dict] = field(\n        default=None,\n        metadata={\n            \"help\": \"key word arguments to be passed along `torch.utils.checkpoint.checkpoint` method - e.g. `use_reentrant=False`\"\n        },\n    )\n\n\ndef extract_anthropic_prompt(prompt_and_response):\n    \"\"\"Extract the anthropic prompt from a prompt and response pair.\"\"\"\n    search_term = \"\\n\\nAssistant:\"\n    search_term_idx = prompt_and_response.rfind(search_term)\n    assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n    return prompt_and_response[: search_term_idx + len(search_term)]\n\n\ndef get_hh(split: str, sanity_check: bool = False, silent: bool = False, cache_dir: str = None) -> Dataset:\n    \"\"\"Load the Anthropic Helpful-Harmless dataset from Hugging Face and convert it to the necessary format.\n\n    The dataset is converted to a dictionary with the following structure:\n    {\n        'prompt': List[str],\n        'chosen': List[str],\n        'rejected': List[str],\n    }\n\n    Prompts should be structured as follows:\n      \\n\\nHuman: <prompt>\\n\\nAssistant:\n    Multiple turns are allowed, but the prompt should always start with \\n\\nHuman: and end with \\n\\nAssistant:.\n    \"\"\"\n    dataset = load_dataset(\"Anthropic/hh-rlhf\", split=split, cache_dir=cache_dir)\n    if sanity_check:\n        dataset = dataset.select(range(min(len(dataset), 1000)))\n\n    def split_prompt_and_responses(sample) -> Dict[str, str]:\n        prompt = extract_anthropic_prompt(sample[\"chosen\"])\n        return {\n            \"prompt\": prompt,\n            \"chosen\": sample[\"chosen\"][len(prompt) :],\n            \"rejected\": sample[\"rejected\"][len(prompt) :],\n        }\n\n    return dataset.map(split_prompt_and_responses)\n\n\nif __name__ == \"__main__\":\n    parser = HfArgumentParser(ScriptArguments)\n    script_args = parser.parse_args_into_dataclasses()[0]\n\n    # 1. load a pretrained model\n    model = AutoModelForCausalLM.from_pretrained(script_args.model_name_or_path)\n\n    if script_args.ignore_bias_buffers:\n        # torch distributed hack\n        model._ddp_params_and_buffers_to_ignore = [\n            name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n        ]\n\n    model_ref = AutoModelForCausalLM.from_pretrained(script_args.model_name_or_path)\n\n    tokenizer = AutoTokenizer.from_pretrained(script_args.model_name_or_path)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    # 2. Load the Anthropic Helpful-Harmless dataset\n    train_dataset = get_hh(\"train\", sanity_check=script_args.sanity_check)\n\n    # 3. Load evaluation dataset\n    eval_dataset = get_hh(\"test\", sanity_check=script_args.sanity_check)\n\n    # 4. initialize training arguments:\n    training_args = TrainingArguments(\n        per_device_train_batch_size=script_args.per_device_train_batch_size,\n        max_steps=script_args.max_steps,\n        remove_unused_columns=False,\n        gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n        learning_rate=script_args.learning_rate,\n        evaluation_strategy=\"steps\",\n        logging_first_step=True,\n        logging_steps=10,  # match results in blog post\n        eval_steps=500,\n        output_dir=\"./test\",\n        optim=\"rmsprop\",\n        warmup_steps=150,\n        report_to=script_args.report_to,\n        bf16=True,\n        gradient_checkpointing=script_args.gradient_checkpointing,\n        # TODO: uncomment that on the next transformers release\n        # gradient_checkpointing_kwargs=script_args.gradient_checkpointing_kwargs,\n    )\n\n    if script_args.use_peft:\n        peft_config = LoraConfig(\n            r=script_args.peft_lora_r,\n            lora_alpha=script_args.peft_lora_alpha,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n    else:\n        peft_config = None\n\n    # 5. initialize the DPO trainer\n    dpo_trainer = DPOTrainer(\n        model,\n        model_ref,\n        args=training_args,\n        beta=script_args.beta,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        max_length=script_args.max_length,\n        max_target_length=script_args.max_target_length,\n        max_prompt_length=script_args.max_prompt_length,\n        generate_during_eval=True,\n        peft_config=peft_config,\n    )\n\n    # 6. train\n    dpo_trainer.train()\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/scripts/ppo.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# coding=utf-8\n\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport torch\nimport tyro\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nfrom peft import LoraConfig\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, pipeline\n\nfrom trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed\nfrom trl.core import LengthSampler\nfrom trl.import_utils import is_npu_available, is_xpu_available\n\n\ntqdm.pandas()\n\n\n@dataclass\nclass ScriptArguments:\n    ppo_config: PPOConfig = field(\n        default_factory=lambda: PPOConfig(\n            model_name=\"lvwerra/gpt2-imdb\",\n            query_dataset=\"imdb\",\n            reward_model=\"sentiment-analysis:lvwerra/distilbert-imdb\",\n            learning_rate=1.41e-5,\n            log_with=None,\n            mini_batch_size=128,\n            batch_size=128,\n            gradient_accumulation_steps=1,\n            early_stopping=False,\n            target_kl=6.0,\n            kl_penalty=\"kl\",\n            seed=0,\n            use_score_scaling=False,\n            use_score_norm=False,\n            score_clip=None,\n        )\n    )\n    use_seq2seq: bool = False\n    \"\"\"whether to use seq2seq models\"\"\"\n    use_peft: bool = False\n    \"\"\"whether to use peft\"\"\"\n    peft_config: Optional[LoraConfig] = field(\n        default_factory=lambda: LoraConfig(\n            r=16,\n            lora_alpha=16,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        ),\n    )\n    trust_remote_code: bool = field(default=False, metadata={\"help\": \"Enable `trust_remote_code`\"})\n\n\nargs = tyro.cli(ScriptArguments)\n\n\n# We then define the arguments to pass to the sentiment analysis pipeline.\n# We set `return_all_scores` to True to get the sentiment score for each token.\nsent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}\n\ntrl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead\n\n\n# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n# from the `datasets` library. One should customize this function to train the model on\n# its own dataset.\ndef build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):\n    \"\"\"\n    Build dataset for training. This builds the dataset from `load_dataset`, one should\n    customize this function to train the model on its own dataset.\n\n    Args:\n        query_dataset (`str`):\n            The name of the dataset to be loaded.\n\n    Returns:\n        dataloader (`torch.utils.data.DataLoader`):\n            The dataloader for the dataset.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n    # load imdb with datasets\n    ds = load_dataset(query_dataset, split=\"train\")\n    ds = ds.rename_columns({\"text\": \"review\"})\n    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n\n    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n\n    def tokenize(sample):\n        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n        return sample\n\n    ds = ds.map(tokenize, batched=False)\n    ds.set_format(type=\"torch\")\n    return ds\n\n\n# We retrieve the dataloader by calling the `build_dataset` function.\ndataset = build_dataset(args.ppo_config, args.ppo_config.query_dataset)\n\n\ndef collator(data):\n    return dict((key, [d[key] for d in data]) for key in data[0])\n\n\n# set seed before initializing value head for deterministic eval\nset_seed(args.ppo_config.seed)\n\n# Now let's build the model, the reference model, and the tokenizer.\nif not args.use_peft:\n    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=args.trust_remote_code)\n    device_map = None\n    peft_config = None\nelse:\n    peft_config = args.peft_config\n    ref_model = None\n    # Copy the model to each device\n    device_map = {\"\": Accelerator().local_process_index}\n\nmodel = trl_model_class.from_pretrained(\n    args.ppo_config.model_name,\n    trust_remote_code=args.trust_remote_code,\n    device_map=device_map,\n    peft_config=peft_config,\n)\n\n\ntokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)\n\n# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\nppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)\n\n# We then build the sentiment analysis pipeline, passing the model name and the\n# sentiment analysis pipeline arguments. Let's also make sure to set the device\n# to the same device as the PPOTrainer.\ndevice = ppo_trainer.accelerator.device\nif ppo_trainer.accelerator.num_processes == 1:\n    if is_xpu_available():\n        device = \"xpu:0\"\n    elif is_npu_available():\n        device = \"npu:0\"\n    else:\n        device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\nds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin\ntask, model_name = args.ppo_config.reward_model.split(\":\")\nif ds_plugin is not None and ds_plugin.is_zero3_init_enabled():\n    with ds_plugin.zero3_init_context_manager(enable=False):\n        sentiment_pipe = pipeline(task, model=model_name, device=device)\nelse:\n    sentiment_pipe = pipeline(task, model=model_name, device=device)\n\n# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.\nif sentiment_pipe.tokenizer.pad_token_id is None:\n    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id\n\nif sentiment_pipe.model.config.pad_token_id is None:\n    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id\n\n# We then define the arguments to pass to the `generate` function. These arguments\n# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n# the `generate` function of the trained model.\ngeneration_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n    \"max_new_tokens\": 32,\n}\n\nfor epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n    query_tensors = batch[\"input_ids\"]\n\n    # Get response from gpt2\n    response_tensors, ref_response_tensors = ppo_trainer.generate(\n        query_tensors, return_prompt=False, generate_ref_response=True, **generation_kwargs\n    )\n    batch[\"response\"] = tokenizer.batch_decode(response_tensors)\n    batch[\"ref_response\"] = tokenizer.batch_decode(ref_response_tensors)\n\n    # Compute sentiment score\n    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n    ref_texts = [q + r for q, r in zip(batch[\"query\"], batch[\"ref_response\"])]\n    ref_pipe_outputs = sentiment_pipe(ref_texts, **sent_kwargs)\n    ref_rewards = [torch.tensor(output[1][\"score\"]) for output in ref_pipe_outputs]\n    batch[\"ref_rewards\"] = ref_rewards\n\n    # Run PPO step\n    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n    ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=[\"query\", \"response\", \"ref_response\", \"ref_rewards\"])\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/research_projects/toxicity/scripts/evaluate-toxicity.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import argparse\nimport csv\n\nimport evaluate\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom trl.import_utils import is_npu_available, is_xpu_available\n\n\ntoxicity = evaluate.load(\"ybelkada/toxicity\", \"DaNLP/da-electra-hatespeech-detection\", module_type=\"measurement\")\nds = load_dataset(\"OxAISH-AL-LLM/wiki_toxic\", split=\"test\")\n\nparser = argparse.ArgumentParser(description=\"Evaluate de-toxified models\")\nparser.add_argument(\"--model_type\", default=\"all\", type=str, help=\"Relative path to the source model folder\")\nparser.add_argument(\"--output_file\", default=\"toxicity.csv\", type=str, help=\"Relative path to the source model folder\")\nparser.add_argument(\"--batch_size\", default=64, type=int, help=\"Batch size\")\nparser.add_argument(\"--num_samples\", default=400, type=int, help=\"Number of samples\")\nparser.add_argument(\"--context_length\", default=2000, type=int, help=\"Number of samples\")\nparser.add_argument(\"--max_new_tokens\", default=30, type=int, help=\"Max new tokens for generation\")\nargs = parser.parse_args()\n\n\nif args.model_type == \"all\":\n    MODELS_TO_TEST = [\n        \"ybelkada/gpt-neo-125m-detox\",\n        \"EleutherAI/gpt-neo-125M\",\n        \"EleutherAI/gpt-neo-2.7B\",\n        \"ybelkada/gpt-neo-2.7B-detox\",\n        \"ybelkada/gpt-j-6b-sharded-bf16\",\n        \"ybelkada/gpt-j-6b-detoxs\",\n    ]\nelif args.model_type == \"gpt-neo\":\n    MODELS_TO_TEST = [\n        \"ybelkada/gpt-neo-125m-detox\",\n        \"EleutherAI/gpt-neo-125M\",\n        \"EleutherAI/gpt-neo-2.7B\",\n        \"ybelkada/gpt-neo-2.7B-detox\",\n    ]\nelif args.model_type == \"gpt-j\":\n    MODELS_TO_TEST = [\n        \"ybelkada/gpt-j-6b-sharded-bf16\",\n        \"ybelkada/gpt-j-6b-detox\",\n    ]\nelse:\n    MODELS_TO_TEST = [args.model_type]\nNUM_SAMPLES = args.num_samples\nBATCH_SIZE = args.batch_size\noutput_file = args.output_file\nmax_new_tokens = args.max_new_tokens\ncontext_length = args.context_length\nif is_xpu_available():\n    device = torch.xpu.current_device()\nelif is_npu_available():\n    device = torch.npu.current_device()\nelse:\n    device = torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\"\n\n# consider only toxic prompts\nds = ds.filter(lambda x: x[\"label\"] == 1)\n\ntoxicities = {}\n\n# open a csv file\nfile = open(f\"{output_file}\", \"w\", newline=\"\")\nwriter = csv.writer(file)\n# add first rows\nwriter.writerow([\"model_id\", \"mean_toxicity\", \"std_toxicity\"])\n\n\nfor model_id in tqdm(MODELS_TO_TEST):\n    model = AutoModelForCausalLM.from_pretrained(model_id, device_map={\"\": device}, torch_dtype=torch.bfloat16)\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"left\"\n    input_texts = []\n\n    for i, example in enumerate(ds):\n        # set seed\n        torch.manual_seed(42)\n\n        input_text = example[\"comment_text\"]\n        input_texts.append(input_text[:2000])\n\n        if i > NUM_SAMPLES:\n            break\n\n        if (i + 1) % BATCH_SIZE == 0:\n            inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(device)\n            inputs.input_ids = inputs.input_ids[:context_length]\n            inputs.attention_mask = inputs.attention_mask[:context_length]\n            outputs = model.generate(**inputs, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True)\n            generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            generated_texts = [\n                generated_text.replace(input_texts[i], \"\") for i, generated_text in enumerate(generated_texts)\n            ]\n            toxicity_score = toxicity.compute(predictions=generated_texts)\n            input_texts = []\n\n            if model_id not in toxicities:\n                toxicities[model_id] = []\n            toxicities[model_id].extend(toxicity_score[\"toxicity\"])\n\n    # last batch\n    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(device)\n    outputs = model.generate(**inputs, do_sample=True, max_new_tokens=30)\n    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    generated_texts = [generated_text.replace(input_texts[i], \"\") for i, generated_text in enumerate(generated_texts)]\n    toxicity_score = toxicity.compute(predictions=generated_texts)\n    toxicities[model_id].extend(toxicity_score[\"toxicity\"])\n\n    # compute mean & std using np\n    mean = np.mean(toxicities[model_id])\n    std = np.std(toxicities[model_id])\n\n    # save to file\n    writer.writerow([model_id, mean, std])\n\n    # print\n    print(f\"Model: {model_id} - Mean: {mean} - Std: {std}\")\n\n    model = None\n    if is_xpu_available():\n        torch.xpu.empty_cache()\n    elif is_npu_available():\n        torch.npu.empty_cache()\n    else:\n        torch.cuda.empty_cache()\n\n# close file\nfile.close()\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/research_projects/toxicity/scripts/gpt-j-6b-toxicity.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# coding=utf-8\n\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport torch\nfrom datasets import load_dataset\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    HfArgumentParser,\n    RobertaForSequenceClassification,\n    RobertaTokenizer,\n)\n\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, create_reference_model, set_seed\nfrom trl.core import LengthSampler\n\n\ntqdm.pandas()\n\n########################################################################\n# This is a fully working simple example to use trl with accelerate.\n#\n# This example fine-tunes a GPTJ model to generate less toxic contents\n# by using allenai/real-toxicity-prompts dataset. We use PPO\n#  (proximal policy optimization) to optimize the model.\n# in any of the following settings (with the same script):\n#   - single CPU or single GPU\n#   - multi GPUS (using PyTorch distributed mode)\n#   - multi GPUS (using DeepSpeed ZeRO-Offload stages 1 & 2)\n#   - fp16 (mixed-precision) or fp32 (normal precision)\n#\n# To run it in each of these various modes, first initialize the accelerate\n# configuration with `accelerate config`\n#\n########################################################################\n\n\n# We first define the configuration of the experiment, defining the model, the dataset,\n# the training parameters, and the PPO parameters.\n# Check the default arguments in the `PPOConfig` class for more details.\n# If you want to log with tensorboard, add the kwarg\n# `project_kwargs={\"logging_dir\": PATH_TO_LOGS}` to the PPOConfig.\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    The name of the Casual LM model we wish to fine-tune with PPO\n    \"\"\"\n\n    # NOTE: gpt2 models use Conv1D instead of Linear layers which are not yet supported in 8 bit mode\n    # models like gpt-neo* models are more suitable.\n    model_name: Optional[str] = field(default=\"ybelkada/gpt-j-6b-sharded-bf16\", metadata={\"help\": \"the model name\"})\n    log_with: Optional[str] = field(default=None, metadata={\"help\": \"use 'wandb' to log with wandb\"})\n    learning_rate: Optional[float] = field(default=(1.47e-5) * 2, metadata={\"help\": \"the learning rate\"})\n    mini_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"the PPO minibatch size\"})\n    batch_size: Optional[int] = field(default=16, metadata={\"help\": \"the batch size\"})\n    gradient_accumulation_steps: Optional[int] = field(\n        default=1, metadata={\"help\": \"the number of gradient accumulation steps\"}\n    )\n    model_save_path: Optional[str] = field(\n        default=\"./gpt-j-6B-detoxified-long-context-26-shl-1e4-final\",\n        metadata={\"help\": \"the path to save the model\"},\n    )\n\n\nparser = HfArgumentParser(ScriptArguments)\nscript_args = parser.parse_args_into_dataclasses()[0]\n\nconfig = PPOConfig(\n    model_name=script_args.model_name,\n    learning_rate=script_args.learning_rate,\n    log_with=script_args.log_with,\n    ppo_epochs=100,\n    mini_batch_size=script_args.mini_batch_size,\n    batch_size=script_args.batch_size,\n    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n)\n\n\n# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n# from the `datasets` library. One should customize this function to train the model on\n# its own dataset.\ndef build_dataset(\n    config, dataset_name=\"allenai/real-toxicity-prompts\", input_min_text_length=5, input_max_text_length=10\n):\n    \"\"\"\n    Build dataset for training. This builds the dataset from `load_dataset`, one should\n    customize this function to train the model on its own dataset.\n\n    Args:\n        dataset_name (`str`):\n            The name of the dataset to be loaded.\n\n    Returns:\n        dataloader (`torch.utils.data.DataLoader`):\n            The dataloader for the dataset.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    ds = load_dataset(dataset_name, split=\"train\")\n\n    def filter_fn(sample):\n        toxicity = sample[\"prompt\"][\"toxicity\"]\n        return toxicity is not None and toxicity > 0.3\n\n    ds = ds.filter(filter_fn, batched=False)\n\n    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n\n    def tokenize(sample):\n        prompt = sample[\"prompt\"][\"text\"]\n        continuation = sample[\"continuation\"][\"text\"]\n\n        sample[\"input_ids\"] = tokenizer.encode(prompt + continuation)[: input_size()]\n        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n        return sample\n\n    ds = ds.map(tokenize, batched=False)\n    ds.set_format(type=\"torch\")\n\n    ds = ds.train_test_split(test_size=0.2, shuffle=False)[\"train\"]\n\n    return ds\n\n\n# We retrieve the dataloader by calling the `build_dataset` function.\nmin_input_length = 30\nmax_input_length = 40\ndataset = build_dataset(config, input_min_text_length=min_input_length, input_max_text_length=max_input_length)\n\n\ndef collator(data):\n    return dict((key, [d[key] for d in data]) for key in data[0])\n\n\n# set seed before initializing value head for deterministic eval\nset_seed(config.seed)\n\n# Now let's build the model, the reference model, and the tokenizer. We first load the model\n# in bfloat16 to save memory using `transformers`.\nmodel = AutoModelForCausalLM.from_pretrained(config.model_name, torch_dtype=torch.bfloat16)\n# And then we pass the loaded model to `AutoModelForCausalLMWithValueHead`.\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(model)\n\n# We create a reference model by sharing 20 layers\nref_model = create_reference_model(model, num_shared_layers=20)\n\n# We make sure to use `Adam` optimizer on the model parameters that require gradients.\noptimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)\n\n# GPT-2 / GPT-J tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.\n# only for this model.\ntokenizer = AutoTokenizer.from_pretrained(config.model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\nppo_trainer = PPOTrainer(\n    config,\n    model,\n    ref_model=ref_model,\n    tokenizer=tokenizer,\n    dataset=dataset,\n    data_collator=collator,\n    optimizer=optimizer,\n)\n\n# We then build the reward pipeline, we will use the toxicity model to compute the reward.\n# We first load the toxicity model and tokenizer.\ntoxicity_model_id = \"facebook/roberta-hate-speech-dynabench-r4-target\"\ntoxicity_tokenizer = RobertaTokenizer.from_pretrained(toxicity_model_id)\n# We load the toxicity model in fp16 to save memory.\ntoxicity_model = RobertaForSequenceClassification.from_pretrained(toxicity_model_id, torch_dtype=torch.float16).to(\n    ppo_trainer.accelerator.device\n)\n\n\n# We then define the arguments to pass to the `generate` function. These arguments\n# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n# the `generate` function of the trained model.\ngeneration_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n}\noutput_min_length = 20\noutput_max_length = 30\noutput_length_sampler = LengthSampler(output_min_length, output_max_length)\n\nmodel_save_path = script_args.model_save_path\n\nfor epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n    query_tensors = batch[\"input_ids\"]\n\n    # Get response from the policy model\n    response_tensors = []\n    for query in query_tensors:\n        gen_len = output_length_sampler()\n        generation_kwargs[\"max_new_tokens\"] = gen_len\n        response = ppo_trainer.generate(query, **generation_kwargs)\n        response_tensors.append(response.squeeze()[-gen_len:])\n    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n\n    # Compute sentiment score # noqa\n    texts = batch[\"response\"]\n    toxicity_inputs = toxicity_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(\n        ppo_trainer.accelerator.device\n    )\n    logits = toxicity_model(**toxicity_inputs).logits.float()\n    toxicity_labels = (logits[:, 0]).tolist()\n\n    rewards = [torch.tensor(output) for output in toxicity_labels]\n\n    # Run PPO step\n    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n    ppo_trainer.log_stats(stats, batch, rewards)\n\n    # Save model every 100 epochs\n    if epoch % 100 == 0:\n        if ppo_trainer.accelerator.is_main_process:\n            ppo_trainer.save_pretrained(model_save_path)\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/research_projects/tools/calculator.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# coding=utf-8\n\nimport re\n\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, load_tool\n\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, TextEnvironment\n\n\ndef generate_data(n):\n    \"\"\"Generate random arithmetic tasks and answers.\"\"\"\n    tasks, answers = [], []\n    for _ in range(n):\n        a = np.random.randint(0, 50)\n        b = np.random.randint(0, 50)\n        op = np.random.choice([\"-\", \"+\", \"*\"])\n        tasks.append(f\"\\n\\nWhat is {a} {op} {b}?\")\n        if op == \"-\":\n            answers.append(a - b)\n        elif op == \"+\":\n            answers.append(a + b)\n        else:\n            answers.append(a * b)\n    return tasks, answers\n\n\ndef exact_match_reward(responses, answers=None):\n    \"\"\"Reward if generated response contains correct answer.\"\"\"\n    rewards = []\n    pattern = r\"Result\\s*=\\s*(-?\\d+(?:\\.\\d+)?)\\s*<submit>\"  # generated by chatGPT\n    for response, answer in zip(responses, answers):\n        reward = 0.0\n        predicted_number = None\n        match_pattern = re.findall(pattern, response)\n        if match_pattern:\n            predicted_number = float(match_pattern[0])\n        if predicted_number is not None:\n            if np.abs(predicted_number - answer) < 0.01:\n                reward += 1.0\n        rewards.append(torch.tensor(reward))\n    return rewards\n\n\n# set up models\nmodel_id = \"gpt2\"\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(model_id)\nmodel_ref = AutoModelForCausalLMWithValueHead.from_pretrained(model_id)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\n\n# system prompt\nprompt = \"\"\"\\\nWhat is 13-3?\n\n<request><SimpleCalculatorTool>13-3<call>10.0<response>\n\nResult=10<submit>\n\nWhat is 4*3?\n\n<request><SimpleCalculatorTool>4*3<call>12.0<response>\n\nResult=12<submit>\"\"\"\n\ngeneration_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n    \"eos_token_id\": -1,\n    \"max_new_tokens\": 32,\n}\n\n# trainer\nppo_config = PPOConfig(\n    batch_size=256,\n    learning_rate=1.41e-5,\n    mini_batch_size=64,\n    log_with=\"wandb\",\n)\nppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)\n\n# text env\ntext_env = TextEnvironment(\n    model,\n    tokenizer,\n    {\"SimpleCalculatorTool\": load_tool(\"ybelkada/simple-calculator\")},\n    exact_match_reward,\n    prompt,\n    generation_kwargs=generation_kwargs,\n)\n\n# main training loop\nfor step in range(100):\n    tasks, answers = generate_data(ppo_config.batch_size)\n    queries, responses, masks, rewards, histories = text_env.run(tasks, answers=answers)\n    train_stats = ppo_trainer.step(queries, responses, rewards, masks)\n\n    response_texts = [tokenizer.decode(response) for response in responses]\n    query_texts = [tokenizer.decode(query) for query in queries]\n    texts = {\"query\": [qt.split(\"<submit>\")[-1].strip() for qt in query_texts], \"response\": response_texts}\n    ppo_trainer.log_stats(train_stats, texts, rewards, columns_to_log=[\"query\", \"response\", \"answer\"])\nppo_trainer.save_pretrained(model_id + \"-calculator\")\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/research_projects/tools/triviaqa.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# coding=utf-8\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig\nfrom transformers import AutoTokenizer, HfArgumentParser, load_tool\n\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, TextEnvironment\n\n\nos.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\n@dataclass\nclass ScriptArguments:\n    model_name: Optional[str] = field(default=\"bigcode/starcoderbase\", metadata={\"help\": \"the model name\"})\n    log_with: Optional[str] = field(default=None, metadata={\"help\": \"use 'wandb' to log with wandb\"})\n    learning_rate: Optional[float] = field(default=1e-5, metadata={\"help\": \"the learning rate\"})\n    mini_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"the PPO minibatch size\"})\n    batch_size: Optional[int] = field(default=32, metadata={\"help\": \"the batch size\"})\n    gradient_accumulation_steps: Optional[int] = field(\n        default=16, metadata={\"help\": \"the number of gradient accumulation steps\"}\n    )\n    max_new_tokens: Optional[int] = field(default=256, metadata={\"help\": \"max number of generated tokens per turn\"})\n    ppo_epochs: Optional[int] = field(default=1, metadata={\"help\": \"max number of ppo epochs\"})\n    iterations: Optional[int] = field(default=1000, metadata={\"help\": \"the number of iterations\"})\n    seed: Optional[int] = field(default=0, metadata={\"help\": \"the random seed\"})\n\n\nparser = HfArgumentParser(ScriptArguments)\nargs = parser.parse_args_into_dataclasses()[0]\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"c_proj\", \"c_attn\", \"q_attn\"],\n)\n\n# set up models\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\n    args.model_name,\n    use_auth_token=True,\n    trust_remote_code=True,\n    load_in_4bit=True,\n    peft_config=lora_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(args.model_name, use_auth_token=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n# system prompt\nprompt = \"\"\"\\\nAnswer the following question:\n\nQ: In which branch of the arts is Patricia Neary famous?\nA: Ballets\nA2: <request><Wiki>Patricia Neary<call>Patricia Neary (born October 27, 1942) is an American ballerina, choreographer and ballet director, who has been particularly active in Switzerland. She has also been a highly successful ambassador for the Balanchine Trust, bringing George Balanchine's ballets to 60 cities around the globe.<response>\nResult=Ballets<submit>\n\nQ: Who won Super Bowl XX?\nA: Chicago Bears\nA2: <request><Wiki>Super Bowl XX<call>Super Bowl XX was an American football game between the National Football Conference (NFC) champion Chicago Bears and the American Football Conference (AFC) champion New England Patriots to decide the National Football League (NFL) champion for the 1985 season. The Bears defeated the Patriots by the score of 46\u201310, capturing their first NFL championship (and Chicago's first overall sports victory) since 1963, three years prior to the birth of the Super Bowl. Super Bowl XX was played on January 26, 1986 at the Louisiana Superdome in New Orleans.<response>\nResult=Chicago Bears<submit>\n\nQ: \"\"\"\n\ngeneration_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n    \"eos_token_id\": -1,\n    \"max_new_tokens\": args.max_new_tokens,\n}\n\n# trainer\nconfig = PPOConfig(\n    batch_size=args.batch_size,\n    model_name=args.model_name,\n    learning_rate=args.learning_rate,\n    log_with=args.log_with,\n    mini_batch_size=args.mini_batch_size,\n    ppo_epochs=args.ppo_epochs,\n    gradient_accumulation_steps=args.gradient_accumulation_steps,\n    seed=args.seed,\n    optimize_cuda_cache=True,\n)\nppo_trainer = PPOTrainer(config=config, model=model, tokenizer=tokenizer)\ndataset = load_dataset(\"trivia_qa\", \"rc\", split=\"train\")\nlocal_seed = args.seed + ppo_trainer.accelerator.process_index * 100003  # Prime\ndataset = dataset.shuffle(local_seed)\n\n\ndef data_generator():\n    for i in range(len(dataset)):\n        yield dataset[i][\"question\"], [item for item in dataset[i][\"answer\"][\"normalized_aliases\"]]\n\n\ngen = data_generator()\ngen = iter(gen)\n\n\ndef generate_data(n):\n    tasks, answers = [], []\n    for i in range(n):\n        q, a = next(gen)\n        tasks.append(q)\n        answers.append(a)\n    return tasks, answers\n\n\ndef exact_match_reward(responses, answers=None):\n    \"\"\"Reward if generated response contains correct answer.\"\"\"\n    rewards = []\n    for response, answer in zip(responses, answers):\n        reward = 0.0\n        for a in answer:\n            if a.lower() in response.lower():\n                reward += 1.0\n                break\n        rewards.append(torch.tensor(reward))\n    return rewards\n\n\n# text env\ntool = load_tool(\"vwxyzjn/pyserini-wikipedia-kilt-doc\")\n# limit the amount if tokens\ntool_fn = lambda x: tool(x).split(\"\\n\")[1][:600]  # noqa\ntext_env = TextEnvironment(\n    model,\n    tokenizer,\n    {\"Wiki\": tool_fn},\n    exact_match_reward,\n    prompt,\n    generation_kwargs=generation_kwargs,\n    max_tool_reponse=400,\n)\n\n\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\nprint_trainable_parameters(model)\n# main training loop\nfor i in range(args.iterations):\n    tasks, answers = generate_data(config.batch_size)\n    queries, responses, masks, rewards, histories = text_env.run(tasks, answers=answers)\n    train_stats = ppo_trainer.step(queries, responses, rewards, masks)\n    response_texts = [tokenizer.decode(response) for response in responses]\n    query_texts = [tokenizer.decode(query) for query in queries]\n    texts = {\n        \"query\": [qt.split(\"<submit>\")[-1].strip() for qt in query_texts],\n        \"response\": response_texts,\n        \"answer\": [\", \".join(item) for item in answers],\n    }\n    all_rewards = ppo_trainer.accelerator.gather(torch.tensor(rewards, device=ppo_trainer.accelerator.device))\n    ppo_trainer.log_stats(\n        train_stats, texts, [item for item in all_rewards], columns_to_log=[\"query\", \"response\", \"answer\"]\n    )\n    if i % 100 == 0:\n        ppo_trainer.save_pretrained(f\"models/{args.model_name}_{args.seed}_{i}_triviaqa\")\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/research_projects/tools/python_interpreter.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# coding=utf-8\n\nimport os\nimport re\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig\nfrom transformers import AutoTokenizer, HfArgumentParser, load_tool\n\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, TextEnvironment\n\n\nos.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\n@dataclass\nclass ScriptArguments:\n    model_name: Optional[str] = field(default=\"bigcode/starcoderbase\", metadata={\"help\": \"the model name\"})\n    learning_rate: Optional[float] = field(default=1e-5, metadata={\"help\": \"the learning rate\"})\n    mini_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"the PPO minibatch size\"})\n    batch_size: Optional[int] = field(default=32, metadata={\"help\": \"the batch size\"})\n    gradient_accumulation_steps: Optional[int] = field(\n        default=16, metadata={\"help\": \"the number of gradient accumulation steps\"}\n    )\n    max_new_tokens: Optional[int] = field(default=256, metadata={\"help\": \"max number of generated tokens per turn\"})\n    ppo_epochs: Optional[int] = field(default=1, metadata={\"help\": \"max number of ppo epochs\"})\n    n_epochs: Optional[int] = field(default=32, metadata={\"help\": \"max number of ppo epochs\"})\n\n\nparser = HfArgumentParser(ScriptArguments)\nargs = parser.parse_args_into_dataclasses()[0]\n\n\ndef exact_match_reward(responses, answers=None):\n    \"\"\"Reward if generated response contains correct answer.\"\"\"\n    rewards = []\n    pattern = r\"Result\\s*=\\s*(-?\\d+(?:\\.\\d+)?)\\s*<submit>\"  # generated by chatGPT\n    for response, answer in zip(responses, answers):\n        reward = 0.0\n        try:\n            predicted_number = None\n            match_pattern = re.findall(pattern, response)\n            if match_pattern:\n                predicted_number = float(match_pattern[0])\n            if predicted_number is not None:\n                if np.abs((predicted_number - float(answer))) < 0.1:\n                    reward += 1.0\n        except:  # noqa\n            pass\n        rewards.append(torch.tensor(reward))\n    return rewards\n\n\ndef evaluate(test_dataloader, text_env, ppo_trainer):\n    test_rewards = []\n    for test_batch in test_dataloader:\n        _, _, _, rewards, _ = text_env.run(test_batch[\"query\"], answers=test_batch[\"answer\"])\n        test_rewards.extend(rewards)\n    test_rewards = ppo_trainer.accelerator.gather_for_metrics(\n        torch.stack(test_rewards).to(ppo_trainer.accelerator.device)\n    )\n    return test_rewards.mean()\n\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"c_proj\", \"c_attn\", \"q_attn\"],\n)\n\n# set up models\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\n    args.model_name,\n    use_auth_token=True,\n    load_in_4bit=True,\n    peft_config=lora_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(args.model_name, use_auth_token=True)\ntokenizer.pad_token = tokenizer.eos_token\n\nds = load_dataset(\"gsm8k\", \"main\", split=\"train\")\nds = ds.rename_columns({\"question\": \"query\"})\nds = ds.map(lambda x: {\"answer\": x[\"answer\"].split(\"#### \")[1]})\nds = ds.select(range(1, len(ds)))  # skip the first sample which is used in prompt\n\nds_test = load_dataset(\"gsm8k\", \"main\", split=\"test\")\nds_test = ds_test.rename_columns({\"question\": \"query\"})\nds_test = ds_test.map(lambda x: {\"answer\": x[\"answer\"].split(\"#### \")[1]})\n\ntest_dataloader = torch.utils.data.DataLoader(ds_test, batch_size=args.batch_size)\n\n# prompt\nprompt = \"\"\"\\\nExample of using a Python API to solve math questions.\n\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n\n<request><PythonInterpreter>\ndef solution():\n    money_initial = 23\n    bagels = 5\n    bagel_cost = 3\n    money_spent = bagels * bagel_cost\n    money_left = money_initial - money_spent\n    result = money_left\n    return result\nprint(solution())\n<call>72<response>\n\nResult = 72 <submit>\n\nQ: \"\"\"\n\ngeneration_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n    \"eos_token_id\": -1,\n    \"max_new_tokens\": args.max_new_tokens,\n}\n\n# trainer\nppo_config = PPOConfig(\n    batch_size=args.batch_size,\n    learning_rate=args.learning_rate,\n    mini_batch_size=args.mini_batch_size,\n    ppo_epochs=args.ppo_epochs,\n    gradient_accumulation_steps=args.gradient_accumulation_steps,\n    log_with=\"wandb\",\n    tracker_project_name=\"trl-gsm8k\",\n    remove_unused_columns=False,\n    optimize_cuda_cache=True,\n)\n\nppo_trainer = PPOTrainer(config=ppo_config, model=model, tokenizer=tokenizer, dataset=ds)\ntest_dataloader = ppo_trainer.accelerator.prepare(test_dataloader)\n\n# text env\ntext_env = TextEnvironment(\n    model,\n    tokenizer,\n    [load_tool(\"lvwerra/python-interpreter\")],\n    exact_match_reward,\n    prompt,\n    max_turns=2,\n    generation_kwargs=generation_kwargs,\n)\n\n# main training loop\nfor epoch in range(args.n_epochs):\n    for step, batch in enumerate(ppo_trainer.dataloader):\n        if (step == 0) and (epoch % 4 == 0):  # evaluate every 4 epochs\n            reward_mean_test = evaluate(test_dataloader, text_env, ppo_trainer)\n        else:\n            reward_mean_test = None\n\n        queries, responses, masks, rewards, histories = text_env.run(batch[\"query\"], answers=batch[\"answer\"])\n        train_stats = ppo_trainer.step(queries, responses, rewards, masks)\n\n        # logging\n        if reward_mean_test is not None:\n            train_stats[\"env/reward_mean_test\"] = reward_mean_test\n        texts = {\n            \"query\": batch[\"query\"],\n            \"response\": [tokenizer.decode(response) for response in responses],\n            \"answer\": batch[\"answer\"],\n        }\n        ppo_trainer.log_stats(train_stats, texts, rewards, columns_to_log=[\"query\", \"response\", \"answer\"])\n\nreward_mean_test = evaluate(test_dataloader, text_env, ppo_trainer)\nppo_trainer.save_pretrained(f\"model/{args.model_name}-gsm8k\")\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/research_projects/stack_llama_2/scripts/sft_llama2.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# Fine-Tune Llama2-7b on SE paired dataset\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport torch\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nfrom peft import AutoPeftModelForCausalLM, LoraConfig\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n\nfrom trl import SFTTrainer\nfrom trl.import_utils import is_npu_available, is_xpu_available\nfrom trl.trainer import ConstantLengthDataset\n\n\n@dataclass\nclass ScriptArguments:\n    model_name: Optional[str] = field(default=\"meta-llama/Llama-2-7b-hf\", metadata={\"help\": \"the model name\"})\n    dataset_name: Optional[str] = field(default=\"lvwerra/stack-exchange-paired\", metadata={\"help\": \"the dataset name\"})\n    subset: Optional[str] = field(default=\"data/finetune\", metadata={\"help\": \"the subset to use\"})\n    split: Optional[str] = field(default=\"train\", metadata={\"help\": \"the split to use\"})\n    size_valid_set: Optional[int] = field(default=4000, metadata={\"help\": \"the size of the validation set\"})\n    streaming: Optional[bool] = field(default=True, metadata={\"help\": \"whether to stream the dataset\"})\n    shuffle_buffer: Optional[int] = field(default=5000, metadata={\"help\": \"the shuffle buffer size\"})\n    seq_length: Optional[int] = field(default=1024, metadata={\"help\": \"the sequence length\"})\n    num_workers: Optional[int] = field(default=4, metadata={\"help\": \"the number of workers\"})\n    packing: Optional[bool] = field(default=True, metadata={\"help\": \"whether to use packing for SFTTrainer\"})\n\n    # LoraConfig\n    lora_alpha: Optional[float] = field(default=16, metadata={\"help\": \"the lora alpha parameter\"})\n    lora_dropout: Optional[float] = field(default=0.05, metadata={\"help\": \"the lora dropout parameter\"})\n    lora_r: Optional[int] = field(default=8, metadata={\"help\": \"the lora r parameter\"})\n\n\nparser = HfArgumentParser((ScriptArguments, TrainingArguments))\nscript_args, training_args = parser.parse_args_into_dataclasses()\npeft_config = LoraConfig(\n    r=script_args.lora_r,\n    lora_alpha=script_args.lora_alpha,\n    lora_dropout=script_args.lora_dropout,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nif training_args.group_by_length and script_args.packing:\n    raise ValueError(\"Cannot use both packing and group by length\")\n\n# `gradient_checkpointing` was True by default until `1f3314`, but it's actually not used.\n# `gradient_checkpointing=True` will cause `Variable._execution_engine.run_backward`.\nif training_args.gradient_checkpointing:\n    raise ValueError(\"gradient_checkpointing not supported\")\n\n\ndef chars_token_ratio(dataset, tokenizer, nb_examples=400):\n    \"\"\"\n    Estimate the average number of characters per token in the dataset.\n    \"\"\"\n    total_characters, total_tokens = 0, 0\n    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n        text = prepare_sample_text(example)\n        total_characters += len(text)\n        if tokenizer.is_fast:\n            total_tokens += len(tokenizer(text).tokens())\n        else:\n            total_tokens += len(tokenizer.tokenize(text))\n\n    return total_characters / total_tokens\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef prepare_sample_text(example):\n    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n    text = f\"Question: {example['question']}\\n\\nAnswer: {example['response_j']}\"\n    return text\n\n\ndef create_datasets(tokenizer, args):\n    dataset = load_dataset(\n        args.dataset_name,\n        data_dir=args.subset,\n        split=args.split,\n        use_auth_token=True,\n        num_proc=args.num_workers if not args.streaming else None,\n        streaming=args.streaming,\n    )\n    if args.streaming:\n        print(\"Loading the dataset in streaming mode\")\n        valid_data = dataset.take(args.size_valid_set)\n        train_data = dataset.skip(args.size_valid_set)\n        train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=None)\n    else:\n        dataset = dataset.train_test_split(test_size=0.005, seed=None)\n        train_data = dataset[\"train\"]\n        valid_data = dataset[\"test\"]\n        print(f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\")\n\n    chars_per_token = chars_token_ratio(train_data, tokenizer)\n    print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")\n\n    train_dataset = ConstantLengthDataset(\n        tokenizer,\n        train_data,\n        formatting_func=prepare_sample_text,\n        infinite=True,\n        seq_length=args.seq_length,\n        chars_per_token=chars_per_token,\n    )\n    valid_dataset = ConstantLengthDataset(\n        tokenizer,\n        valid_data,\n        formatting_func=prepare_sample_text,\n        infinite=False,\n        seq_length=args.seq_length,\n        chars_per_token=chars_per_token,\n    )\n    return train_dataset, valid_dataset\n\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    script_args.model_name,\n    quantization_config=bnb_config,\n    device_map={\"\": Accelerator().local_process_index},\n    trust_remote_code=True,\n    use_auth_token=True,\n)\nbase_model.config.use_cache = False\n\n\ntokenizer = AutoTokenizer.from_pretrained(script_args.model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n\ntrain_dataset, eval_dataset = create_datasets(tokenizer, script_args)\n\ntrainer = SFTTrainer(\n    model=base_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    packing=script_args.packing,\n    max_seq_length=None,\n    tokenizer=tokenizer,\n    args=training_args,\n)\ntrainer.train()\ntrainer.save_model(training_args.output_dir)\n\noutput_dir = os.path.join(training_args.output_dir, \"final_checkpoint\")\ntrainer.model.save_pretrained(output_dir)\n\n# Free memory for merging weights\ndel base_model\nif is_xpu_available():\n    torch.xpu.empty_cache()\nelif is_npu_available():\n    torch.npu.empty_cache()\nelse:\n    torch.cuda.empty_cache()\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel = model.merge_and_unload()\n\noutput_merged_dir = os.path.join(training_args.output_dir, \"final_merged_checkpoint\")\nmodel.save_pretrained(output_merged_dir, safe_serialization=True)\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/research_projects/stack_llama_2/scripts/dpo_llama2.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# 0. imports\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Optional\n\nimport torch\nfrom datasets import Dataset, load_dataset\nfrom peft import LoraConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, TrainingArguments\n\nfrom trl import DPOTrainer\n\n\n# Define and parse arguments.\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    The arguments for the DPO training script.\n    \"\"\"\n\n    # data parameters\n    beta: Optional[float] = field(default=0.1, metadata={\"help\": \"the beta parameter for DPO loss\"})\n\n    # training parameters\n    model_name_or_path: Optional[str] = field(\n        default=\"../sft/results/final_checkpoint\",\n        metadata={\"help\": \"the location of the SFT model name or path\"},\n    )\n    learning_rate: Optional[float] = field(default=5e-4, metadata={\"help\": \"optimizer learning rate\"})\n    lr_scheduler_type: Optional[str] = field(default=\"cosine\", metadata={\"help\": \"the lr scheduler type\"})\n    warmup_steps: Optional[int] = field(default=100, metadata={\"help\": \"the number of warmup steps\"})\n    weight_decay: Optional[float] = field(default=0.05, metadata={\"help\": \"the weight decay\"})\n    optimizer_type: Optional[str] = field(default=\"paged_adamw_32bit\", metadata={\"help\": \"the optimizer type\"})\n\n    per_device_train_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"train batch size per device\"})\n    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"eval batch size per device\"})\n    gradient_accumulation_steps: Optional[int] = field(\n        default=4, metadata={\"help\": \"the number of gradient accumulation steps\"}\n    )\n    gradient_checkpointing: Optional[bool] = field(\n        default=True, metadata={\"help\": \"whether to use gradient checkpointing\"}\n    )\n\n    lora_alpha: Optional[float] = field(default=16, metadata={\"help\": \"the lora alpha parameter\"})\n    lora_dropout: Optional[float] = field(default=0.05, metadata={\"help\": \"the lora dropout parameter\"})\n    lora_r: Optional[int] = field(default=8, metadata={\"help\": \"the lora r parameter\"})\n\n    max_prompt_length: Optional[int] = field(default=512, metadata={\"help\": \"the maximum prompt length\"})\n    max_length: Optional[int] = field(default=1024, metadata={\"help\": \"the maximum sequence length\"})\n    max_steps: Optional[int] = field(default=1000, metadata={\"help\": \"max number of training steps\"})\n    logging_steps: Optional[int] = field(default=10, metadata={\"help\": \"the logging frequency\"})\n    save_steps: Optional[int] = field(default=100, metadata={\"help\": \"the saving frequency\"})\n    eval_steps: Optional[int] = field(default=100, metadata={\"help\": \"the evaluation frequency\"})\n\n    output_dir: Optional[str] = field(default=\"./results\", metadata={\"help\": \"the output directory\"})\n    log_freq: Optional[int] = field(default=1, metadata={\"help\": \"the logging frequency\"})\n\n    # instrumentation\n    sanity_check: Optional[bool] = field(default=False, metadata={\"help\": \"only train on 1000 samples\"})\n    report_to: Optional[str] = field(\n        default=\"wandb\",\n        metadata={\n            \"help\": 'The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,'\n            '`\"comet_ml\"`, `\"mlflow\"`, `\"neptune\"`, `\"tensorboard\"`,`\"clearml\"` and `\"wandb\"`. '\n            'Use `\"all\"` to report to all integrations installed, `\"none\"` for no integrations.'\n        },\n    )\n    # debug argument for distributed training\n    ignore_bias_buffers: Optional[bool] = field(\n        default=False,\n        metadata={\n            \"help\": \"fix for DDP issues with LM bias/mask buffers - invalid scalar type,`inplace operation. See\"\n            \"https://github.com/huggingface/transformers/issues/22482#issuecomment-1595790992\"\n        },\n    )\n\n\ndef get_stack_exchange_paired(\n    data_dir: str = \"data/rl\",\n    sanity_check: bool = False,\n    cache_dir: str = None,\n    num_proc=24,\n) -> Dataset:\n    \"\"\"Load the stack-exchange-paired dataset from Hugging Face and convert it to the necessary format.\n\n    The dataset is converted to a dictionary with the following structure:\n    {\n        'prompt': List[str],\n        'chosen': List[str],\n        'rejected': List[str],\n    }\n\n    Prompts are structured as follows:\n      \"Question: \" + <prompt> + \"\\n\\nAnswer: \"\n    \"\"\"\n    dataset = load_dataset(\n        \"lvwerra/stack-exchange-paired\",\n        split=\"train\",\n        cache_dir=cache_dir,\n        data_dir=data_dir,\n    )\n    original_columns = dataset.column_names\n\n    if sanity_check:\n        dataset = dataset.select(range(min(len(dataset), 1000)))\n\n    def return_prompt_and_responses(samples) -> Dict[str, str]:\n        return {\n            \"prompt\": [\"Question: \" + question + \"\\n\\nAnswer: \" for question in samples[\"question\"]],\n            \"chosen\": samples[\"response_j\"],\n            \"rejected\": samples[\"response_k\"],\n        }\n\n    return dataset.map(\n        return_prompt_and_responses,\n        batched=True,\n        num_proc=num_proc,\n        remove_columns=original_columns,\n    )\n\n\nif __name__ == \"__main__\":\n    parser = HfArgumentParser(ScriptArguments)\n    script_args = parser.parse_args_into_dataclasses()[0]\n\n    # 1. load a pretrained model\n    model = AutoModelForCausalLM.from_pretrained(\n        script_args.model_name_or_path,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        load_in_4bit=True,\n    )\n    model.config.use_cache = False\n\n    if script_args.ignore_bias_buffers:\n        # torch distributed hack\n        model._ddp_params_and_buffers_to_ignore = [\n            name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n        ]\n\n    model_ref = AutoModelForCausalLM.from_pretrained(\n        script_args.model_name_or_path,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        load_in_4bit=True,\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n    tokenizer.pad_token = tokenizer.eos_token\n\n    # 2. Load the Stack-exchange paired dataset\n    train_dataset = get_stack_exchange_paired(data_dir=\"data/rl\", sanity_check=script_args.sanity_check)\n    train_dataset = train_dataset.filter(\n        lambda x: len(x[\"prompt\"]) + len(x[\"chosen\"]) <= script_args.max_length\n        and len(x[\"prompt\"]) + len(x[\"rejected\"]) <= script_args.max_length\n    )\n\n    # 3. Load evaluation dataset\n    eval_dataset = get_stack_exchange_paired(data_dir=\"data/evaluation\", sanity_check=True)\n    eval_dataset = eval_dataset.filter(\n        lambda x: len(x[\"prompt\"]) + len(x[\"chosen\"]) <= script_args.max_length\n        and len(x[\"prompt\"]) + len(x[\"rejected\"]) <= script_args.max_length\n    )\n\n    # 4. initialize training arguments:\n    training_args = TrainingArguments(\n        per_device_train_batch_size=script_args.per_device_train_batch_size,\n        per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n        max_steps=script_args.max_steps,\n        logging_steps=script_args.logging_steps,\n        save_steps=script_args.save_steps,\n        gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n        gradient_checkpointing=script_args.gradient_checkpointing,\n        learning_rate=script_args.learning_rate,\n        evaluation_strategy=\"steps\",\n        eval_steps=script_args.eval_steps,\n        output_dir=script_args.output_dir,\n        report_to=script_args.report_to,\n        lr_scheduler_type=script_args.lr_scheduler_type,\n        warmup_steps=script_args.warmup_steps,\n        optim=script_args.optimizer_type,\n        bf16=True,\n        remove_unused_columns=False,\n        run_name=\"dpo_llama2\",\n    )\n\n    peft_config = LoraConfig(\n        r=script_args.lora_r,\n        lora_alpha=script_args.lora_alpha,\n        lora_dropout=script_args.lora_dropout,\n        target_modules=[\n            \"q_proj\",\n            \"v_proj\",\n            \"k_proj\",\n            \"out_proj\",\n            \"fc_in\",\n            \"fc_out\",\n            \"wte\",\n        ],\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    # 5. initialize the DPO trainer\n    dpo_trainer = DPOTrainer(\n        model,\n        model_ref,\n        args=training_args,\n        beta=script_args.beta,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        peft_config=peft_config,\n        max_prompt_length=script_args.max_prompt_length,\n        max_length=script_args.max_length,\n    )\n\n    # 6. train\n    dpo_trainer.train()\n    dpo_trainer.save_model(script_args.output_dir)\n\n    # 7. save\n    output_dir = os.path.join(script_args.output_dir, \"final_checkpoint\")\n    dpo_trainer.model.save_pretrained(output_dir)\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/research_projects/stack_llama/scripts/merge_peft_adapter.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from dataclasses import dataclass, field\nfrom typing import Optional\n\nimport torch\nfrom peft import PeftConfig, PeftModel\nfrom transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, HfArgumentParser\n\n\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    The input names representing the Adapter and Base model fine-tuned with PEFT, and the output name representing the\n    merged model.\n    \"\"\"\n\n    adapter_model_name: Optional[str] = field(default=None, metadata={\"help\": \"the adapter name\"})\n    base_model_name: Optional[str] = field(default=None, metadata={\"help\": \"the base model name\"})\n    output_name: Optional[str] = field(default=None, metadata={\"help\": \"the merged model name\"})\n\n\nparser = HfArgumentParser(ScriptArguments)\nscript_args = parser.parse_args_into_dataclasses()[0]\nassert script_args.adapter_model_name is not None, \"please provide the name of the Adapter you would like to merge\"\nassert script_args.base_model_name is not None, \"please provide the name of the Base model\"\nassert script_args.output_name is not None, \"please provide the output name of the merged model\"\n\npeft_config = PeftConfig.from_pretrained(script_args.adapter_model_name)\nif peft_config.task_type == \"SEQ_CLS\":\n    # The sequence classification task is used for the reward model in PPO\n    model = AutoModelForSequenceClassification.from_pretrained(\n        script_args.base_model_name, num_labels=1, torch_dtype=torch.bfloat16\n    )\nelse:\n    model = AutoModelForCausalLM.from_pretrained(\n        script_args.base_model_name, return_dict=True, torch_dtype=torch.bfloat16\n    )\n\ntokenizer = AutoTokenizer.from_pretrained(script_args.base_model_name)\n\n# Load the PEFT model\nmodel = PeftModel.from_pretrained(model, script_args.adapter_model_name)\nmodel.eval()\n\nmodel = model.merge_and_unload()\n\nmodel.save_pretrained(f\"{script_args.output_name}\")\ntokenizer.save_pretrained(f\"{script_args.output_name}\")\nmodel.push_to_hub(f\"{script_args.output_name}\", use_temp_dir=False)\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/research_projects/stack_llama/scripts/supervised_finetuning.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import argparse\nimport os\n\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nfrom peft import LoraConfig\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, logging, set_seed\n\nfrom trl import SFTTrainer\nfrom trl.trainer import ConstantLengthDataset\n\n\n\"\"\"\nFine-Tune Llama-7b on SE paired dataset\n\"\"\"\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_path\", type=str, default=\"\")\n    parser.add_argument(\"--dataset_name\", type=str, default=\"lvwerra/stack-exchange-paired\")\n    parser.add_argument(\"--subset\", type=str, default=\"data/finetune\")\n    parser.add_argument(\"--split\", type=str, default=\"train\")\n    parser.add_argument(\"--size_valid_set\", type=int, default=4000)\n    parser.add_argument(\"--streaming\", action=\"store_true\")\n    parser.add_argument(\"--shuffle_buffer\", type=int, default=5000)\n\n    parser.add_argument(\"--seq_length\", type=int, default=1024)\n    parser.add_argument(\"--max_steps\", type=int, default=10000)\n    parser.add_argument(\"--batch_size\", type=int, default=4)\n    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n    parser.add_argument(\"--eos_token_id\", type=int, default=49152)\n\n    parser.add_argument(\"--learning_rate\", type=float, default=1e-4)\n    parser.add_argument(\"--lr_scheduler_type\", type=str, default=\"cosine\")\n    parser.add_argument(\"--num_warmup_steps\", type=int, default=100)\n    parser.add_argument(\"--weight_decay\", type=float, default=0.05)\n\n    parser.add_argument(\"--local_rank\", type=int, default=0)\n    parser.add_argument(\"--fp16\", action=\"store_true\", default=False)\n    parser.add_argument(\"--bf16\", action=\"store_true\", default=False)\n    parser.add_argument(\"--gradient_checkpointing\", action=\"store_true\", default=False)\n    parser.add_argument(\"--seed\", type=int, default=0)\n    parser.add_argument(\"--num_workers\", type=int, default=None)\n    parser.add_argument(\"--output_dir\", type=str, default=\"./checkpoints\")\n    parser.add_argument(\"--log_freq\", default=1, type=int)\n    parser.add_argument(\"--eval_freq\", default=1000, type=int)\n    parser.add_argument(\"--save_freq\", default=1000, type=int)\n\n    return parser.parse_args()\n\n\ndef chars_token_ratio(dataset, tokenizer, nb_examples=400):\n    \"\"\"\n    Estimate the average number of characters per token in the dataset.\n    \"\"\"\n    total_characters, total_tokens = 0, 0\n    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n        text = prepare_sample_text(example)\n        total_characters += len(text)\n        if tokenizer.is_fast:\n            total_tokens += len(tokenizer(text).tokens())\n        else:\n            total_tokens += len(tokenizer.tokenize(text))\n\n    return total_characters / total_tokens\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef prepare_sample_text(example):\n    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n    text = f\"Question: {example['question']}\\n\\nAnswer: {example['response_j']}\"\n    return text\n\n\ndef create_datasets(tokenizer, args):\n    dataset = load_dataset(\n        args.dataset_name,\n        data_dir=args.subset,\n        split=args.split,\n        use_auth_token=True,\n        num_proc=args.num_workers if not args.streaming else None,\n        streaming=args.streaming,\n    )\n    if args.streaming:\n        print(\"Loading the dataset in streaming mode\")\n        valid_data = dataset.take(args.size_valid_set)\n        train_data = dataset.skip(args.size_valid_set)\n        train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=args.seed)\n    else:\n        dataset = dataset.train_test_split(test_size=0.005, seed=args.seed)\n        train_data = dataset[\"train\"]\n        valid_data = dataset[\"test\"]\n        print(f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\")\n\n    chars_per_token = chars_token_ratio(train_data, tokenizer)\n    print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")\n\n    train_dataset = ConstantLengthDataset(\n        tokenizer,\n        train_data,\n        formatting_func=prepare_sample_text,\n        infinite=True,\n        seq_length=args.seq_length,\n        chars_per_token=chars_per_token,\n    )\n    valid_dataset = ConstantLengthDataset(\n        tokenizer,\n        valid_data,\n        formatting_func=prepare_sample_text,\n        infinite=False,\n        seq_length=args.seq_length,\n        chars_per_token=chars_per_token,\n    )\n    return train_dataset, valid_dataset\n\n\ndef run_training(args, train_data, val_data):\n    print(\"Loading the model\")\n\n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    train_data.start_iteration = 0\n\n    print(\"Starting main loop\")\n\n    training_args = TrainingArguments(\n        output_dir=args.output_dir,\n        dataloader_drop_last=True,\n        evaluation_strategy=\"steps\",\n        max_steps=args.max_steps,\n        eval_steps=args.eval_freq,\n        save_steps=args.save_freq,\n        logging_steps=args.log_freq,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        learning_rate=args.learning_rate,\n        lr_scheduler_type=args.lr_scheduler_type,\n        warmup_steps=args.num_warmup_steps,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        gradient_checkpointing=args.gradient_checkpointing,\n        fp16=args.fp16,\n        bf16=args.bf16,\n        weight_decay=args.weight_decay,\n        run_name=\"llama-7b-finetuned\",\n        report_to=\"wandb\",\n        ddp_find_unused_parameters=False,\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_path, load_in_8bit=True, device_map={\"\": Accelerator().process_index}\n    )\n\n    trainer = SFTTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_data,\n        eval_dataset=val_data,\n        peft_config=lora_config,\n        packing=True,\n    )\n\n    print_trainable_parameters(trainer.model)\n\n    print(\"Training...\")\n    trainer.train()\n\n    print(\"Saving last checkpoint of the model\")\n    trainer.model.save_pretrained(os.path.join(args.output_dir, \"final_checkpoint/\"))\n\n\ndef main(args):\n    tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n    train_dataset, eval_dataset = create_datasets(tokenizer, args)\n    run_training(args, train_dataset, eval_dataset)\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n    assert args.model_path != \"\", \"Please provide the llama model path\"\n\n    set_seed(args.seed)\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    logging.set_verbosity_error()\n\n    main(args)\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/research_projects/stack_llama/scripts/rl_training.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# coding=utf-8\n\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport torch\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nfrom peft import LoraConfig\nfrom tqdm import tqdm\nfrom transformers import Adafactor, AutoTokenizer, HfArgumentParser, pipeline\n\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\nfrom trl.core import LengthSampler\n\n\ntqdm.pandas()\n\n\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    The name of the Casual LM model we wish to fine-tune with PPO\n    \"\"\"\n\n    # NOTE: gpt2 models use Conv1D instead of Linear layers which are not yet supported in 8 bit mode\n    # models like gpt-neo* models are more suitable.\n    model_name: Optional[str] = field(default=\"\", metadata={\"help\": \"the model name\"})\n    tokenizer_name: Optional[str] = field(default=\"\", metadata={\"help\": \"the tokenizer name\"})\n    reward_model_name: Optional[str] = field(default=\"\", metadata={\"help\": \"the reward model name\"})\n    log_with: Optional[str] = field(default=None, metadata={\"help\": \"use 'wandb' to log with wandb\"})\n    learning_rate: Optional[float] = field(default=1.41e-5, metadata={\"help\": \"the learning rate\"})\n    output_max_length: Optional[int] = field(default=128, metadata={\"help\": \"maximum length for generation\"})\n    mini_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"the PPO minibatch size\"})\n    batch_size: Optional[int] = field(default=32, metadata={\"help\": \"the batch size\"})\n    ppo_epochs: Optional[int] = field(default=4, metadata={\"help\": \"the number of ppo epochs\"})\n    gradient_accumulation_steps: Optional[int] = field(\n        default=4, metadata={\"help\": \"the number of gradient accumulation steps\"}\n    )\n    adafactor: Optional[bool] = field(default=False, metadata={\"help\": \"whether to use the adafactor optimizer\"})\n    early_stopping: Optional[bool] = field(default=False, metadata={\"help\": \"whether to early stop\"})\n    target_kl: Optional[float] = field(default=0.1, metadata={\"help\": \"kl target for early stopping\"})\n    reward_baseline: Optional[float] = field(\n        default=0.0,\n        metadata={\"help\": \"a baseline value that is subtracted from the reward\"},\n    )\n    batched_gen: Optional[bool] = field(default=False, metadata={\"help\": \"whether to use the batched text gen\"})\n    save_freq: Optional[int] = field(default=None, metadata={\"help\": \"n steps to save the model\"})\n    output_dir: Optional[str] = field(default=\"runs/\", metadata={\"help\": \"n steps to save the model\"})\n    seed: Optional[int] = field(default=0, metadata={\"help\": \"the seed\"})\n    steps: Optional[int] = field(default=20000, metadata={\"help\": \"number of epochs\"})\n    init_kl_coef: Optional[float] = field(\n        default=0.2,\n        metadata={\"help\": \"Initial KL penalty coefficient (used for adaptive and linear control)\"},\n    )\n\n    adap_kl_ctrl: Optional[bool] = field(default=True, metadata={\"help\": \"Use adaptive KL control, otherwise linear\"})\n\n\nparser = HfArgumentParser(ScriptArguments)\nscript_args: ScriptArguments = parser.parse_args_into_dataclasses()[0]\nreward_model_name = script_args.reward_model_name\ndataset_name = \"lvwerra/stack-exchange-paired\"\nconfig = PPOConfig(\n    steps=script_args.steps,\n    model_name=script_args.model_name,\n    learning_rate=script_args.learning_rate,\n    log_with=script_args.log_with,\n    batch_size=script_args.batch_size,\n    mini_batch_size=script_args.mini_batch_size,\n    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n    optimize_cuda_cache=True,\n    early_stopping=script_args.early_stopping,\n    target_kl=script_args.target_kl,\n    ppo_epochs=script_args.ppo_epochs,\n    seed=script_args.seed,\n    init_kl_coef=script_args.init_kl_coef,\n    adap_kl_ctrl=script_args.adap_kl_ctrl,\n)\n\ntrain_dataset = load_dataset(\"lvwerra/stack-exchange-paired\", data_dir=\"data/rl\", split=\"train\")\ntrain_dataset = train_dataset.select(range(100000))\noriginal_columns = train_dataset.column_names\n\n# We then define the arguments to pass to the sentiment analysis pipeline.\n# We set `return_all_scores` to True to get the sentiment score for each token.\nsent_kwargs = {\n    \"return_all_scores\": True,\n    \"function_to_apply\": \"none\",\n    \"batch_size\": 16,\n    \"truncation\": True,\n}\n\ntokenizer = AutoTokenizer.from_pretrained(script_args.tokenizer_name)\n# GPT-2 tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.\n# only for this model.\n\nif getattr(tokenizer, \"pad_token\", None) is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n\n# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n# from the `datasets` library. One should customize this function to train the model on\n# its own dataset.\ndef build_dataset(\n    tokenizer,\n    dataset_name=\"lvwerra/stack-exchange-paired\",\n):\n    \"\"\"\n    Build dataset for training. This builds the dataset from `load_dataset`, one should\n    customize this function to train the model on its own dataset.\n\n    Args:\n        dataset_name (`str`):\n            The name of the dataset to be loaded.\n\n    Returns:\n        dataloader (`torch.utils.data.DataLoader`):\n            The dataloader for the dataset.\n    \"\"\"\n\n    num_proc = 24\n\n    def preprocess_function(examples):\n        new_examples = {\n            \"query\": [],\n            \"input_ids\": [],\n        }\n        for question in examples[\"question\"]:\n            query = \"Question: \" + question + \"\\n\\nAnswer: \"\n            tokenized_question = tokenizer(query, truncation=True)\n            new_examples[\"query\"].append(query)\n            new_examples[\"input_ids\"].append(tokenized_question[\"input_ids\"])\n\n        return new_examples\n\n    ds = train_dataset.map(\n        preprocess_function,\n        batched=True,\n        num_proc=num_proc,\n        remove_columns=original_columns,\n    )\n    ds = ds.filter(lambda x: len(x[\"input_ids\"]) < 512, batched=False)\n\n    ds.set_format(type=\"torch\")\n    return ds\n\n\n# We retrieve the dataloader by calling the `build_dataset` function.\ndataset = build_dataset(tokenizer)\n\n\ndef collator(data):\n    return dict((key, [d[key] for d in data]) for key in data[0])\n\n\n# set seed before initializing value head for deterministic eval\nset_seed(config.seed)\n\n# Now let's build the model, the reference model, and the tokenizer.\ncurrent_device = Accelerator().local_process_index\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\n    config.model_name,\n    load_in_8bit=True,\n    device_map={\"\": current_device},\n    peft_config=lora_config,\n)\n\noptimizer = None\nif script_args.adafactor:\n    optimizer = Adafactor(\n        filter(lambda p: p.requires_grad, model.parameters()),\n        scale_parameter=False,\n        relative_step=False,\n        warmup_init=False,\n        lr=config.learning_rate,\n    )\n# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\nppo_trainer = PPOTrainer(\n    config,\n    model,\n    ref_model=None,\n    tokenizer=tokenizer,\n    dataset=dataset,\n    data_collator=collator,\n    optimizer=optimizer,\n)\n\n# We then build the sentiment analysis pipeline using our reward model, passing the\n# model name and the sentiment analysis pipeline arguments. Let's also make sure to\n# set the device to the same device as the PPOTrainer.\ndevice = ppo_trainer.accelerator.device\nif ppo_trainer.accelerator.num_processes == 1:\n    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a ` pipeline` bug\nsentiment_pipe = pipeline(\n    \"sentiment-analysis\",\n    model=reward_model_name,\n    device_map={\"\": current_device},\n    model_kwargs={\"load_in_8bit\": True},\n    tokenizer=tokenizer,\n    return_token_type_ids=False,\n)\n\n# We then define the arguments to pass to the `generate` function. These arguments\n# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n# the `generate` function of the trained model.\ngeneration_kwargs = {\n    # \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.pad_token_id,\n    \"eos_token_id\": 100_000,\n}\noutput_min_length = 32\noutput_max_length = script_args.output_max_length\noutput_length_sampler = LengthSampler(output_min_length, output_max_length)\n\nfor epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n    if epoch >= config.total_ppo_epochs:\n        break\n\n    question_tensors = batch[\"input_ids\"]\n\n    response_tensors = ppo_trainer.generate(\n        question_tensors,\n        return_prompt=False,\n        length_sampler=output_length_sampler,\n        **generation_kwargs,\n    )\n    batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n\n    # Compute reward score (using the sentiment analysis pipeline)\n    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n    rewards = [torch.tensor(output[0][\"score\"] - script_args.reward_baseline) for output in pipe_outputs]\n\n    # Run PPO step\n    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n    ppo_trainer.log_stats(stats, batch, rewards)\n\n    if script_args.save_freq and epoch and epoch % script_args.save_freq == 0:\n        ppo_trainer.save_pretrained(script_args.output_dir + f\"step_{epoch}\")\n"
    },
    {
        "file": "kwaai-pai/app/trl/examples/research_projects/stack_llama/scripts/reward_modeling.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Union\n\nimport evaluate\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom datasets import load_dataset\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    HfArgumentParser,\n    PreTrainedTokenizerBase,\n    Trainer,\n    TrainerCallback,\n    TrainingArguments,\n)\nfrom transformers.utils import PaddingStrategy\n\n\n# Define and parse arguments.\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    These arguments vary depending on how many GPUs you have, what their capacity and features are, and what size model you want to train.\n    \"\"\"\n\n    local_rank: Optional[int] = field(default=-1, metadata={\"help\": \"Used for multi-gpu\"})\n    resume_from_checkpoint: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"If you want to resume training where it left off.\"},\n    )\n    deepspeed: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"Path to deepspeed config if using deepspeed. You may need this if the model that you want to train doesn't fit on a single GPU.\"\n        },\n    )\n    per_device_train_batch_size: Optional[int] = field(default=4)\n    per_device_eval_batch_size: Optional[int] = field(default=1)\n    gradient_accumulation_steps: Optional[int] = field(default=1)\n    learning_rate: Optional[float] = field(default=2e-5)\n    weight_decay: Optional[float] = field(default=0.001)\n    model_name: Optional[str] = field(\n        default=\"gpt2\",\n        metadata={\n            \"help\": \"The model that you want to train from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc.\"\n        },\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"The tokenizer for your model, if left empty will use the default for your model\",\n        },\n    )\n    bf16: Optional[bool] = field(\n        default=True,\n        metadata={\n            \"help\": \"This essentially cuts the training time in half if you want to sacrifice a little precision and have a supported GPU.\"\n        },\n    )\n    num_train_epochs: Optional[int] = field(\n        default=1,\n        metadata={\"help\": \"The number of training epochs for the reward model.\"},\n    )\n    train_subset: Optional[int] = field(\n        default=100000,\n        metadata={\"help\": \"The size of the subset of the training data to use\"},\n    )\n    eval_subset: Optional[int] = field(\n        default=50000,\n        metadata={\"help\": \"The size of the subset of the eval data to use\"},\n    )\n    gradient_checkpointing: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Enables gradient checkpointing.\"},\n    )\n    optim: Optional[str] = field(\n        default=\"adamw_hf\",\n        metadata={\"help\": \"The optimizer to use.\"},\n    )\n    lr_scheduler_type: Optional[str] = field(\n        default=\"linear\",\n        metadata={\"help\": \"The lr scheduler\"},\n    )\n    max_length: Optional[int] = field(default=512)\n    eval_first_step: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Whether to run eval after the first step\"},\n    )\n\n\nparser = HfArgumentParser(ScriptArguments)\nscript_args = parser.parse_args_into_dataclasses()[0]\n\n# Load the human stack-exchange-paired dataset for tuning the reward model.\ntrain_dataset = load_dataset(\"lvwerra/stack-exchange-paired\", data_dir=\"data/reward\", split=\"train\")\nif script_args.train_subset > 0:\n    train_dataset = train_dataset.select(range(script_args.train_subset))\neval_dataset = load_dataset(\"lvwerra/stack-exchange-paired\", data_dir=\"data/evaluation\", split=\"train\")\nif script_args.eval_subset > 0:\n    eval_dataset = eval_dataset.select(range(script_args.eval_subset))\n# Define the training args. Needs to be done before the model is loaded if you are using deepspeed.\nmodel_name_split = script_args.model_name.split(\"/\")[-1]\noutput_name = (\n    f\"{model_name_split}_peft_stack-exchange-paired_rmts__{script_args.train_subset}_{script_args.learning_rate}\"\n)\n\ntraining_args = TrainingArguments(\n    output_dir=output_name,\n    learning_rate=script_args.learning_rate,\n    per_device_train_batch_size=script_args.per_device_train_batch_size,\n    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n    num_train_epochs=script_args.num_train_epochs,\n    weight_decay=script_args.weight_decay,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n    gradient_checkpointing=script_args.gradient_checkpointing,\n    deepspeed=script_args.deepspeed,\n    local_rank=script_args.local_rank,\n    remove_unused_columns=False,\n    label_names=[],\n    bf16=script_args.bf16,\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    optim=script_args.optim,\n    lr_scheduler_type=script_args.lr_scheduler_type,\n)\n# Load the value-head model and tokenizer.\ntokenizer_name = script_args.tokenizer_name if script_args.tokenizer_name is not None else script_args.model_name\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_auth_token=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    script_args.model_name, num_labels=1, torch_dtype=torch.bfloat16\n)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\n# Need to do this for gpt2, because it doesn't have an official pad token.\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = tokenizer.eos_token_id\nmodel.config.use_cache = not script_args.gradient_checkpointing\nnum_proc = 24  # Can adjust to be higher if you have more processors.\noriginal_columns = train_dataset.column_names\n\n\n# Turn the dataset into pairs of post + summaries, where text_j is the preferred question + answer and text_k is the other.\n# Then tokenize the dataset.\ndef preprocess_function(examples):\n    new_examples = {\n        \"input_ids_j\": [],\n        \"attention_mask_j\": [],\n        \"input_ids_k\": [],\n        \"attention_mask_k\": [],\n    }\n    for question, response_j, response_k in zip(examples[\"question\"], examples[\"response_j\"], examples[\"response_k\"]):\n        tokenized_j = tokenizer(\"Question: \" + question + \"\\n\\nAnswer: \" + response_j, truncation=True)\n        tokenized_k = tokenizer(\"Question: \" + question + \"\\n\\nAnswer: \" + response_k, truncation=True)\n\n        new_examples[\"input_ids_j\"].append(tokenized_j[\"input_ids\"])\n        new_examples[\"attention_mask_j\"].append(tokenized_j[\"attention_mask\"])\n        new_examples[\"input_ids_k\"].append(tokenized_k[\"input_ids\"])\n        new_examples[\"attention_mask_k\"].append(tokenized_k[\"attention_mask\"])\n\n    return new_examples\n\n\n# preprocess the dataset and filter out QAs that are longer than script_args.max_length\ntrain_dataset = train_dataset.map(\n    preprocess_function,\n    batched=True,\n    num_proc=num_proc,\n    remove_columns=original_columns,\n)\ntrain_dataset = train_dataset.filter(\n    lambda x: len(x[\"input_ids_j\"]) <= script_args.max_length and len(x[\"input_ids_k\"]) <= script_args.max_length\n)\n\neval_dataset = eval_dataset.map(\n    preprocess_function,\n    batched=True,\n    num_proc=num_proc,\n    remove_columns=original_columns,\n)\neval_dataset = eval_dataset.filter(\n    lambda x: len(x[\"input_ids_j\"]) <= script_args.max_length and len(x[\"input_ids_k\"]) <= script_args.max_length\n)\n\n\n# We need to define a special data collator that batches the data in our j vs k format.\n@dataclass\nclass RewardDataCollatorWithPadding:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    return_tensors: str = \"pt\"\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n        features_j = []\n        features_k = []\n        for feature in features:\n            features_j.append(\n                {\n                    \"input_ids\": feature[\"input_ids_j\"],\n                    \"attention_mask\": feature[\"attention_mask_j\"],\n                }\n            )\n            features_k.append(\n                {\n                    \"input_ids\": feature[\"input_ids_k\"],\n                    \"attention_mask\": feature[\"attention_mask_k\"],\n                }\n            )\n        batch_j = self.tokenizer.pad(\n            features_j,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=self.return_tensors,\n        )\n        batch_k = self.tokenizer.pad(\n            features_k,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=self.return_tensors,\n        )\n        batch = {\n            \"input_ids_j\": batch_j[\"input_ids\"],\n            \"attention_mask_j\": batch_j[\"attention_mask\"],\n            \"input_ids_k\": batch_k[\"input_ids\"],\n            \"attention_mask_k\": batch_k[\"attention_mask\"],\n            \"return_loss\": True,\n        }\n        return batch\n\n\n# Define the metric that we'll use for validation.\naccuracy = evaluate.load(\"accuracy\")\n\n\ndef compute_metrics(eval_pred):\n    predictions, _ = eval_pred\n    # Here, predictions is rewards_j and rewards_k.\n    # We want to see how much of the time rewards_j > rewards_k.\n    predictions = np.argmax(predictions, axis=0)\n    labels = np.zeros(predictions.shape)\n    return accuracy.compute(predictions=predictions, references=labels)\n\n\nclass RewardTrainer(Trainer):\n    # Define how to compute the reward loss. We use the InstructGPT pairwise logloss: https://arxiv.org/abs/2203.02155\n    def compute_loss(self, model, inputs, return_outputs=False):\n        rewards_j = model(input_ids=inputs[\"input_ids_j\"], attention_mask=inputs[\"attention_mask_j\"])[0]\n        rewards_k = model(input_ids=inputs[\"input_ids_k\"], attention_mask=inputs[\"attention_mask_k\"])[0]\n        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n        if return_outputs:\n            return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n        return loss\n\n\n# Train the model, woohoo.\ntrainer = RewardTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n    data_collator=RewardDataCollatorWithPadding(tokenizer=tokenizer, max_length=script_args.max_length),\n)\n\n\nif script_args.eval_first_step:\n\n    class EvaluateFirstStepCallback(TrainerCallback):\n        def on_step_end(self, args, state, control, **kwargs):\n            if state.global_step == 1:\n                control.should_evaluate = True\n\n    trainer.add_callback(EvaluateFirstStepCallback())\n\ntrainer.train(script_args.resume_from_checkpoint)\n\nprint(\"Saving last checkpoint of the model\")\nmodel.save_pretrained(output_name + \"_peft_last_checkpoint\")\n"
    },
    {
        "file": "kwaai-pai/app/trl/benchmark/upload_benchmark.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from dataclasses import dataclass\n\nimport tyro\nfrom huggingface_hub import HfApi\n\n\n@dataclass\nclass Args:\n    folder_path: str = \"benchmark/trl\"\n    path_in_repo: str = \"images/benchmark\"\n    repo_id: str = \"trl-internal-testing/example-images\"\n    repo_type: str = \"dataset\"\n\n\nargs = tyro.cli(Args)\napi = HfApi()\n\napi.upload_folder(\n    folder_path=args.folder_path,\n    path_in_repo=args.path_in_repo,\n    repo_id=args.repo_id,\n    repo_type=args.repo_type,\n)\n"
    },
    {
        "file": "kwaai-pai/app/trl/benchmark/benchmark.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import argparse\nimport math\nimport os\nimport shlex\nimport subprocess\nimport uuid\nfrom distutils.util import strtobool\n\nimport requests\n\n\ndef parse_args():\n    # fmt: off\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--command\", type=str, default=\"\",\n        help=\"the command to run\")\n    parser.add_argument(\"--num-seeds\", type=int, default=3,\n        help=\"the number of random seeds\")\n    parser.add_argument(\"--start-seed\", type=int, default=1,\n        help=\"the number of the starting seed\")\n    parser.add_argument(\"--workers\", type=int, default=0,\n        help=\"the number of workers to run benchmark experimenets\")\n    parser.add_argument(\"--auto-tag\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"if toggled, the runs will be tagged with git tags, commit, and pull request number if possible\")\n    parser.add_argument(\"--slurm-template-path\", type=str, default=None,\n        help=\"the path to the slurm template file (see docs for more details)\")\n    parser.add_argument(\"--slurm-gpus-per-task\", type=int, default=1,\n        help=\"the number of gpus per task to use for slurm jobs\")\n    parser.add_argument(\"--slurm-total-cpus\", type=int, default=50,\n        help=\"the number of gpus per task to use for slurm jobs\")\n    parser.add_argument(\"--slurm-ntasks\", type=int, default=1,\n        help=\"the number of tasks to use for slurm jobs\")\n    parser.add_argument(\"--slurm-nodes\", type=int, default=None,\n        help=\"the number of nodes to use for slurm jobs\")\n    args = parser.parse_args()\n    # fmt: on\n    return args\n\n\ndef run_experiment(command: str):\n    command_list = shlex.split(command)\n    print(f\"running {command}\")\n\n    # Use subprocess.PIPE to capture the output\n    fd = subprocess.Popen(command_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    output, errors = fd.communicate()\n\n    return_code = fd.returncode\n    assert return_code == 0, f\"Command failed with error: {errors.decode('utf-8')}\"\n\n    # Convert bytes to string and strip leading/trailing whitespaces\n    return output.decode(\"utf-8\").strip()\n\n\ndef autotag() -> str:\n    wandb_tag = \"\"\n    print(\"autotag feature is enabled\")\n    git_tag = \"\"\n    try:\n        git_tag = subprocess.check_output([\"git\", \"describe\", \"--tags\"]).decode(\"ascii\").strip()\n        print(f\"identified git tag: {git_tag}\")\n    except subprocess.CalledProcessError as e:\n        print(e)\n    if len(git_tag) == 0:\n        try:\n            count = int(subprocess.check_output([\"git\", \"rev-list\", \"--count\", \"HEAD\"]).decode(\"ascii\").strip())\n            hash = subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"]).decode(\"ascii\").strip()\n            git_tag = f\"no-tag-{count}-g{hash}\"\n            print(f\"identified git tag: {git_tag}\")\n        except subprocess.CalledProcessError as e:\n            print(e)\n    wandb_tag = f\"{git_tag}\"\n\n    git_commit = subprocess.check_output([\"git\", \"rev-parse\", \"--verify\", \"HEAD\"]).decode(\"ascii\").strip()\n    try:\n        # try finding the pull request number on github\n        prs = requests.get(f\"https://api.github.com/search/issues?q=repo:huggingface/trl+is:pr+{git_commit}\")\n        if prs.status_code == 200:\n            prs = prs.json()\n            if len(prs[\"items\"]) > 0:\n                pr = prs[\"items\"][0]\n                pr_number = pr[\"number\"]\n                wandb_tag += f\",pr-{pr_number}\"\n        print(f\"identified github pull request: {pr_number}\")\n    except Exception as e:\n        print(e)\n\n    return wandb_tag\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    if args.auto_tag:\n        existing_wandb_tag = os.environ.get(\"WANDB_TAGS\", \"\")\n        wandb_tag = autotag()\n        if len(wandb_tag) > 0:\n            if len(existing_wandb_tag) > 0:\n                os.environ[\"WANDB_TAGS\"] = \",\".join([existing_wandb_tag, wandb_tag])\n            else:\n                os.environ[\"WANDB_TAGS\"] = wandb_tag\n    print(\"WANDB_TAGS: \", os.environ.get(\"WANDB_TAGS\", \"\"))\n    commands = []\n    for seed in range(0, args.num_seeds):\n        commands += [\" \".join([args.command, \"--seed\", str(args.start_seed + seed)])]\n\n    print(\"======= commands to run:\")\n    for command in commands:\n        print(command)\n\n    if args.workers > 0 and args.slurm_template_path is None:\n        from concurrent.futures import ThreadPoolExecutor\n\n        executor = ThreadPoolExecutor(max_workers=args.workers, thread_name_prefix=\"cleanrl-benchmark-worker-\")\n        for command in commands:\n            executor.submit(run_experiment, command)\n        executor.shutdown(wait=True)\n    else:\n        print(\"not running the experiments because --workers is set to 0; just printing the commands to run\")\n\n    # SLURM logic\n    if args.slurm_template_path is not None:\n        if not os.path.exists(\"slurm\"):\n            os.makedirs(\"slurm\")\n        if not os.path.exists(\"slurm/logs\"):\n            os.makedirs(\"slurm/logs\")\n        print(\"======= slurm commands to run:\")\n        with open(args.slurm_template_path) as f:\n            slurm_template = f.read()\n        slurm_template = slurm_template.replace(\"{{array}}\", f\"0-{len(commands) - 1}%{args.workers}\")\n        slurm_template = slurm_template.replace(\n            \"{{seeds}}\", f\"({' '.join([str(args.start_seed + int(seed)) for seed in range(args.num_seeds)])})\"\n        )\n        slurm_template = slurm_template.replace(\"{{len_seeds}}\", f\"{args.num_seeds}\")\n        slurm_template = slurm_template.replace(\"{{command}}\", args.command)\n        slurm_template = slurm_template.replace(\"{{gpus_per_task}}\", f\"{args.slurm_gpus_per_task}\")\n        total_gpus = args.slurm_gpus_per_task * args.slurm_ntasks\n        slurm_cpus_per_gpu = math.ceil(args.slurm_total_cpus / total_gpus)\n        slurm_template = slurm_template.replace(\"{{cpus_per_gpu}}\", f\"{slurm_cpus_per_gpu}\")\n        slurm_template = slurm_template.replace(\"{{ntasks}}\", f\"{args.slurm_ntasks}\")\n        if args.slurm_nodes is not None:\n            slurm_template = slurm_template.replace(\"{{nodes}}\", f\"#SBATCH --nodes={args.slurm_nodes}\")\n        else:\n            slurm_template = slurm_template.replace(\"{{nodes}}\", \"\")\n        filename = str(uuid.uuid4())\n        open(os.path.join(\"slurm\", f\"{filename}.slurm\"), \"w\").write(slurm_template)\n        slurm_path = os.path.join(\"slurm\", f\"{filename}.slurm\")\n        print(f\"saving command in {slurm_path}\")\n        if args.workers > 0:\n            job_id = run_experiment(f\"sbatch --parsable {slurm_path}\")\n            print(f\"Job ID: {job_id}\")\n"
    },
    {
        "file": "kwaai-pai/app/trl/benchmark/post_github_comment.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import json\nimport os\n\nfrom ghapi.all import GhApi\n\n\nFOLDER_STRING = os.environ.get(\"FOLDER_STRING\", \"\")\nfolder = f\"benchmark/trl/{FOLDER_STRING}\"\nhost_url = f\"https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/benchmark/{FOLDER_STRING}\"\n\n# Create a GitHub API instance\ngithub_context = json.loads(os.environ[\"GITHUB_CONTEXT\"])\ntoken = os.environ[\"PERSONAL_ACCESS_TOKEN_GITHUB\"]  # this needs to refreshed every 12 months\nstatus_message = \"**[COSTA BENCHMARK BOT]**: Here are the results\"\nbody = status_message\nrepo = github_context[\"repository\"]\nowner, repo = repo.split(\"/\")\napi = GhApi(owner=owner, repo=repo, token=token)\n\n# for each `.png` file in the folder, add it to the comment\nfor file in os.listdir(folder):\n    if file.endswith(\".png\"):\n        body += f\"\\n![{file}]({host_url}/{file})\"\n\n# Create a comment on the issue\napi.issues.create_comment(issue_number=github_context[\"event\"][\"issue\"][\"number\"], body=body)\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_no_peft.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import sys\nimport unittest\nfrom unittest.mock import patch\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom .testing_utils import is_peft_available, require_peft\n\n\nclass DummyDataset(torch.utils.data.Dataset):\n    def __init__(self, query_data, response_data):\n        self.query_data = query_data\n        self.response_data = response_data\n\n    def __len__(self):\n        return len(self.query_data)\n\n    def __getitem__(self, idx):\n        return self.query_data[idx], self.response_data[idx]\n\n\nEXPECTED_STATS = [\n    \"objective/kl\",\n    \"objective/kl_dist\",\n    \"objective/logprobs\",\n    \"objective/ref_logprobs\",\n    \"objective/kl_coef\",\n    \"objective/entropy\",\n    \"ppo/mean_non_score_reward\",\n    \"ppo/loss/policy\",\n    \"ppo/loss/value\",\n    \"ppo/loss/total\",\n    \"ppo/policy/entropy\",\n    \"ppo/policy/approxkl\",\n    \"ppo/policy/policykl\",\n    \"ppo/policy/clipfrac\",\n    \"ppo/policy/advantages\",\n    \"ppo/policy/advantages_mean\",\n    \"ppo/policy/ratio\",\n    \"ppo/returns/mean\",\n    \"ppo/returns/var\",\n    \"ppo/val/vpred\",\n    \"ppo/val/error\",\n    \"ppo/val/clipfrac\",\n    \"ppo/val/mean\",\n    \"ppo/val/var\",\n    \"ppo/val/var_explained\",\n    \"time/ppo/forward_pass\",\n    \"time/ppo/compute_rewards\",\n    \"time/ppo/optimize_step\",\n    \"time/ppo/calc_stats\",\n    \"time/ppo/total\",\n    \"ppo/learning_rate\",\n]\n\n\n@require_peft\nclass TestPeftDependancy(unittest.TestCase):\n    def setUp(self):\n        self.causal_lm_model_id = \"trl-internal-testing/tiny-random-GPTNeoXForCausalLM\"\n        self.seq_to_seq_model_id = \"trl-internal-testing/tiny-random-T5ForConditionalGeneration\"\n\n        if is_peft_available():\n            from peft import LoraConfig, get_peft_model\n\n            lora_config = LoraConfig(\n                r=16,\n                lora_alpha=32,\n                lora_dropout=0.05,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n\n            causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)\n            self.peft_model = get_peft_model(causal_lm_model, lora_config)\n\n    def test_no_peft(self):\n        with patch.dict(sys.modules, {\"peft\": None}):\n            from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead\n\n            # Check that loading a model with `peft` will raise an error\n            with self.assertRaises(ModuleNotFoundError):\n                import peft  # noqa\n\n            trl_model = AutoModelForCausalLMWithValueHead.from_pretrained(self.causal_lm_model_id)  # noqa\n            trl_seq2seq_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(self.seq_to_seq_model_id)  # noqa\n\n    def test_imports_no_peft(self):\n        with patch.dict(sys.modules, {\"peft\": None}):\n            from trl import (  # noqa\n                AutoModelForCausalLMWithValueHead,\n                AutoModelForSeq2SeqLMWithValueHead,\n                PPOConfig,\n                PPOTrainer,\n                PreTrainedModelWrapper,\n            )\n\n    def test_ppo_trainer_no_peft(self):\n        with patch.dict(sys.modules, {\"peft\": None}):\n            from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n\n            ppo_model_id = \"trl-internal-testing/dummy-GPT2-correct-vocab\"\n\n            trl_model = AutoModelForCausalLMWithValueHead.from_pretrained(ppo_model_id)\n            tokenizer = AutoTokenizer.from_pretrained(ppo_model_id)\n            tokenizer.pad_token_id = tokenizer.eos_token_id\n\n            ppo_config = PPOConfig(batch_size=2, mini_batch_size=1, log_with=None)\n\n            dummy_dataset = DummyDataset(\n                [torch.LongTensor([0, 1, 0, 1, 0, 1]), torch.LongTensor([0, 1, 0, 1, 0, 1])],\n                [torch.LongTensor([1, 0, 1, 0, 1, 0]), torch.LongTensor([0, 1, 0, 1, 0, 1])],\n            )\n\n            ppo_trainer = PPOTrainer(\n                config=ppo_config,\n                model=trl_model,\n                ref_model=None,\n                tokenizer=tokenizer,\n                dataset=dummy_dataset,\n            )\n            dummy_dataloader = ppo_trainer.dataloader\n\n            for query_tensor, response_tensor in dummy_dataloader:\n                # define a reward for response\n                # (this could be any reward such as human feedback or output from another model)\n                reward = [torch.tensor(1.0), torch.tensor(0.0)]\n                # train model\n                train_stats = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n                break\n\n            # check gradients are not None\n            for _, param in trl_model.named_parameters():\n                if param.requires_grad:\n                    self.assertIsNotNone(param.grad)\n\n            # check expected stats\n            for stat in EXPECTED_STATS:\n                self.assertIn(stat, train_stats)\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_dataset_formatting.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import unittest\nfrom typing import Callable\n\nfrom datasets import Dataset, load_dataset\nfrom transformers import AutoTokenizer\n\nfrom trl.extras.dataset_formatting import get_formatting_func_from_dataset\n\n\nclass DatasetFormattingTestCase(unittest.TestCase):\n    def setUp(self):\n        self.llama_tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n        self.chatml_tokenizer = AutoTokenizer.from_pretrained(\"philschmid/gpt2-chatml-tokenizer\")\n\n    def test_get_formatting_func_from_dataset_with_chatml_messages(self):\n        dataset = Dataset.from_dict(\n            {\n                \"messages\": [\n                    [\n                        {\"role\": \"system\", \"content\": \"You are helpful\"},\n                        {\"role\": \"user\", \"content\": \"Hello\"},\n                        {\"role\": \"assistant\", \"content\": \"Hi, how can I help you?\"},\n                    ]\n                ]\n            }\n        )\n\n        # Llama tokenizer\n        formatting_func = get_formatting_func_from_dataset(dataset, self.llama_tokenizer)\n        self.assertTrue(isinstance(formatting_func, Callable))\n        formatted_text = formatting_func(dataset[0])\n        self.assertEqual(\n            formatted_text,\n            \"<s>[INST] <<SYS>>\\nYou are helpful\\n<</SYS>>\\n\\nHello [/INST] Hi, how can I help you? </s>\",\n        )\n        formatted_text = formatting_func(dataset[0:1])\n        self.assertEqual(\n            formatted_text,\n            [\"<s>[INST] <<SYS>>\\nYou are helpful\\n<</SYS>>\\n\\nHello [/INST] Hi, how can I help you? </s>\"],\n        )\n\n        # ChatML tokenizer\n        formatting_func = get_formatting_func_from_dataset(dataset, self.chatml_tokenizer)\n        formatted_text = formatting_func(dataset[0])\n        self.assertEqual(\n            formatted_text,\n            \"<|im_start|>system\\nYou are helpful<|im_end|>\\n<|im_start|>user\\nHello<|im_end|>\\n<|im_start|>assistant\\nHi, how can I help you?<|im_end|>\\n\",\n        )\n        formatted_text = formatting_func(dataset[0:1])\n        self.assertEqual(\n            formatted_text,\n            [\n                \"<|im_start|>system\\nYou are helpful<|im_end|>\\n<|im_start|>user\\nHello<|im_end|>\\n<|im_start|>assistant\\nHi, how can I help you?<|im_end|>\\n\"\n            ],\n        )\n\n    def test_get_formatting_func_from_dataset_with_chatml_conversations(self):\n        dataset = Dataset.from_dict(\n            {\n                \"conversations\": [\n                    [\n                        {\"role\": \"system\", \"content\": \"You are helpful\"},\n                        {\"role\": \"user\", \"content\": \"Hello\"},\n                        {\"role\": \"assistant\", \"content\": \"Hi, how can I help you?\"},\n                    ]\n                ]\n            }\n        )\n        # Llama tokenizer\n        formatting_func = get_formatting_func_from_dataset(dataset, self.llama_tokenizer)\n        self.assertTrue(isinstance(formatting_func, Callable))\n        formatted_text = formatting_func(dataset[0])\n        self.assertEqual(\n            formatted_text,\n            \"<s>[INST] <<SYS>>\\nYou are helpful\\n<</SYS>>\\n\\nHello [/INST] Hi, how can I help you? </s>\",\n        )\n        formatted_text = formatting_func(dataset[0:1])\n        self.assertEqual(\n            formatted_text,\n            [\"<s>[INST] <<SYS>>\\nYou are helpful\\n<</SYS>>\\n\\nHello [/INST] Hi, how can I help you? </s>\"],\n        )\n\n        # ChatML tokenizer\n        formatting_func = get_formatting_func_from_dataset(dataset, self.chatml_tokenizer)\n        formatted_text = formatting_func(dataset[0])\n        self.assertEqual(\n            formatted_text,\n            \"<|im_start|>system\\nYou are helpful<|im_end|>\\n<|im_start|>user\\nHello<|im_end|>\\n<|im_start|>assistant\\nHi, how can I help you?<|im_end|>\\n\",\n        )\n        formatted_text = formatting_func(dataset[0:1])\n        self.assertEqual(\n            formatted_text,\n            [\n                \"<|im_start|>system\\nYou are helpful<|im_end|>\\n<|im_start|>user\\nHello<|im_end|>\\n<|im_start|>assistant\\nHi, how can I help you?<|im_end|>\\n\"\n            ],\n        )\n\n    def test_get_formatting_func_from_dataset_with_instruction(self):\n        dataset = Dataset.from_list(\n            [{\"prompt\": \"What is 2+2?\", \"completion\": \"4\"}, {\"prompt\": \"What is 3+3?\", \"completion\": \"6\"}]\n        )\n        formatting_func = get_formatting_func_from_dataset(dataset, self.llama_tokenizer)\n        self.assertIsNotNone(formatting_func)\n        self.assertTrue(isinstance(formatting_func, Callable))\n        formatted_text = formatting_func(dataset[0])\n        self.assertEqual(formatted_text, \"<s>[INST] What is 2+2? [/INST] 4 </s>\")\n        formatted_text = formatting_func(dataset[0:1])\n        self.assertEqual(formatted_text, [\"<s>[INST] What is 2+2? [/INST] 4 </s>\"])\n\n    def test_get_formatting_func_from_dataset_from_hub(self):\n        ds_1 = load_dataset(\"philschmid/trl-test-instruction\", split=\"train\")\n        ds_2 = load_dataset(\"philschmid/dolly-15k-oai-style\", split=\"train\")\n        for ds in [ds_1, ds_2]:\n            formatting_func = get_formatting_func_from_dataset(ds, self.llama_tokenizer)\n            self.assertIsNotNone(formatting_func)\n            self.assertTrue(isinstance(formatting_func, Callable))\n        ds_3 = load_dataset(\"philschmid/guanaco-sharegpt-style\", split=\"train\")\n        formatting_func = get_formatting_func_from_dataset(ds_3, self.llama_tokenizer)\n        self.assertIsNone(formatting_func)\n\n    def test_get_formatting_func_from_dataset_with_unknown_format(self):\n        dataset = Dataset.from_dict({\"text\": \"test\"})\n        formatting_func = get_formatting_func_from_dataset(dataset, self.llama_tokenizer)\n        self.assertIsNone(formatting_func)\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_e2e.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import subprocess\n\n\ndef test_hello_world():\n    subprocess.run(\n        \"python examples/hello_world.py\",\n        shell=True,\n        check=True,\n    )\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_data_collator_completion_only.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import unittest\n\nimport torch\nfrom transformers import AutoTokenizer\n\nfrom trl import DataCollatorForCompletionOnlyLM\n\n\nclass DataCollatorForCompletionOnlyLMTester(unittest.TestCase):\n    def test_data_collator_finds_response_template_llama2_tokenizer(self):\n        # this should ideally be tested with meta-llama/Llama-2-7b-hf\n        self.tokenizer = AutoTokenizer.from_pretrained(\"trl-internal-testing/dummy-GPT2-correct-vocab\")\n        self.instruction = \"\"\"### System: You are a helpful assistant.\n\n### User: How much is 2+2?\n\n### Assistant: 2+2 equals 4\"\"\"\n        self.instruction_template = \"\\n### User:\"\n        self.response_template = \"\\n### Assistant:\"\n\n        # GPT2Tokenizer: [198, 21017, 11787, 25] -> [21017, 11787, 25]\n        # Llama2Tokenizer: [29871, 13, 2277, 29937, 4911, 29901] -> [2277, 29937, 4911, 29901]\n        # Note: If this test is ever switched to Llama2Tokenizer, this should be double checked,\n        # and possibly switched back to [2:] instead of [1:].\n        # With GPT2Tokenizer, [1:] is correct - we want the 21017 token included, which is ###.\n        self.tokenized_instruction_w_context = self.tokenizer.encode(\n            self.instruction_template, add_special_tokens=False\n        )[1:]\n\n        # GPT2Tokenizer: [198, 21017, 15286, 25] -> [15286, 25]\n        # Llama2Tokenizer: [29871, 13, 2277, 29937, 4007, 22137, 29901] -> [2277, 29937, 4007, 22137, 29901]\n        self.tokenized_response_w_context = self.tokenizer.encode(self.response_template, add_special_tokens=False)[2:]\n\n        # Plain check on string\n        self.assertIn(self.response_template, self.instruction)\n        self.tokenized_instruction = self.tokenizer.encode(self.instruction, add_special_tokens=False)\n\n        # Test the fix for #598\n        # Pass already tokenized (w context) and truncated response_template so token_ids are like in the instruction + response\n        self.collator = DataCollatorForCompletionOnlyLM(self.tokenized_response_w_context, tokenizer=self.tokenizer)\n        self.collator.torch_call([self.tokenized_instruction])\n\n        # Test for PR #749\n        # Pass already tokenized (w context) instruction and response both so token_ids are like in the instruction + response\n        self.collator = DataCollatorForCompletionOnlyLM(\n            self.tokenized_response_w_context, self.tokenized_instruction_w_context, tokenizer=self.tokenizer\n        )\n        self.collator.torch_call([self.tokenized_instruction])\n\n        # Test for PR #1185\n        # We pass in a string where the first user template is different than the rest.\n        # Usually this would happen due to context-sensitive tokenization, but here we\n        # explicitly change the template to test the fix.\n        self.instruction = \"\"\"## User: First instruction\n\n### Assistant: First response\n\n### User: Second instruction\n\n### Assistant: Second response\"\"\"\n        self.tokenized_instruction = self.tokenizer.encode(self.instruction, add_special_tokens=False)\n        self.collator = DataCollatorForCompletionOnlyLM(\n            self.tokenized_response_w_context, self.tokenized_instruction_w_context, tokenizer=self.tokenizer\n        )\n        collator_output = self.collator.torch_call([self.tokenized_instruction])\n        collator_text = self.tokenizer.decode(\n            collator_output[\"labels\"][torch.where(collator_output[\"labels\"] != -100)]\n        )\n        expected_text = \" First response\\n\\n Second response\" \"\"\n        self.assertEqual(collator_text, expected_text)\n\n    def test_data_collator_handling_of_long_sequences(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(\"trl-internal-testing/dummy-GPT2-correct-vocab\")\n        self.instruction = \"\"\"### System: You are a helpful assistant.\n\n### User: How much is 2+2? I'm asking because I'm not sure. And I'm not sure because I'm not good at math.\n\"\"\"\n        self.response_template = \"\\n### Assistant:\"\n        # check DataCollatorForCompletionOnlyLM using response template only\n        self.tokenized_instruction = self.tokenizer.encode(self.instruction, add_special_tokens=False)\n        self.collator = DataCollatorForCompletionOnlyLM(self.response_template, tokenizer=self.tokenizer)\n        encoded_instance = self.collator.torch_call([self.tokenized_instruction])\n        result = torch.all(encoded_instance[\"labels\"] == -100)\n        self.assertTrue(result, \"Not all values in the tensor are -100.\")\n\n        # check DataCollatorForCompletionOnlyLM using response template and instruction template\n        self.instruction_template = \"\\n### User:\"\n        self.collator = DataCollatorForCompletionOnlyLM(\n            self.response_template, self.instruction_template, tokenizer=self.tokenizer\n        )\n        encoded_instance = self.collator.torch_call([self.tokenized_instruction])\n        result = torch.all(encoded_instance[\"labels\"] == -100)\n        self.assertTrue(result, \"Not all values in the tensor are -100.\")\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_reward_trainer.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import tempfile\nimport unittest\n\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction\n\nfrom trl import RewardConfig, RewardTrainer\nfrom trl.trainer import compute_accuracy\n\nfrom .testing_utils import require_peft\n\n\nclass RewardTrainerTester(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.model_id = \"trl-internal-testing/dummy-GPT2-correct-vocab\"\n        cls.model = AutoModelForSequenceClassification.from_pretrained(cls.model_id)\n        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_id)\n        cls.tokenizer.pad_token = cls.tokenizer.eos_token\n\n    def test_accuracy_metrics(self):\n        dummy_eval_predictions = EvalPrediction(torch.FloatTensor([[0.1, 0.9], [0.9, 0.1]]), torch.LongTensor([0, 0]))\n        accuracy = compute_accuracy(dummy_eval_predictions)\n        self.assertEqual(accuracy[\"accuracy\"], 0.5)\n\n    def test_reward_trainer(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = RewardConfig(\n                output_dir=tmp_dir,\n                per_device_train_batch_size=2,\n                max_steps=3,\n                remove_unused_columns=False,\n                gradient_accumulation_steps=4,\n                learning_rate=9e-1,\n                evaluation_strategy=\"steps\",\n            )\n\n            # fmt: off\n            dummy_dataset_dict = {\n                \"input_ids_chosen\": [\n                    torch.LongTensor([0, 1, 2,]),\n                    torch.LongTensor([1, 2]),\n                    torch.LongTensor([0, 1, 2,]),\n                    torch.LongTensor([1, 2]),\n                ],\n                \"attention_mask_chosen\": [\n                    torch.LongTensor([1, 1, 1]),\n                    torch.LongTensor([1, 0]),\n                    torch.LongTensor([1, 1, 1]),\n                    torch.LongTensor([1, 0]),\n                ],\n                \"input_ids_rejected\": [\n                    torch.LongTensor([0, 2,]),\n                    torch.LongTensor([1, 2, 0]),\n                    torch.LongTensor([0, 2,]),\n                    torch.LongTensor([1, 2, 0]),\n                ],\n                \"attention_mask_rejected\": [\n                    torch.LongTensor([1, 1]),\n                    torch.LongTensor([1, 1, 0]),\n                    torch.LongTensor([1, 1]),\n                    torch.LongTensor([1, 1, 1]),\n                ],\n            }\n            # fmt: on\n            dummy_dataset = Dataset.from_dict(dummy_dataset_dict)\n\n            trainer = RewardTrainer(\n                model=self.model,\n                args=training_args,\n                tokenizer=self.tokenizer,\n                train_dataset=dummy_dataset,\n                eval_dataset=dummy_dataset,\n            )\n\n            previous_trainable_params = {n: param.clone() for n, param in trainer.model.named_parameters()}\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n\n            # check the params have changed\n            for n, param in previous_trainable_params.items():\n                new_param = trainer.model.get_parameter(n)\n                # check the params have changed - ignore 0 biases\n                if param.sum() != 0:\n                    self.assertFalse(torch.equal(param, new_param))\n\n            preds = trainer.predict(dummy_dataset)\n            self.assertEqual(preds.predictions.shape, (4, 2))\n\n    @require_peft\n    def test_reward_trainer_peft(self):\n        import peft\n        from peft import LoraConfig, TaskType\n\n        peft_version = peft.__version__\n\n        peft_config = LoraConfig(\n            task_type=TaskType.SEQ_CLS,\n            inference_mode=False,\n            r=8,\n            lora_alpha=32,\n            lora_dropout=0.1,\n        )\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = RewardConfig(\n                output_dir=tmp_dir,\n                per_device_train_batch_size=2,\n                max_steps=6,\n                remove_unused_columns=False,\n                gradient_accumulation_steps=2,\n                learning_rate=9e-1,\n                evaluation_strategy=\"steps\",\n            )\n\n            # fmt: off\n            dummy_dataset_dict = {\n                \"input_ids_chosen\": [\n                    torch.LongTensor([0, 1, 2,]),\n                    torch.LongTensor([1, 2]),\n                    torch.LongTensor([0, 1, 2,]),\n                    torch.LongTensor([1, 2]),\n                ],\n                \"attention_mask_chosen\": [\n                    torch.LongTensor([1, 1, 1]),\n                    torch.LongTensor([1, 0]),\n                    torch.LongTensor([1, 1, 1]),\n                    torch.LongTensor([1, 0]),\n                ],\n                \"input_ids_rejected\": [\n                    torch.LongTensor([0, 2,]),\n                    torch.LongTensor([1, 2, 0]),\n                    torch.LongTensor([0, 2,]),\n                    torch.LongTensor([1, 2, 0]),\n                ],\n                \"attention_mask_rejected\": [\n                    torch.LongTensor([1, 1]),\n                    torch.LongTensor([1, 1, 0]),\n                    torch.LongTensor([1, 1]),\n                    torch.LongTensor([1, 1, 1]),\n                ],\n            }\n            # fmt: on\n            dummy_dataset = Dataset.from_dict(dummy_dataset_dict)\n\n            trainer = RewardTrainer(\n                model=self.model,\n                args=training_args,\n                tokenizer=self.tokenizer,\n                train_dataset=dummy_dataset,\n                eval_dataset=dummy_dataset,\n                peft_config=peft_config,\n            )\n            previous_trainable_params = {}\n            previous_non_trainable_params = {}\n\n            # due to a change in the way the modules to save are dealt in PEFT.\n            trainable_params_name = [\"lora\", \"score\"] if peft_version < \"0.3.0\" else [\"lora\", \"modules_to_save\"]\n\n            # check gradients are not None\n            for n, param in trainer.model.named_parameters():\n                if any([t in n for t in trainable_params_name]):\n                    previous_trainable_params[n] = param.clone()\n                else:\n                    previous_non_trainable_params[n] = param.clone()\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n\n            # check the params have changed\n            for n, param in previous_trainable_params.items():\n                new_param = trainer.model.get_parameter(n)\n                self.assertFalse(torch.allclose(param, new_param, atol=1e-12, rtol=1e-12))\n\n            # check the non trainable params have not changed\n            for n, param in previous_non_trainable_params.items():\n                new_param = trainer.model.get_parameter(n)\n                self.assertTrue(torch.allclose(param, new_param, atol=1e-12, rtol=1e-12))\n\n            preds = trainer.predict(dummy_dataset)\n            self.assertEqual(preds.predictions.shape, (4, 2))\n\n    def test_reward_trainer_assert_value_error(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = RewardConfig(\n                output_dir=tmp_dir,\n                per_device_train_batch_size=2,\n                max_steps=1,\n                remove_unused_columns=False,\n            )\n\n            dummy_dataset_dict = {\n                # fmt: off\n                \"input_ids_b\": [\n                    torch.LongTensor([0, 1, 2,]),\n                    torch.LongTensor([1, 2]),\n                    torch.LongTensor([0, 1, 2,]),\n                    torch.LongTensor([1, 2]),\n                ],\n                \"attention_mask_c\": [\n                    torch.LongTensor([1, 1, 1]),\n                    torch.LongTensor([1, 0]),\n                    torch.LongTensor([1, 1, 1]),\n                    torch.LongTensor([1, 0]),\n                ],\n                \"input_ids_f\": [\n                    torch.LongTensor([0, 2,]),\n                    torch.LongTensor([1, 2, 0]),\n                    torch.LongTensor([0, 2,]),\n                    torch.LongTensor([1, 2, 0]),\n                ],\n                \"attention_mask_g\": [\n                    torch.LongTensor([1, 1]),\n                    torch.LongTensor([1, 1, 0]),\n                    torch.LongTensor([1, 1]),\n                    torch.LongTensor([1, 1, 1]),\n                ],\n                # fmt: on\n            }\n            dummy_dataset = Dataset.from_dict(dummy_dataset_dict)\n\n            trainer = RewardTrainer(\n                model=self.model,\n                args=training_args,\n                tokenizer=self.tokenizer,\n                train_dataset=dummy_dataset,\n            )\n\n            with self.assertRaises(ValueError):\n                trainer.train()\n\n            training_args = RewardConfig(\n                output_dir=tmp_dir,\n                per_device_train_batch_size=2,\n                max_steps=1,\n                remove_unused_columns=True,\n            )\n\n            with self.assertWarns(UserWarning):\n                trainer = RewardTrainer(\n                    model=self.model,\n                    args=training_args,\n                    tokenizer=self.tokenizer,\n                    train_dataset=dummy_dataset,\n                )\n\n    def test_reward_trainer_margin(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = RewardConfig(\n                output_dir=tmp_dir,\n                per_device_train_batch_size=2,\n                max_steps=3,\n                remove_unused_columns=False,\n                gradient_accumulation_steps=4,\n                learning_rate=9e-1,\n                evaluation_strategy=\"steps\",\n            )\n\n            # fmt: off\n            dummy_dataset_dict = {\n                \"input_ids_chosen\": [\n                    torch.LongTensor([0, 1, 2,]),\n                ],\n                \"attention_mask_chosen\": [\n                    torch.LongTensor([1, 1, 1]),\n                ],\n                \"input_ids_rejected\": [\n                    torch.LongTensor([0, 2,]),\n                ],\n                \"attention_mask_rejected\": [\n                    torch.LongTensor([1, 1]),\n                ],\n                \"margin\": [\n                    torch.FloatTensor([1.0]),\n                ]\n            }\n            # fmt: on\n            dummy_dataset = Dataset.from_dict(dummy_dataset_dict)\n\n            trainer = RewardTrainer(\n                model=self.model,\n                args=training_args,\n                tokenizer=self.tokenizer,\n                train_dataset=dummy_dataset,\n                eval_dataset=dummy_dataset,\n            )\n\n            batch = [dummy_dataset[0]]\n            batch = trainer.data_collator(batch)\n            loss, outputs = trainer.compute_loss(trainer.model, batch, return_outputs=True)\n\n            self.assertAlmostEqual(\n                loss,\n                -torch.nn.functional.logsigmoid(\n                    outputs[\"rewards_chosen\"] - outputs[\"rewards_rejected\"] - batch[\"margin\"]\n                ).mean(),\n            )\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_environments.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import unittest\nfrom unittest.mock import patch\n\nimport torch\nfrom transformers import AutoTokenizer\n\nfrom trl import AutoModelForCausalLMWithValueHead, TextEnvironment, TextHistory\n\n\nclass DummyTool:\n    def __call__(self, text):\n        return text\n\n\ndef dummy_generate(histories):\n    for i in range(len(histories)):\n        histories[i].append_segment(\"<request><DummyTool>test<call>\", torch.tensor([1, 2, 3]), system=False)\n    return histories\n\n\nclass TextHistoryTest(unittest.TestCase):\n    def test_text_history_init(self):\n        text = \"Hello there!\"\n        tokens = torch.tensor([1, 2, 3])\n\n        history = TextHistory(text, tokens)\n        self.assertEqual(history.text, text)\n        self.assertTrue(torch.equal(history.tokens, tokens))\n        self.assertTrue(torch.equal(history.token_masks, torch.zeros_like(tokens)))\n\n        history = TextHistory(text, tokens, system=False)\n        self.assertTrue(torch.equal(history.token_masks, torch.ones_like(tokens)))\n\n    def test_text_history_append_segment(self):\n        text = \"Hello there!\"\n        tokens = torch.tensor([1, 2, 3])\n\n        history = TextHistory(text, tokens)\n        history.append_segment(\"General Kenobi!\", torch.tensor([4, 5, 6]), system=False)\n        self.assertEqual(history.text, text + \"General Kenobi!\")\n        self.assertTrue(torch.equal(history.tokens, torch.tensor([1, 2, 3, 4, 5, 6])))\n        self.assertTrue(torch.equal(history.token_masks, torch.tensor([0, 0, 0, 1, 1, 1])))\n\n        history.append_segment(\"You are a bold one!\", torch.tensor([7, 8, 9]))\n        self.assertEqual(history.text, text + \"General Kenobi!\" + \"You are a bold one!\")\n        self.assertTrue(torch.equal(history.tokens, torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])))\n        self.assertTrue(torch.equal(history.token_masks, torch.tensor([0, 0, 0, 1, 1, 1, 0, 0, 0])))\n\n    def test_text_history_complete(self):\n        text = \"Hello there!\"\n        tokens = torch.tensor([1, 2, 3])\n        history = TextHistory(text, tokens)\n        history.complete()\n        self.assertTrue(history.completed)\n        self.assertFalse(history.truncated)\n\n        history.complete(truncated=True)\n        self.assertTrue(history.completed)\n        self.assertTrue(history.truncated)\n\n    def test_text_history_last_segment(self):\n        text = \"Hello there!\"\n        tokens = torch.tensor([1, 2, 3])\n        history = TextHistory(text, tokens)\n        history.append_segment(\"General Kenobi!\", torch.tensor([4, 5, 6]))\n        history.append_segment(\"You are a bold one!\", torch.tensor([7, 8, 9]))\n        self.assertEqual(history.last_text_segment, \"You are a bold one!\")\n\n    def test_text_history_split_query_response(self):\n        text = \"Hello there!\"\n        tokens = torch.tensor([1, 2, 3])\n        history = TextHistory(text, tokens)\n        history.append_segment(\"General Kenobi!\", torch.tensor([4, 5, 6]), system=False)\n        history.append_segment(\"You are a bold one!\", torch.tensor([7, 8, 9]), system=True)\n        query, response, mask = history.split_query_response_tokens()\n\n        self.assertTrue(torch.equal(query, torch.tensor([1, 2, 3])))\n        self.assertTrue(torch.equal(response, torch.tensor([4, 5, 6, 7, 8, 9])))\n        self.assertTrue(torch.equal(mask, torch.tensor([1, 1, 1, 0, 0, 0])))\n\n\nclass TextEnvironmentTester(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # model_id\n        cls.model_id = \"trl-internal-testing/dummy-GPT2-correct-vocab\"\n\n        # get models and tokenizer\n        cls.gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(cls.model_id)\n        cls.gpt2_tokenizer = AutoTokenizer.from_pretrained(cls.model_id)\n        cls.gpt2_tokenizer.pad_token = cls.gpt2_tokenizer.eos_token\n\n    def test_text_environment_setup(self):\n        env = TextEnvironment(\n            self.gpt2_model,\n            self.gpt2_tokenizer,\n            tools=[DummyTool()],\n            reward_fn=lambda x: torch.tensor(1),\n            prompt=\"I am a prompt!\\n\",\n        )\n        self.assertEqual(env.prompt, \"I am a prompt!\\n\")\n        self.assertEqual(list(env.tools.keys()), [\"DummyTool\"])\n        self.assertTrue(isinstance(env.tools[\"DummyTool\"], DummyTool))\n        self.assertEqual(env.reward_fn(\"Hello there!\"), 1)\n\n    def test_text_environment_generate(self):\n        generation_kwargs = {\"do_sample\": False, \"max_new_tokens\": 4, \"pad_token_id\": self.gpt2_tokenizer.eos_token_id}\n        env = TextEnvironment(\n            self.gpt2_model,\n            self.gpt2_tokenizer,\n            tools=[DummyTool()],\n            reward_fn=lambda x: torch.tensor(1),\n            prompt=\"I am a prompt!\\n\",\n            generation_kwargs=generation_kwargs,\n        )\n\n        input_texts = [\"this is a test\", \"this is another, longer test\"]\n\n        model_inputs = [self.gpt2_tokenizer(txt, return_tensors=\"pt\").input_ids.squeeze() for txt in input_texts]\n\n        generations_batched = env._generate_batched(model_inputs, batch_size=2)\n        generations_batched = self.gpt2_tokenizer.batch_decode(generations_batched)\n\n        generations_single = [env._generate_batched([inputs], batch_size=1)[0] for inputs in model_inputs]\n        generations_single = self.gpt2_tokenizer.batch_decode(generations_single)\n\n        self.assertEqual(generations_single, generations_batched)\n\n    def test_text_environment_tool_call_parsing(self):\n        string_valid = \"Something something <request><Tool1>Hello there!<call>\"\n        string_invalid_request = \"Something something <Tool1>Hello there!<call>\"\n        string_invalid_call = \"Something something <request><Tool1>Hello there!\"\n        string_invalid_tool = \"Something something <request>|Tool2|Hello there!<call>\"\n        string_invalid_random = \"<>abcdefghijklm<>nopqrstuvwxyz<>\"\n\n        env = TextEnvironment(\n            self.gpt2_model,\n            self.gpt2_tokenizer,\n            tools=[DummyTool()],\n            reward_fn=lambda x: torch.tensor(1),\n            prompt=\"I am a prompt!\\n\",\n        )\n        tool, response = env.parse_tool_call(string_valid)\n        self.assertEqual(tool, \"Tool1\")\n        self.assertEqual(response, \"Hello there!\")\n\n        tool, response = env.parse_tool_call(string_invalid_request)\n        self.assertEqual(tool, None)\n        self.assertEqual(response, None)\n\n        tool, response = env.parse_tool_call(string_invalid_call)\n        self.assertEqual(tool, None)\n        self.assertEqual(response, None)\n\n        tool, response = env.parse_tool_call(string_invalid_tool)\n        self.assertEqual(tool, None)\n        self.assertEqual(response, None)\n\n        tool, response = env.parse_tool_call(string_invalid_random)\n        self.assertEqual(tool, None)\n        self.assertEqual(response, None)\n\n    def test_text_environment_tool_truncation(self):\n        env = TextEnvironment(\n            self.gpt2_model,\n            self.gpt2_tokenizer,\n            tools={\"dummy\": lambda x: \"a\" * 1000},\n            reward_fn=lambda x: torch.tensor(1),\n            prompt=\"I am a prompt!\\n\",\n        )\n\n        env.max_tool_response = 100\n        history = env.step(TextHistory(\"<request><dummy>Hello there!<call>\", torch.tensor([1, 2, 3])))\n        self.assertEqual(len(history.last_text_segment) - len(env.response_token), 100)\n\n        env.max_tool_response = 500\n        history = env.step(TextHistory(\"<request><dummy>Hello there!<call>\", torch.tensor([1, 2, 3])))\n        self.assertEqual(len(history.last_text_segment) - len(env.response_token), 500)\n\n        env.max_tool_response = 1001\n        history = env.step(TextHistory(\"<request><dummy>Hello there!<call>\", torch.tensor([1, 2, 3])))\n        self.assertEqual(len(history.last_text_segment) - len(env.response_token), 1000)\n\n        env.max_tool_response = 2000\n        history = env.step(TextHistory(\"<request><dummy>Hello there!<call>\", torch.tensor([1, 2, 3])))\n        self.assertEqual(len(history.last_text_segment) - len(env.response_token), 1000)\n\n    @patch.object(TextEnvironment, \"generate\", side_effect=dummy_generate)\n    def test_text_environment_max_calls(self, mock_generate):\n        env = TextEnvironment(\n            self.gpt2_model,\n            self.gpt2_tokenizer,\n            tools={\"DummyTool\": DummyTool()},\n            reward_fn=lambda x: [torch.tensor(1) for _ in x],\n            prompt=\"I am a prompt!\\n\",\n        )\n\n        env.max_turns = 1\n        _, _, _, _, histories = env.run([\"test\"])\n        self.assertEqual(\n            histories[0].text, \"I am a prompt!\\n\" + \"test\" + 1 * \"<request><DummyTool>test<call>test<response>\"\n        )\n\n        env.max_turns = 2\n        _, _, _, _, histories = env.run([\"test\"])\n        self.assertEqual(\n            histories[0].text, \"I am a prompt!\\n\" + \"test\" + 2 * \"<request><DummyTool>test<call>test<response>\"\n        )\n\n        env.max_turns = 4\n        _, _, _, _, histories = env.run([\"test\"])\n        self.assertEqual(\n            histories[0].text, \"I am a prompt!\\n\" + \"test\" + 4 * \"<request><DummyTool>test<call>test<response>\"\n        )\n\n    def test_text_environment_compute_rewards(self):\n        env = TextEnvironment(\n            self.gpt2_model,\n            self.gpt2_tokenizer,\n            tools={\"DummyTool\": DummyTool()},\n            reward_fn=lambda x: [torch.tensor(i) for i, _ in enumerate(x)],\n            prompt=\"I am a prompt!\\n\",\n        )\n\n        histories = [TextHistory(\"<request><DummyTool>test<call>\", torch.tensor([1, 2, 3])) for _ in range(8)]\n        histories = env.compute_reward(histories)\n\n        for i in range(8):\n            self.assertEqual(histories[i].reward, i)\n\n    @patch.object(TextEnvironment, \"generate\", side_effect=dummy_generate)\n    def test_text_environment_run(self, mock_generate):\n        env = TextEnvironment(\n            self.gpt2_model,\n            self.gpt2_tokenizer,\n            tools={\"DummyTool\": DummyTool()},\n            reward_fn=lambda x: [torch.tensor(i) for i, _ in enumerate(x)],\n            prompt=\"I am a prompt!\\n\",\n            max_turns=2,\n        )\n        task_1 = \"Hello there!\"\n        task_2 = \"Hello there! General Kenobi!\"\n\n        query, response, response_mask, reward, histories = env.run([task_1, task_2])\n        self.assertEqual(len(query[0]), 9)\n        self.assertEqual(len(query[1]), 12)\n        self.assertEqual(len(response[0]), 14)\n        self.assertEqual(len(response[1]), 14)\n        self.assertEqual(response_mask[0].sum(), 2 * 3)  # mocked generate always adds 3 toknes\n        self.assertEqual(response_mask[1].sum(), 2 * 3)  # mocked generate always adds 3 toknes\n        self.assertEqual(reward[0], 0)\n        self.assertEqual(reward[1], 1)\n        self.assertEqual(\n            histories[0].text, \"I am a prompt!\\n\" + \"Hello there!\" + 2 * \"<request><DummyTool>test<call>test<response>\"\n        )\n        self.assertEqual(\n            histories[1].text,\n            \"I am a prompt!\\n\" + \"Hello there! General Kenobi!\" + 2 * \"<request><DummyTool>test<call>test<response>\",\n        )\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_dpo_trainer.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import tempfile\nimport unittest\n\nimport torch\nfrom datasets import Dataset\nfrom parameterized import parameterized\nfrom pytest import mark\nfrom transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments\n\nfrom trl import DPOTrainer\n\nfrom .testing_utils import require_bitsandbytes, require_no_wandb, require_peft\n\n\nclass DPOTrainerTester(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.model_id = \"trl-internal-testing/dummy-GPT2-correct-vocab\"\n        cls.model = AutoModelForCausalLM.from_pretrained(cls.model_id)\n        cls.ref_model = AutoModelForCausalLM.from_pretrained(cls.model_id)\n        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_id)\n        cls.tokenizer.pad_token = cls.tokenizer.eos_token\n\n        # get t5 as seq2seq example:\n        model_id = \"trl-internal-testing/tiny-T5ForConditionalGeneration-correct-vocab\"\n        cls.t5_model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n        cls.t5_ref_model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n        cls.t5_tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n    def _init_dummy_dataset(self):\n        # fmt: off\n        dummy_dataset_dict = {\n            \"prompt\": [\n                \"hello\",\n                \"how are you\",\n                \"What is your name?\",\n                \"What is your name?\",\n                \"Which is the best programming language?\",\n                \"Which is the best programming language?\",\n                \"Which is the best programming language?\",\n                \"[INST] How is the stock price? [/INST]\",\n                \"[INST] How is the stock price? [/INST] \",\n            ],\n            \"chosen\": [\n                \"hi nice to meet you\",\n                \"I am fine\",\n                \"My name is Mary\",\n                \"My name is Mary\",\n                \"Python\",\n                \"Python\",\n                \"Python\",\n                \"$46 as of 10am EST\",\n                \"46 as of 10am EST\",\n            ],\n            \"rejected\": [\n                \"leave me alone\",\n                \"I am not fine\",\n                \"Whats it to you?\",\n                \"I dont have a name\",\n                \"Javascript\",\n                \"C++\",\n                \"Java\",\n                \" $46 as of 10am EST\",\n                \" 46 as of 10am EST\",\n            ],\n        }\n        # fmt: on\n        return Dataset.from_dict(dummy_dataset_dict)\n\n    @parameterized.expand(\n        [\n            [\"gpt2\", \"sigmoid\", True],\n            [\"t5\", \"hinge\", False],\n            [\"gpt2\", \"ipo\", False],\n            [\"t5\", \"ipo\", True],\n            [\"gpt2\", \"kto_pair\", True],\n            [\"t5\", \"kto_pair\", False],\n        ]\n    )\n    def test_dpo_trainer(self, name, loss_type, pre_compute):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                per_device_train_batch_size=2,\n                max_steps=3,\n                remove_unused_columns=False,\n                gradient_accumulation_steps=1,\n                learning_rate=9e-1,\n                evaluation_strategy=\"steps\",\n            )\n\n            dummy_dataset = self._init_dummy_dataset()\n\n            if name == \"gpt2\":\n                model = self.model\n                ref_model = self.ref_model\n                tokenizer = self.tokenizer\n            elif name == \"t5\":\n                model = self.t5_model\n                ref_model = self.t5_ref_model\n                tokenizer = self.t5_tokenizer\n\n            trainer = DPOTrainer(\n                model=model,\n                ref_model=ref_model,\n                beta=0.1,\n                loss_type=loss_type,\n                args=training_args,\n                tokenizer=tokenizer,\n                train_dataset=dummy_dataset,\n                eval_dataset=dummy_dataset,\n                precompute_ref_log_probs=pre_compute,\n            )\n\n            previous_trainable_params = {n: param.clone() for n, param in trainer.model.named_parameters()}\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n\n            # check the params have changed\n            for n, param in previous_trainable_params.items():\n                new_param = trainer.model.get_parameter(n)\n                # check the params have changed - ignore 0 biases\n                if param.sum() != 0:\n                    self.assertFalse(torch.equal(param, new_param))\n\n    def test_dpo_trainer_without_providing_ref_model(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                per_device_train_batch_size=2,\n                max_steps=3,\n                remove_unused_columns=False,\n                gradient_accumulation_steps=4,\n                learning_rate=9e-1,\n                evaluation_strategy=\"steps\",\n            )\n\n            dummy_dataset = self._init_dummy_dataset()\n\n            trainer = DPOTrainer(\n                model=self.model,\n                ref_model=None,\n                beta=0.1,\n                args=training_args,\n                tokenizer=self.tokenizer,\n                train_dataset=dummy_dataset,\n                eval_dataset=dummy_dataset,\n                precompute_ref_log_probs=True,\n            )\n\n            previous_trainable_params = {n: param.clone() for n, param in trainer.model.named_parameters()}\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n\n            # check the params have changed\n            for n, param in previous_trainable_params.items():\n                new_param = trainer.model.get_parameter(n)\n                # check the params have changed - ignore 0 biases\n                if param.sum() != 0:\n                    self.assertFalse(torch.equal(param, new_param))\n\n    @require_peft\n    @mark.peft_test\n    def test_dpo_trainer_without_providing_ref_model_with_lora(self):\n        from peft import LoraConfig\n\n        lora_config = LoraConfig(\n            r=16,\n            lora_alpha=32,\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                per_device_train_batch_size=2,\n                max_steps=3,\n                remove_unused_columns=False,\n                gradient_accumulation_steps=4,\n                learning_rate=9e-1,\n                evaluation_strategy=\"steps\",\n            )\n\n            dummy_dataset = self._init_dummy_dataset()\n\n            trainer = DPOTrainer(\n                model=self.model,\n                ref_model=None,\n                beta=0.1,\n                args=training_args,\n                tokenizer=self.tokenizer,\n                train_dataset=dummy_dataset,\n                eval_dataset=dummy_dataset,\n                peft_config=lora_config,\n                precompute_ref_log_probs=True,\n            )\n\n            previous_trainable_params = {n: param.clone() for n, param in trainer.model.named_parameters()}\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n\n            # check the params have changed\n            for n, param in previous_trainable_params.items():\n                if \"lora\" in n:\n                    new_param = trainer.model.get_parameter(n)\n                    # check the params have changed - ignore 0 biases\n                    if param.sum() != 0:\n                        self.assertFalse(torch.equal(param, new_param))\n\n    @require_no_wandb\n    def test_dpo_trainer_generate_during_eval_no_wandb(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                per_device_train_batch_size=2,\n                max_steps=3,\n                remove_unused_columns=False,\n                gradient_accumulation_steps=1,\n                learning_rate=9e-1,\n                evaluation_strategy=\"steps\",\n            )\n\n            dummy_dataset = self._init_dummy_dataset()\n\n            with self.assertRaisesRegex(\n                ValueError,\n                expected_regex=\"`generate_during_eval=True` requires Weights and Biases to be installed.\"\n                \" Please install `wandb` to resolve.\",\n            ):\n                DPOTrainer(\n                    model=self.model,\n                    ref_model=None,\n                    beta=0.1,\n                    args=training_args,\n                    tokenizer=self.tokenizer,\n                    train_dataset=dummy_dataset,\n                    eval_dataset=dummy_dataset,\n                    generate_during_eval=True,\n                )\n\n    @require_peft\n    @mark.peft_test\n    def test_dpo_lora_save(self):\n        from peft import LoraConfig, get_peft_model\n\n        lora_config = LoraConfig(\n            r=16,\n            lora_alpha=32,\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n\n        # lora model\n        model = AutoModelForCausalLM.from_pretrained(self.model_id)\n        model_peft = get_peft_model(model, lora_config)\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                per_device_train_batch_size=2,\n                max_steps=3,\n                remove_unused_columns=False,\n                gradient_accumulation_steps=4,\n                learning_rate=9e-1,\n                evaluation_strategy=\"steps\",\n            )\n\n            dummy_dataset = self._init_dummy_dataset()\n\n            # dpo train lora model with a lora config\n            trainer = DPOTrainer(\n                model=model_peft,\n                ref_model=None,\n                beta=0.1,\n                args=training_args,\n                tokenizer=self.tokenizer,\n                train_dataset=dummy_dataset,\n                eval_dataset=dummy_dataset,\n                peft_config=lora_config,\n                precompute_ref_log_probs=True,\n            )\n\n            # train the model\n            trainer.train()\n\n            # save peft adapter\n            trainer.save_model()\n\n            # assert that the model is loaded without giving OSError\n            try:\n                AutoModelForCausalLM.from_pretrained(tmp_dir)\n            except OSError:\n                self.fail(\"Loading the saved peft adapter failed\")\n\n    @require_peft\n    @require_bitsandbytes\n    @mark.peft_test\n    def test_dpo_lora_bf16_autocast_llama(self):\n        # Note this test only works on compute capability > 7 GPU devices\n        from peft import LoraConfig\n\n        model_id = \"HuggingFaceM4/tiny-random-LlamaForCausalLM\"\n        tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n        lora_config = LoraConfig(\n            r=16,\n            lora_alpha=32,\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n\n        # lora model\n        model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                per_device_train_batch_size=2,\n                max_steps=3,\n                remove_unused_columns=False,\n                gradient_accumulation_steps=4,\n                learning_rate=9e-1,\n                evaluation_strategy=\"steps\",\n                bf16=True,\n            )\n\n            dummy_dataset = self._init_dummy_dataset()\n\n            # dpo train lora model with a lora config\n            trainer = DPOTrainer(\n                model=model,\n                ref_model=None,\n                beta=0.1,\n                args=training_args,\n                tokenizer=tokenizer,\n                train_dataset=dummy_dataset,\n                eval_dataset=dummy_dataset,\n                peft_config=lora_config,\n                generate_during_eval=True,\n            )\n\n            # train the model\n            trainer.train()\n\n            # save peft adapter\n            trainer.save_model()\n\n    @parameterized.expand(\n        [\n            [\"gpt2\", \"sigmoid\", False, False],\n            [\"gpt2\", \"sigmoid\", False, True],\n            [\"gpt2\", \"sigmoid\", True, False],\n            [\"gpt2\", \"sigmoid\", True, True],\n            [\"gpt2\", \"ipo\", False, False],\n            [\"gpt2\", \"ipo\", False, True],\n            [\"gpt2\", \"ipo\", True, False],\n            [\"gpt2\", \"ipo\", True, True],\n            [\"gpt2\", \"kto_pair\", False, False],\n            [\"gpt2\", \"kto_pair\", False, True],\n            [\"gpt2\", \"kto_pair\", True, False],\n            [\"gpt2\", \"kto_pair\", True, True],\n        ]\n    )\n    @require_bitsandbytes\n    @require_peft\n    @mark.peft_test\n    def test_dpo_lora_bf16_autocast(self, name, loss_type, pre_compute, gen_during_eval):\n        # Note this test only works on compute capability > 7 GPU devices\n        from peft import LoraConfig\n\n        lora_config = LoraConfig(\n            r=16,\n            lora_alpha=32,\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n\n        # lora model\n        model = AutoModelForCausalLM.from_pretrained(self.model_id, load_in_4bit=True)\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                per_device_train_batch_size=2,\n                max_steps=3,\n                remove_unused_columns=False,\n                gradient_accumulation_steps=4,\n                learning_rate=9e-1,\n                evaluation_strategy=\"steps\",\n                bf16=True,\n            )\n\n            dummy_dataset = self._init_dummy_dataset()\n\n            # dpo train lora model with a lora config\n            trainer = DPOTrainer(\n                model=model,\n                ref_model=None,\n                beta=0.1,\n                args=training_args,\n                tokenizer=self.tokenizer,\n                train_dataset=dummy_dataset,\n                eval_dataset=dummy_dataset,\n                peft_config=lora_config,\n                generate_during_eval=gen_during_eval,\n                loss_type=loss_type,\n                precompute_ref_log_probs=pre_compute,\n            )\n\n            # train the model\n            trainer.train()\n\n            # save peft adapter\n            trainer.save_model()\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_sft_trainer.py",
        "recommendation": "The code file is too long to analyze. Please select a shorter file.",
        "code_snippet": "import copy\nimport os\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n\nfrom trl import SFTTrainer\nfrom trl.import_utils import is_peft_available\nfrom trl.trainer import ConstantLengthDataset, DataCollatorForCompletionOnlyLM\n\nfrom .testing_utils import require_peft\n\n\ndef formatting_prompts_func(example):\n    text = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n    return text\n\n\ndef formatting_prompts_func_batched(example):\n    output_text = []\n    for i, question in enumerate(example[\"question\"]):\n        text = f\"### Question: {question}\\n ### Answer: {example['answer'][i]}\"\n        output_text.append(text)\n    return output_text\n\n\nif is_peft_available():\n    from peft import LoraConfig, PeftModel\n\n\nclass SFTTrainerTester(unittest.TestCase):\n    r\"\"\" \"\"\"\n\n    @classmethod\n    def setUpClass(cls):\n        cls.model_id = \"trl-internal-testing/dummy-GPT2-correct-vocab\"\n        cls.model = AutoModelForCausalLM.from_pretrained(cls.model_id)\n        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_id)\n        cls.tokenizer.pad_token = cls.tokenizer.eos_token\n        cls.dummy_dataset = Dataset.from_dict(\n            {\n                \"question\": [\n                    \"Does llamas know how to code?\",\n                    \"Does llamas know how to fly?\",\n                    \"Does llamas know how to talk?\",\n                    \"Does llamas know how to code?\",\n                    \"Does llamas know how to fly?\",\n                    \"Does llamas know how to talk?\",\n                    \"Does llamas know how to swim?\",\n                ],\n                \"answer\": [\n                    \"Yes, llamas are very good at coding.\",\n                    \"No, llamas can't fly.\",\n                    \"Yes, llamas are very good at talking.\",\n                    \"Yes, llamas are very good at coding.\",\n                    \"No, llamas can't fly.\",\n                    \"Yes, llamas are very good at talking.\",\n                    \"No, llamas can't swim.\",\n                ],\n                \"text\": [\n                    \"### Question: Does llamas know how to code?\\n ### Answer: Yes, llamas are very good at coding.\",\n                    \"### Question: Does llamas know how to fly?\\n ### Answer: No, llamas can't fly.\",\n                    \"### Question: Does llamas know how to talk?\\n ### Answer: Yes, llamas are very good at talking.\",\n                    \"### Question: Does llamas know how to code?\\n ### Answer: Yes, llamas are very good at coding.\",\n                    \"### Question: Does llamas know how to fly?\\n ### Answer: No, llamas can't fly.\",\n                    \"### Question: Does llamas know how to talk?\\n ### Answer: Yes, llamas are very good at talking.\",\n                    \"### Question: Does llamas know how to swim?\\n ### Answer: No, llamas can't swim.\",\n                ],\n            }\n        )\n        cls.dummy_chatml_dataset = Dataset.from_dict(\n            {\n                \"messages\": [\n                    [\n                        {\"role\": \"system\", \"content\": \"You are helpful\"},\n                        {\"role\": \"user\", \"content\": \"Hello\"},\n                        {\"role\": \"assistant\", \"content\": \"Hi, how can I help you?\"},\n                        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n                        {\"role\": \"assistant\", \"content\": \"4\"},\n                        {\"role\": \"user\", \"content\": \"What is 3+3?\"},\n                        {\"role\": \"assistant\", \"content\": \"6\"},\n                    ],\n                    [\n                        {\"role\": \"system\", \"content\": \"You are helpful\"},\n                        {\"role\": \"user\", \"content\": \"Hello\"},\n                        {\"role\": \"assistant\", \"content\": \"Hi, how can I help you?\"},\n                    ],\n                ]\n            }\n        )\n        cls.dummy_instruction_dataset = Dataset.from_list(\n            [\n                {\"prompt\": \"What is 2+2?\", \"completion\": \"4\"},\n                {\"prompt\": \"What is 3+3?\", \"completion\": \"6\"},\n                {\"prompt\": \"What is 4+4?\", \"completion\": \"8\"},\n                {\"prompt\": \"What is 2+2?\", \"completion\": \"4\"},\n                {\"prompt\": \"What is 3+3?\", \"completion\": \"6\"},\n                {\"prompt\": \"What is 4+4?\", \"completion\": \"8\"},\n                {\"prompt\": \"What is 2+2?\", \"completion\": \"4\"},\n                {\"prompt\": \"What is 3+3?\", \"completion\": \"6\"},\n                {\"prompt\": \"What is 4+4?\", \"completion\": \"8\"},\n                {\"prompt\": \"What is 2+2?\", \"completion\": \"4\"},\n                {\"prompt\": \"What is 3+3?\", \"completion\": \"6\"},\n                {\"prompt\": \"What is 4+4?\", \"completion\": \"8\"},\n            ]\n        )\n\n        cls.train_dataset = ConstantLengthDataset(\n            cls.tokenizer,\n            cls.dummy_dataset,\n            dataset_text_field=None,\n            formatting_func=formatting_prompts_func,\n            seq_length=16,\n            num_of_sequences=16,\n        )\n\n        cls.eval_dataset = ConstantLengthDataset(\n            cls.tokenizer,\n            cls.dummy_dataset,\n            dataset_text_field=None,\n            formatting_func=formatting_prompts_func,\n            seq_length=16,\n            num_of_sequences=16,\n        )\n\n    def test_constant_length_dataset(self):\n        formatted_dataset = ConstantLengthDataset(\n            self.tokenizer,\n            self.dummy_dataset,\n            dataset_text_field=None,\n            formatting_func=formatting_prompts_func,\n        )\n\n        self.assertTrue(len(formatted_dataset) == len(self.dummy_dataset))\n        self.assertTrue(len(formatted_dataset) > 0)\n\n        for example in formatted_dataset:\n            self.assertTrue(\"input_ids\" in example)\n            self.assertTrue(\"labels\" in example)\n\n            self.assertTrue(len(example[\"input_ids\"]) == formatted_dataset.seq_length)\n            self.assertTrue(len(example[\"labels\"]) == formatted_dataset.seq_length)\n\n            decoded_text = self.tokenizer.decode(example[\"input_ids\"])\n            self.assertTrue((\"Question\" in decoded_text) and (\"Answer\" in decoded_text))\n\n    def test_sft_trainer(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=4,\n                eval_steps=2,\n                save_steps=2,\n                per_device_train_batch_size=2,\n            )\n\n            trainer = SFTTrainer(\n                model=self.model_id,\n                args=training_args,\n                train_dataset=self.train_dataset,\n                eval_dataset=self.eval_dataset,\n                packing=True,\n            )\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n            self.assertIsNotNone(trainer.state.log_history[0][\"eval_loss\"])\n\n            self.assertTrue(\"model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-2\"))\n\n    def test_sft_trainer_uncorrect_data(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=2,\n                eval_steps=1,\n                save_steps=1,\n                per_device_train_batch_size=2,\n            )\n\n            with self.assertRaises(ValueError):\n                _ = SFTTrainer(\n                    model=self.model,\n                    args=training_args,\n                    train_dataset=self.dummy_dataset,\n                    packing=True,\n                )\n            # this should work since the dummy chatml include the correct format\n            _ = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.dummy_chatml_dataset,\n                max_seq_length=32,  # make sure there is at least 1 packed sequence\n                num_of_sequences=32,\n                packing=True,\n            )\n            _ = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.dummy_chatml_dataset,\n                packing=False,\n            )\n            # this should work since the dummy instruction dataset is the correct format\n            _ = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.dummy_instruction_dataset,\n                max_seq_length=16,  # make sure there is at least 1 packed sequence\n                packing=True,\n            )\n            _ = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.dummy_instruction_dataset,\n                packing=False,\n            )\n            # This should work\n            _ = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.dummy_dataset,\n                formatting_func=formatting_prompts_func,\n                max_seq_length=32,  # make sure there is at least 1 packed sequence\n                packing=True,\n            )\n\n            with self.assertRaises(ValueError):\n                # This should not work because not enough data for one sample\n                _ = SFTTrainer(\n                    model=self.model,\n                    args=training_args,\n                    train_dataset=self.dummy_dataset,\n                    formatting_func=formatting_prompts_func,\n                    max_seq_length=1024,  # make sure there is NOT at least 1 packed sequence\n                    packing=True,\n                )\n\n            # This should not work as well\n            with self.assertRaises(ValueError):\n                _ = SFTTrainer(\n                    model=self.model,\n                    args=training_args,\n                    train_dataset=self.dummy_dataset,\n                    formatting_func=formatting_prompts_func,\n                    packing=False,\n                )\n\n            # but this should work\n            _ = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.dummy_dataset,\n                formatting_func=formatting_prompts_func_batched,\n                packing=False,\n            )\n\n    def test_sft_trainer_with_model_num_train_epochs(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=2,\n                eval_steps=1,\n                save_steps=1,\n                num_train_epochs=2,\n                per_device_train_batch_size=2,\n            )\n\n            trainer = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.train_dataset,\n                eval_dataset=self.eval_dataset,\n                packing=True,\n            )\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n            self.assertIsNotNone(trainer.state.log_history[0][\"eval_loss\"])\n\n            self.assertTrue(\"model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-2\"))\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=2,\n                save_steps=1,\n                num_train_epochs=2,\n                per_device_train_batch_size=2,\n            )\n\n            trainer = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.dummy_dataset,\n                dataset_text_field=\"text\",\n                max_seq_length=16,\n                num_of_sequences=16,\n                packing=True,\n            )\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n\n            self.assertTrue(\"model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-2\"))\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=2,\n                save_steps=1,\n                num_train_epochs=2,\n                per_device_train_batch_size=2,\n            )\n\n            trainer = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.dummy_dataset,\n                dataset_text_field=\"text\",\n                max_seq_length=16,\n            )\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n\n            self.assertTrue(\"model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-1\"))\n\n    def test_sft_trainer_with_model(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=2,\n                eval_steps=1,\n                save_steps=1,\n                per_device_train_batch_size=2,\n            )\n\n            trainer = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.train_dataset,\n                eval_dataset=self.eval_dataset,\n                packing=True,\n            )\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n            self.assertIsNotNone(trainer.state.log_history[0][\"eval_loss\"])\n\n            self.assertTrue(\"model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-2\"))\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=2,\n                save_steps=1,\n                per_device_train_batch_size=2,\n            )\n\n            trainer = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.dummy_dataset,\n                dataset_text_field=\"text\",\n                max_seq_length=16,\n                num_of_sequences=16,\n                packing=True,\n            )\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n\n            self.assertTrue(\"model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-2\"))\n\n        # with formatting_func + packed\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=2,\n                save_steps=1,\n                per_device_train_batch_size=2,\n            )\n\n            trainer = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.dummy_dataset,\n                formatting_func=formatting_prompts_func,\n                max_seq_length=16,\n                num_of_sequences=16,\n                packing=True,\n            )\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n\n            self.assertTrue(\"model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-2\"))\n\n        # with formatting_func + packed\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=2,\n                save_steps=1,\n                per_device_train_batch_size=2,\n            )\n\n            trainer = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.dummy_dataset,\n                formatting_func=formatting_prompts_func_batched,\n                max_seq_length=16,\n            )\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n\n            self.assertTrue(\"model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-2\"))\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=2,\n                save_steps=1,\n                per_device_train_batch_size=2,\n            )\n\n            trainer = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.dummy_dataset,\n                dataset_text_field=\"text\",\n                max_seq_length=16,\n            )\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n\n            self.assertTrue(\"model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-1\"))\n\n    def test_sft_trainer_with_multiple_eval_datasets(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=1,\n                eval_steps=1,\n                save_steps=1,\n                per_device_train_batch_size=2,\n            )\n\n            trainer = SFTTrainer(\n                model=self.model_id,\n                args=training_args,\n                train_dataset=self.train_dataset,\n                eval_dataset={\n                    \"data1\": self.eval_dataset,\n                    \"data2\": self.eval_dataset,\n                },\n                packing=True,\n            )\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n            self.assertIsNotNone(trainer.state.log_history[0][\"eval_data1_loss\"])\n            self.assertIsNotNone(trainer.state.log_history[1][\"eval_data2_loss\"])\n\n            self.assertTrue(\"model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-1\"))\n\n    def test_data_collator_completion_lm(self):\n        response_template = \"### Response:\\n\"\n        data_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=self.tokenizer, mlm=False)\n\n        text = \"\"\"\\n\\n### Instructions:\\nHello all this should be masked\\n\\n### Response:\\nI have not been masked correctly.\"\"\"\n        encoded_text = self.tokenizer(text)\n\n        examples = [encoded_text]\n\n        batch = data_collator(examples)\n        labels = batch[\"labels\"]\n        last_pad_idx = np.where(labels == -100)[1][-1]\n        result_text = self.tokenizer.decode(batch[\"input_ids\"][0, last_pad_idx + 1 :])\n        self.assertEqual(result_text, \"I have not been masked correctly.\")\n\n    def test_data_collator_completion_lm_with_multiple_text(self):\n        tokenizer = copy.deepcopy(self.tokenizer)\n        tokenizer.padding_side = \"left\"\n\n        response_template = \"### Response:\\n\"\n        data_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer, mlm=False)\n\n        text1 = \"\"\"\\n\\n### Instructions:\\nHello all this should be masked\\n\\n### Response:\\nI have not been masked correctly.\"\"\"\n        text2 = \"\"\"\\n\\n### Instructions:\\nThis is another longer text that should also be masked. This text is significantly longer than the previous one.\\n\\n### Response:\\nI have not been masked correctly.\"\"\"\n\n        encoded_text1 = tokenizer(text1)\n        encoded_text2 = tokenizer(text2)\n\n        examples = [encoded_text1, encoded_text2]\n\n        batch = data_collator(examples)\n\n        for i in range(2):\n            labels = batch[\"labels\"][i]\n            last_pad_idx = np.where(labels == -100)[0][-1]\n            result_text = tokenizer.decode(batch[\"input_ids\"][i, last_pad_idx + 1 :])\n            self.assertEqual(result_text, \"I have not been masked correctly.\")\n\n    def test_data_collator_chat_completion_lm(self):\n        instruction_template = \"### Human:\"\n        assistant_template = \"### Assistant:\"\n        data_collator = DataCollatorForCompletionOnlyLM(\n            response_template=assistant_template,\n            instruction_template=instruction_template,\n            tokenizer=self.tokenizer,\n            mlm=False,\n        )\n\n        text = \"\"\"### Human: Hello all this should be masked.### Assistant: I should not be masked.### Human: All this should be masked too.### Assistant: I should not be masked too.\"\"\"\n        encoded_text = self.tokenizer(text)\n\n        examples = [encoded_text]\n\n        batch = data_collator(examples)\n        labels = batch[\"labels\"]\n        non_masked_tokens = batch[\"input_ids\"][labels != -100]\n        result_text = self.tokenizer.decode(non_masked_tokens)\n        self.assertEqual(result_text, \" I should not be masked. I should not be masked too.\")\n\n    def test_data_collator_chat_completion_lm_with_multiple_text(self):\n        tokenizer = copy.deepcopy(self.tokenizer)\n        tokenizer.padding_side = \"left\"\n\n        instruction_template = \"### Human:\"\n        assistant_template = \"### Assistant:\"\n        data_collator = DataCollatorForCompletionOnlyLM(\n            response_template=assistant_template,\n            instruction_template=instruction_template,\n            tokenizer=tokenizer,\n            mlm=False,\n        )\n\n        text1 = \"\"\"### Human: Hello all this should be masked.### Assistant: I should not be masked.\"\"\"\n        text2 = \"\"\"### Human: Hello all this should be masked.### Assistant: I should not be masked.### Human: All this should be masked too.### Assistant: I should not be masked too.\"\"\"\n        encoded_text1 = tokenizer(text1)\n        encoded_text2 = tokenizer(text2)\n\n        examples = [encoded_text1, encoded_text2]\n\n        batch = data_collator(examples)\n        labels = batch[\"labels\"]\n        input_ids = batch[\"input_ids\"]\n\n        non_masked_tokens1 = input_ids[0][labels[0] != -100]\n        result_text1 = tokenizer.decode(non_masked_tokens1)\n        self.assertEqual(result_text1, \" I should not be masked.\")\n\n        non_masked_tokens2 = input_ids[1][labels[1] != -100]\n        result_text2 = tokenizer.decode(non_masked_tokens2)\n        self.assertEqual(result_text2, \" I should not be masked. I should not be masked too.\")\n\n    def test_sft_trainer_infinite_with_model(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=5,\n                eval_steps=1,\n                save_steps=1,\n                per_device_train_batch_size=2,\n            )\n\n            trainer = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.train_dataset,\n                eval_dataset=self.eval_dataset,\n                packing=True,\n                max_seq_length=500,\n            )\n\n            self.assertTrue(trainer.train_dataset.infinite)\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n            self.assertIsNotNone(trainer.state.log_history[0][\"eval_loss\"])\n\n            # make sure the trainer did 5 steps\n            self.assertTrue(\"model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-5\"))\n\n    def test_sft_trainer_infinite_with_model_epochs(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                num_train_epochs=1,\n                per_device_train_batch_size=2,\n                save_strategy=\"epoch\",\n            )\n\n            trainer = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.train_dataset,\n                eval_dataset=self.eval_dataset,\n                packing=True,\n                max_seq_length=500,\n            )\n\n            self.assertFalse(trainer.train_dataset.infinite)\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n\n            # make sure the trainer did 5 steps\n            self.assertTrue(\"model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-4\"))\n\n    def test_sft_trainer_with_model_neftune(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=2,\n                eval_steps=1,\n                save_steps=1,\n                per_device_train_batch_size=2,\n            )\n\n            trainer = SFTTrainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=self.train_dataset,\n                eval_dataset=self.eval_dataset,\n                neftune_noise_alpha=5,\n                packing=True,\n            )\n\n            trainer.model = trainer._trl_activate_neftune(trainer.model)\n\n            device = trainer.model.get_input_embeddings().weight.device\n            trainer.model.train()\n\n            torch.random.manual_seed(42)\n            embeds_neftune = trainer.model.get_input_embeddings()(torch.LongTensor([[1, 0, 1]]).to(device))\n\n            torch.random.manual_seed(24)\n            embeds_neftune_2 = trainer.model.get_input_embeddings()(torch.LongTensor([[1, 0, 1]]).to(device))\n\n            self.assertFalse(torch.allclose(embeds_neftune, embeds_neftune_2))\n            self.assertTrue(len(trainer.model.get_input_embeddings()._forward_hooks) > 0)\n\n            trainer.neftune_hook_handle.remove()\n\n            trainer.train()\n\n            # Make sure forward pass works fine\n            _ = trainer.model(torch.LongTensor([[1, 0, 1]]).to(device))\n            self.assertTrue(len(trainer.model.get_input_embeddings()._forward_hooks) == 0)\n\n    @require_peft\n    def test_peft_sft_trainer(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=4,\n                eval_steps=2,\n                save_steps=2,\n                per_device_train_batch_size=2,\n            )\n\n            peft_config = LoraConfig(\n                r=16,\n                lora_alpha=32,\n                lora_dropout=0.05,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n\n            trainer = SFTTrainer(\n                model=self.model_id,\n                args=training_args,\n                train_dataset=self.train_dataset,\n                eval_dataset=self.eval_dataset,\n                peft_config=peft_config,\n                packing=True,\n            )\n\n            self.assertTrue(isinstance(trainer.model, PeftModel))\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n            self.assertIsNotNone(trainer.state.log_history[0][\"eval_loss\"])\n\n            self.assertTrue(\"adapter_model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-2\"))\n            self.assertTrue(\"adapter_config.json\" in os.listdir(tmp_dir + \"/checkpoint-2\"))\n            self.assertTrue(\"model.safetensors\" not in os.listdir(tmp_dir + \"/checkpoint-2\"))\n\n    @require_peft\n    def test_peft_sft_trainer_gc(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=4,\n                eval_steps=2,\n                save_steps=2,\n                per_device_train_batch_size=2,\n                gradient_checkpointing=True,\n            )\n\n            peft_config = LoraConfig(\n                r=16,\n                lora_alpha=32,\n                lora_dropout=0.05,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n\n            trainer = SFTTrainer(\n                model=self.model_id,\n                args=training_args,\n                train_dataset=self.train_dataset,\n                eval_dataset=self.eval_dataset,\n                peft_config=peft_config,\n                packing=True,\n            )\n\n            self.assertTrue(isinstance(trainer.model, PeftModel))\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n            self.assertIsNotNone(trainer.state.log_history[0][\"eval_loss\"])\n\n            self.assertTrue(\"adapter_model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-2\"))\n            self.assertTrue(\"adapter_config.json\" in os.listdir(tmp_dir + \"/checkpoint-2\"))\n            self.assertTrue(\"model.safetensors\" not in os.listdir(tmp_dir + \"/checkpoint-2\"))\n\n    @require_peft\n    def test_peft_sft_trainer_neftune(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            training_args = TrainingArguments(\n                output_dir=tmp_dir,\n                dataloader_drop_last=True,\n                evaluation_strategy=\"steps\",\n                max_steps=4,\n                eval_steps=2,\n                save_steps=2,\n                per_device_train_batch_size=2,\n            )\n\n            peft_config = LoraConfig(\n                r=16,\n                lora_alpha=32,\n                lora_dropout=0.05,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n\n            trainer = SFTTrainer(\n                model=self.model_id,\n                args=training_args,\n                train_dataset=self.train_dataset,\n                eval_dataset=self.eval_dataset,\n                peft_config=peft_config,\n                neftune_noise_alpha=5,\n                packing=True,\n            )\n\n            trainer.model = trainer._trl_activate_neftune(trainer.model)\n\n            self.assertTrue(isinstance(trainer.model, PeftModel))\n\n            device = trainer.model.get_input_embeddings().weight.device\n            trainer.model.train()\n\n            torch.random.manual_seed(42)\n            embeds_neftune = trainer.model.get_input_embeddings()(torch.LongTensor([[1, 0, 1]]).to(device))\n\n            torch.random.manual_seed(24)\n            embeds_neftune_2 = trainer.model.get_input_embeddings()(torch.LongTensor([[1, 0, 1]]).to(device))\n\n            self.assertFalse(torch.allclose(embeds_neftune, embeds_neftune_2))\n            self.assertTrue(len(trainer.model.get_input_embeddings()._forward_hooks) > 0)\n\n            trainer.neftune_hook_handle.remove()\n\n            trainer.train()\n\n            self.assertIsNotNone(trainer.state.log_history[-1][\"train_loss\"])\n            self.assertIsNotNone(trainer.state.log_history[0][\"eval_loss\"])\n\n            self.assertTrue(\"adapter_model.safetensors\" in os.listdir(tmp_dir + \"/checkpoint-2\"))\n            self.assertTrue(\"adapter_config.json\" in os.listdir(tmp_dir + \"/checkpoint-2\"))\n            self.assertTrue(\"model.safetensors\" not in os.listdir(tmp_dir + \"/checkpoint-2\"))\n\n            # Make sure forward pass works fine to check if embeddings forward is not broken.\n            _ = trainer.model(torch.LongTensor([[1, 0, 1]]).to(device))\n            self.assertTrue(len(trainer.model.get_input_embeddings()._forward_hooks) == 0)\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_ddpo_trainer.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import gc\nimport unittest\nimport torch\nfrom trl import is_diffusers_available, is_peft_available\nfrom .testing_utils import require_diffusers\n\n\nif is_diffusers_available() and is_peft_available():\n    from trl import DDPOConfig, DDPOTrainer, DefaultDDPOStableDiffusionPipeline\n\n\ndef scorer_function(images, prompts, metadata):\n    return torch.randn(1) * 3.0, {}\n\n\ndef prompt_function():\n    return (\"cabbages\", {})\n\n\n@require_diffusers\nclass DDPOTrainerTester(unittest.TestCase):\n    \"\"\"\n    Test the DDPOTrainer class.\n    \"\"\"\n\n    def setUp(self):\n        self.ddpo_config = DDPOConfig(\n            num_epochs=2,\n            train_gradient_accumulation_steps=1,\n            per_prompt_stat_tracking_buffer_size=32,\n            sample_num_batches_per_epoch=2,\n            sample_batch_size=2,\n            mixed_precision=None,\n            save_freq=1000000,\n        )\n        pretrained_model = \"hf-internal-testing/tiny-stable-diffusion-torch\"\n        pretrained_revision = \"main\"\n\n        pipeline = DefaultDDPOStableDiffusionPipeline(\n            pretrained_model, pretrained_model_revision=pretrained_revision, use_lora=False\n        )\n\n        self.trainer = DDPOTrainer(self.ddpo_config, scorer_function, prompt_function, pipeline)\n\n        return super().setUp()\n\n    def tearDown(self) -> None:\n        gc.collect()\n\n    def test_loss(self):\n        advantage = torch.tensor([-1.0])\n        clip_range = 0.0001\n        ratio = torch.tensor([1.0])\n        loss = self.trainer.loss(advantage, clip_range, ratio)\n        self.assertEqual(loss.item(), 1.0)\n\n    def test_generate_samples(self):\n        samples, output_pairs = self.trainer._generate_samples(1, 2)\n        self.assertEqual(len(samples), 1)\n        self.assertEqual(len(output_pairs), 1)\n        self.assertEqual(len(output_pairs[0][0]), 2)\n\n    def test_calculate_loss(self):\n        samples, _ = self.trainer._generate_samples(1, 2)\n        sample = samples[0]\n\n        latents = sample[\"latents\"][0, 0].unsqueeze(0)\n        next_latents = sample[\"next_latents\"][0, 0].unsqueeze(0)\n        log_probs = sample[\"log_probs\"][0, 0].unsqueeze(0)\n        timesteps = sample[\"timesteps\"][0, 0].unsqueeze(0)\n        prompt_embeds = sample[\"prompt_embeds\"]\n        advantage = torch.tensor([1.0], device=prompt_embeds.device)\n\n        self.assertEqual(latents.shape, (1, 4, 64, 64))\n        self.assertEqual(next_latents.shape, (1, 4, 64, 64))\n        self.assertEqual(log_probs.shape, (1,))\n        self.assertEqual(timesteps.shape, (1,))\n        self.assertEqual(prompt_embeds.shape, (2, 77, 32))\n        loss, approx_kl, clipfrac = self.trainer.calculate_loss(\n            latents, timesteps, next_latents, log_probs, advantage, prompt_embeds\n        )\n\n        self.assertTrue(torch.isfinite(loss.cpu()))\n\n\n@require_diffusers\nclass DDPOTrainerWithLoRATester(DDPOTrainerTester):\n    \"\"\"\n    Test the DDPOTrainer class.\n    \"\"\"\n\n    def setUp(self):\n        self.ddpo_config = DDPOConfig(\n            num_epochs=2,\n            train_gradient_accumulation_steps=1,\n            per_prompt_stat_tracking_buffer_size=32,\n            sample_num_batches_per_epoch=2,\n            sample_batch_size=2,\n            mixed_precision=None,\n            save_freq=1000000,\n        )\n        pretrained_model = \"hf-internal-testing/tiny-stable-diffusion-torch\"\n        pretrained_revision = \"main\"\n\n        pipeline = DefaultDDPOStableDiffusionPipeline(\n            pretrained_model, pretrained_model_revision=pretrained_revision, use_lora=True\n        )\n\n        self.trainer = DDPOTrainer(self.ddpo_config, scorer_function, prompt_function, pipeline)\n\n        return super().setUp()\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_iterative_sft_trainer.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import tempfile\nimport unittest\n\nimport torch\nfrom datasets import Dataset\nfrom parameterized import parameterized\nfrom transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments\n\nfrom trl import IterativeSFTTrainer\n\n\nclass IterativeTrainerTester(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.model_id = \"trl-internal-testing/dummy-GPT2-correct-vocab\"\n        cls.model = AutoModelForCausalLM.from_pretrained(cls.model_id)\n        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_id)\n        cls.tokenizer.pad_token = cls.tokenizer.eos_token\n\n        # get t5 as seq2seq example:\n        model_id = \"trl-internal-testing/tiny-T5ForConditionalGeneration-correct-vocab\"\n        cls.t5_model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n        cls.t5_tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n    def _init_tensor_dummy_dataset(self):\n        dummy_dataset_dict = {\n            \"input_ids\": [torch.tensor([5303, 3621]), torch.tensor([3666, 1438, 318]), torch.tensor([5303, 3621])],\n            \"attention_mask\": [torch.tensor([1, 1]), torch.tensor([1, 1, 1]), torch.tensor([1, 1])],\n            \"labels\": [torch.tensor([5303, 3621]), torch.tensor([3666, 1438, 318]), torch.tensor([5303, 3621])],\n        }\n\n        dummy_dataset = Dataset.from_dict(dummy_dataset_dict)\n        dummy_dataset.set_format(\"torch\")\n        return dummy_dataset\n\n    def _init_textual_dummy_dataset(self):\n        dummy_dataset_dict = {\n            \"texts\": [\"Testing the IterativeSFTTrainer.\", \"This is a test of the IterativeSFTTrainer\"],\n            \"texts_labels\": [\"Testing the IterativeSFTTrainer.\", \"This is a test of the IterativeSFTTrainer\"],\n        }\n\n        dummy_dataset = Dataset.from_dict(dummy_dataset_dict)\n        dummy_dataset.set_format(\"torch\")\n        return dummy_dataset\n\n    def setUp(self):\n        # initialize trainer\n        self.model.train()\n        return super().setUp()\n\n    @parameterized.expand(\n        [\n            [\"gpt2\", \"tensor\"],\n            [\"gpt2\", \"text\"],\n            [\"t5\", \"tensor\"],\n            [\"t5\", \"text\"],\n        ]\n    )\n    def test_iterative_step_from_tensor(self, model_name, input_name):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            # initialize dataset\n            if input_name == \"tensor\":\n                dummy_dataset = self._init_tensor_dummy_dataset()\n                inputs = {\n                    \"input_ids\": dummy_dataset[\"input_ids\"],\n                    \"attention_mask\": dummy_dataset[\"attention_mask\"],\n                    \"labels\": dummy_dataset[\"labels\"],\n                }\n            else:\n                dummy_dataset = self._init_textual_dummy_dataset()\n                inputs = {\n                    \"texts\": dummy_dataset[\"texts\"],\n                    \"texts_labels\": dummy_dataset[\"texts_labels\"],\n                }\n\n            if model_name == \"gpt2\":\n                model = self.model\n                tokenizer = self.tokenizer\n            else:\n                model = self.t5_model\n                tokenizer = self.t5_tokenizer\n\n            args = TrainingArguments(\n                output_dir=tmp_dir,\n                per_device_train_batch_size=2,\n                max_steps=2,\n            )\n            iterative_trainer = IterativeSFTTrainer(model=model, args=args, tokenizer=tokenizer)\n\n            iterative_trainer.step(**inputs)\n\n            for param in iterative_trainer.model.parameters():\n                assert param.grad is not None\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_modeling_value_head.py",
        "recommendation": "The code file is too long to analyze. Please select a shorter file.",
        "code_snippet": "import gc\nimport tempfile\nimport unittest\n\nimport torch\nfrom transformers import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n\nfrom trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\n\n\nALL_CAUSAL_LM_MODELS = [\n    \"trl-internal-testing/tiny-random-CodeGenForCausalLM\",\n    \"trl-internal-testing/tiny-random-GPTJForCausalLM\",\n    \"trl-internal-testing/tiny-random-GPTNeoForCausalLM\",\n    \"trl-internal-testing/tiny-random-GPTNeoXForCausalLM\",\n    \"trl-internal-testing/tiny-random-OPTForCausalLM\",\n    \"trl-internal-testing/tiny-random-BloomForCausalLM\",\n    \"trl-internal-testing/tiny-random-GPT2LMHeadModel\",\n    \"trl-internal-testing/tiny-random-CodeGenForCausalLM-sharded\",\n    \"trl-internal-testing/tiny-random-GPTNeoXForCausalLM-safetensors-sharded\",\n    \"trl-internal-testing/tiny-random-GPTNeoXForCausalLM-safetensors\"\n    # \"trl-internal-testing/tiny-random-LlamaForCausalLM\", uncomment on the next transformers release\n]\n\nALL_SEQ2SEQ_MODELS = [\n    \"trl-internal-testing/tiny-random-BartForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-BigBirdPegasusForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-BlenderbotForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-BlenderbotSmallForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-FSMTForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-LEDForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-LongT5ForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-M2M100ForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-MarianMTModel\",\n    \"trl-internal-testing/tiny-random-MBartForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-MT5ForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-MvpForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-PegasusForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-PegasusXForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-PLBartForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-ProphetNetForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-SwitchTransformersForConditionalGeneration\",\n    \"trl-internal-testing/tiny-random-T5ForConditionalGeneration\",\n]\n\n\nclass VHeadModelTester:\n    all_model_names = None\n    trl_model_class = None\n    transformers_model_class = None\n\n    def test_value_head(self):\n        r\"\"\"\n        Test if the v-head is added to the model successfully\n        \"\"\"\n        for model_name in self.all_model_names:\n            model = self.trl_model_class.from_pretrained(model_name)\n            self.assertTrue(hasattr(model, \"v_head\"))\n\n    def test_value_head_shape(self):\n        r\"\"\"\n        Test if the v-head has the correct shape\n        \"\"\"\n        for model_name in self.all_model_names:\n            model = self.trl_model_class.from_pretrained(model_name)\n            self.assertTrue(model.v_head.summary.weight.shape[0] == 1)\n\n    def test_value_head_init_random(self):\n        r\"\"\"\n        Test if the v-head has been randomly initialized.\n        We can check that by making sure the bias is different\n        than zeros by default.\n        \"\"\"\n        for model_name in self.all_model_names:\n            model = self.trl_model_class.from_pretrained(model_name)\n            self.assertFalse(torch.allclose(model.v_head.summary.bias, torch.zeros_like(model.v_head.summary.bias)))\n\n    def test_value_head_not_str(self):\n        r\"\"\"\n        Test if the v-head is added to the model successfully, by passing a non `PretrainedModel`\n        as an argument to `from_pretrained`.\n        \"\"\"\n        for model_name in self.all_model_names:\n            pretrained_model = self.transformers_model_class.from_pretrained(model_name)\n            model = self.trl_model_class.from_pretrained(pretrained_model)\n            self.assertTrue(hasattr(model, \"v_head\"))\n\n    def test_from_save_trl(self):\n        \"\"\"\n        Test if the model can be saved and loaded from a directory and get the same weights\n        Including the additional modules (e.g. v_head)\n        \"\"\"\n        for model_name in self.all_model_names:\n            model = self.trl_model_class.from_pretrained(model_name)\n\n            with tempfile.TemporaryDirectory() as tmp_dir:\n                model.save_pretrained(tmp_dir)\n\n                model_from_save = self.trl_model_class.from_pretrained(tmp_dir)\n\n            # Check if the weights are the same\n            for key in model_from_save.state_dict():\n                self.assertTrue(torch.allclose(model_from_save.state_dict()[key], model.state_dict()[key]))\n\n    def test_from_save_trl_sharded(self):\n        \"\"\"\n        Test if the model can be saved and loaded from a directory and get the same weights - sharded case\n        \"\"\"\n        for model_name in self.all_model_names:\n            model = self.trl_model_class.from_pretrained(model_name)\n\n            with tempfile.TemporaryDirectory() as tmp_dir:\n                model.save_pretrained(tmp_dir)\n\n                model_from_save = self.trl_model_class.from_pretrained(tmp_dir)\n\n            # Check if the weights are the same\n            for key in model_from_save.state_dict():\n                self.assertTrue(torch.allclose(model_from_save.state_dict()[key], model.state_dict()[key]))\n\n    def test_from_save_transformers_sharded(self):\n        \"\"\"\n        Test if the model can be saved and loaded using transformers and get the same weights - sharded case\n        \"\"\"\n        for model_name in self.all_model_names:\n            transformers_model = self.trl_model_class.transformers_parent_class.from_pretrained(model_name)\n\n            trl_model = self.trl_model_class.from_pretrained(model_name)\n\n            with tempfile.TemporaryDirectory() as tmp_dir:\n                trl_model.save_pretrained(tmp_dir, max_shard_size=\"1MB\")\n                transformers_model_from_save = self.trl_model_class.transformers_parent_class.from_pretrained(tmp_dir)\n\n            # Check if the weights are the same\n            for key in transformers_model.state_dict():\n                self.assertTrue(\n                    torch.allclose(\n                        transformers_model_from_save.state_dict()[key], transformers_model.state_dict()[key]\n                    )\n                )\n\n    def test_from_save_transformers(self):\n        \"\"\"\n        Test if the model can be saved and loaded using transformers and get the same weights.\n        We override the test of the super class to check if the weights are the same.\n        \"\"\"\n        for model_name in self.all_model_names:\n            transformers_model = self.trl_model_class.transformers_parent_class.from_pretrained(model_name)\n\n            trl_model = self.trl_model_class.from_pretrained(model_name)\n\n            with tempfile.TemporaryDirectory() as tmp_dir:\n                trl_model.save_pretrained(tmp_dir)\n                transformers_model_from_save = self.trl_model_class.transformers_parent_class.from_pretrained(tmp_dir)\n\n            # Check if the weights are the same\n            for key in transformers_model.state_dict():\n                self.assertTrue(\n                    torch.allclose(\n                        transformers_model_from_save.state_dict()[key], transformers_model.state_dict()[key]\n                    )\n                )\n\n            # Check if the trl model has the same keys as the transformers model\n            # except the v_head\n            for key in trl_model.state_dict():\n                if \"v_head\" not in key:\n                    self.assertTrue(key in transformers_model.state_dict())\n                    # check if the weights are the same\n                    self.assertTrue(torch.allclose(trl_model.state_dict()[key], transformers_model.state_dict()[key]))\n\n            # check if they have the same modules\n            self.assertTrue(\n                set(transformers_model_from_save.state_dict().keys()) == set(transformers_model.state_dict().keys())\n            )\n\n\nclass CausalLMValueHeadModelTester(VHeadModelTester, unittest.TestCase):\n    \"\"\"\n    Testing suite for v-head models.\n    \"\"\"\n\n    all_model_names = ALL_CAUSAL_LM_MODELS\n    trl_model_class = AutoModelForCausalLMWithValueHead\n    transformers_model_class = AutoModelForCausalLM\n\n    def tearDown(self):\n        # free memory\n        gc.collect()\n\n    def test_inference(self):\n        r\"\"\"\n        Test if the model can be used for inference and outputs 3 values\n        - logits, loss, and value states\n        \"\"\"\n        EXPECTED_OUTPUT_SIZE = 3\n\n        for model_name in self.all_model_names:\n            model = self.trl_model_class.from_pretrained(model_name)\n            input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n            outputs = model(input_ids)\n\n            # Check if the outputs are of the right size - here\n            # we always output 3 values - logits, loss, and value states\n            self.assertEqual(len(outputs), EXPECTED_OUTPUT_SIZE)\n\n    def test_dropout_config(self):\n        r\"\"\"\n        Test if we instantiate a model by adding `summary_drop_prob` to the config\n        it will be added to the v_head\n        \"\"\"\n        for model_name in self.all_model_names:\n            pretrained_model = self.transformers_model_class.from_pretrained(model_name)\n            pretrained_model.config.summary_dropout_prob = 0.5\n            model = self.trl_model_class.from_pretrained(pretrained_model)\n\n            # Check if v head of the model has the same dropout as the config\n            self.assertEqual(model.v_head.dropout.p, pretrained_model.config.summary_dropout_prob)\n\n    def test_dropout_kwargs(self):\n        r\"\"\"\n        Test if we instantiate a model by adding `summary_drop_prob` to the config\n        it will be added to the v_head\n        \"\"\"\n        for model_name in self.all_model_names:\n            v_head_kwargs = {\"summary_dropout_prob\": 0.5}\n\n            model = self.trl_model_class.from_pretrained(model_name, **v_head_kwargs)\n\n            # Check if v head of the model has the same dropout as the config\n            self.assertEqual(model.v_head.dropout.p, 0.5)\n\n            model = self.trl_model_class.from_pretrained(model_name, summary_dropout_prob=0.5)\n\n            # Check if v head of the model has the same dropout as the config\n            self.assertEqual(model.v_head.dropout.p, 0.5)\n\n    def test_generate(self):\n        r\"\"\"\n        Test if `generate` works for every model\n        \"\"\"\n        for model_name in self.all_model_names:\n            model = self.trl_model_class.from_pretrained(model_name)\n            input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n\n            # Just check if the generation works\n            _ = model.generate(input_ids)\n\n    def test_raise_error_not_causallm(self):\n        # Test with a model without a LM head\n        model_id = \"trl-internal-testing/tiny-random-GPT2Model\"\n        # This should raise a ValueError\n        with self.assertRaises(ValueError):\n            pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n            _ = AutoModelForCausalLMWithValueHead.from_pretrained(pretrained_model.transformer)\n\n    def test_transformers_bf16_kwargs(self):\n        r\"\"\"\n        Test if the transformers kwargs are correctly passed\n        Here we check that loading a model in half precision works as expected, i.e. the weights of\n        the `pretrained_model` attribute is loaded in half precision and you can run a dummy\n        forward pass without any issue.\n        \"\"\"\n        for model_name in self.all_model_names:\n            trl_model = self.trl_model_class.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n\n            lm_head_namings = self.trl_model_class.lm_head_namings\n\n            self.assertTrue(\n                any(hasattr(trl_model.pretrained_model, lm_head_naming) for lm_head_naming in lm_head_namings)\n            )\n\n            for lm_head_naming in lm_head_namings:\n                if hasattr(trl_model.pretrained_model, lm_head_naming):\n                    self.assertTrue(getattr(trl_model.pretrained_model, lm_head_naming).weight.dtype == torch.bfloat16)\n\n            dummy_input = torch.LongTensor([[0, 1, 0, 1]])\n\n            # check dummy forward pass works in half precision\n            _ = trl_model(dummy_input)\n\n    @unittest.skip(\"This test needs to be run manually due to HF token issue.\")\n    def test_push_to_hub(self):\n        for model_name in self.all_model_names:\n            model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\n            if \"sharded\" in model_name:\n                model.push_to_hub(model_name + \"-ppo\", use_auth_token=True, max_shard_size=\"1MB\")\n            else:\n                model.push_to_hub(model_name + \"-ppo\", use_auth_token=True)\n\n            model_from_pretrained = AutoModelForCausalLMWithValueHead.from_pretrained(model_name + \"-ppo\")\n            # check all keys\n            self.assertEqual(model.state_dict().keys(), model_from_pretrained.state_dict().keys())\n\n            for name, param in model.state_dict().items():\n                self.assertTrue(\n                    torch.allclose(param, model_from_pretrained.state_dict()[name]),\n                    f\"Parameter {name} is not the same after push_to_hub and from_pretrained\",\n                )\n\n\nclass Seq2SeqValueHeadModelTester(VHeadModelTester, unittest.TestCase):\n    \"\"\"\n    Testing suite for v-head models.\n    \"\"\"\n\n    all_model_names = ALL_SEQ2SEQ_MODELS\n    trl_model_class = AutoModelForSeq2SeqLMWithValueHead\n    transformers_model_class = AutoModelForSeq2SeqLM\n\n    def tearDown(self):\n        # free memory\n        gc.collect()\n\n    def test_inference(self):\n        r\"\"\"\n        Test if the model can be used for inference and outputs 3 values\n        - logits, loss, and value states\n        \"\"\"\n        EXPECTED_OUTPUT_SIZE = 3\n\n        for model_name in self.all_model_names:\n            model = self.trl_model_class.from_pretrained(model_name)\n            input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n            decoder_input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n            outputs = model(input_ids, decoder_input_ids=decoder_input_ids)\n\n            # Check if the outputs are of the right size - here\n            # we always output 3 values - logits, loss, and value states\n            self.assertEqual(len(outputs), EXPECTED_OUTPUT_SIZE)\n\n    def test_dropout_config(self):\n        r\"\"\"\n        Test if we instantiate a model by adding `summary_drop_prob` to the config\n        it will be added to the v_head\n        \"\"\"\n        for model_name in self.all_model_names:\n            pretrained_model = self.transformers_model_class.from_pretrained(model_name)\n            pretrained_model.config.summary_dropout_prob = 0.5\n            model = self.trl_model_class.from_pretrained(pretrained_model)\n\n            # Check if v head of the model has the same dropout as the config\n            self.assertEqual(model.v_head.dropout.p, pretrained_model.config.summary_dropout_prob)\n\n    def test_dropout_kwargs(self):\n        r\"\"\"\n        Test if we instantiate a model by adding `summary_drop_prob` to the config\n        it will be added to the v_head\n        \"\"\"\n        for model_name in self.all_model_names:\n            v_head_kwargs = {\"summary_dropout_prob\": 0.5}\n\n            model = self.trl_model_class.from_pretrained(model_name, **v_head_kwargs)\n\n            # Check if v head of the model has the same dropout as the config\n            self.assertEqual(model.v_head.dropout.p, 0.5)\n\n            model = self.trl_model_class.from_pretrained(model_name, summary_dropout_prob=0.5)\n\n            # Check if v head of the model has the same dropout as the config\n            self.assertEqual(model.v_head.dropout.p, 0.5)\n\n    def test_generate(self):\n        r\"\"\"\n        Test if `generate` works for every model\n        \"\"\"\n        for model_name in self.all_model_names:\n            model = self.trl_model_class.from_pretrained(model_name)\n            input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n            decoder_input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n\n            # Just check if the generation works\n            _ = model.generate(input_ids, decoder_input_ids=decoder_input_ids)\n\n    def test_raise_error_not_causallm(self):\n        # Test with a model without a LM head\n        model_id = \"trl-internal-testing/tiny-random-T5Model\"\n        # This should raise a ValueError\n        with self.assertRaises(ValueError):\n            pretrained_model = AutoModel.from_pretrained(model_id)\n            _ = self.trl_model_class.from_pretrained(pretrained_model)\n\n    @unittest.skip(\"This test needs to be run manually due to HF token issue.\")\n    def test_push_to_hub(self):\n        for model_name in self.all_model_names:\n            model = self.trl_model_class.from_pretrained(model_name)\n            if \"sharded\" in model_name:\n                model.push_to_hub(model_name + \"-ppo\", use_auth_token=True, max_shard_size=\"1MB\")\n            else:\n                model.push_to_hub(model_name + \"-ppo\", use_auth_token=True)\n\n            model_from_pretrained = self.trl_model_class.from_pretrained(model_name + \"-ppo\")\n            # check all keys\n            self.assertEqual(model.state_dict().keys(), model_from_pretrained.state_dict().keys())\n\n            for name, param in model.state_dict().items():\n                self.assertTrue(\n                    torch.allclose(param, model_from_pretrained.state_dict()[name]),\n                    f\"Parameter {name} is not the same after push_to_hub and from_pretrained\",\n                )\n\n    def test_transformers_bf16_kwargs(self):\n        r\"\"\"\n        Test if the transformers kwargs are correctly passed\n        Here we check that loading a model in half precision works as expected, i.e. the weights of\n        the `pretrained_model` attribute is loaded in half precision and you can run a dummy\n        forward pass without any issue.\n        \"\"\"\n        for model_name in self.all_model_names:\n            trl_model = self.trl_model_class.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n\n            lm_head_namings = self.trl_model_class.lm_head_namings\n\n            if model_name == \"trl-internal-testing/tiny-random-FSMTForConditionalGeneration\":\n                # skip the test for FSMT as it does not support mixed-prec\n                continue\n\n            self.assertTrue(\n                any(hasattr(trl_model.pretrained_model, lm_head_naming) for lm_head_naming in lm_head_namings)\n            )\n\n            for lm_head_naming in lm_head_namings:\n                if hasattr(trl_model.pretrained_model, lm_head_naming):\n                    self.assertTrue(getattr(trl_model.pretrained_model, lm_head_naming).weight.dtype == torch.bfloat16)\n\n            dummy_input = torch.LongTensor([[0, 1, 0, 1]])\n\n            # check dummy forward pass works in half precision\n            _ = trl_model(input_ids=dummy_input, decoder_input_ids=dummy_input)\n\n\nclass ReferenceModelTest(unittest.TestCase):\n    def setUp(self):\n        self.model = AutoModelForCausalLMWithValueHead.from_pretrained(\n            \"trl-internal-testing/tiny-random-GPT2LMHeadModel\"\n        )\n        self.test_input = torch.tensor([[0, 1, 2, 3]])\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1)\n        self.layer_format = \"pretrained_model.transformer.h.{layer}.attn.c_attn.weight\"\n\n    def test_independent_reference(self):\n        layer_0 = self.layer_format.format(layer=0)\n        layer_5 = self.layer_format.format(layer=4)\n\n        ref_model = create_reference_model(self.model)\n\n        first_layer_before = self.model.get_parameter(layer_0).data.clone()\n        last_layer_before = self.model.get_parameter(layer_5).data.clone()\n\n        first_ref_layer_before = ref_model.get_parameter(layer_0).data.clone()\n        last_ref_layer_before = ref_model.get_parameter(layer_5).data.clone()\n\n        output = self.model(input_ids=self.test_input, labels=self.test_input)\n        output[1].backward()\n        self.optimizer.step()\n\n        first_layer_after = self.model.get_parameter(layer_0).data.clone()\n        last_layer_after = self.model.get_parameter(layer_5).data.clone()\n\n        first_ref_layer_after = ref_model.get_parameter(layer_0).data.clone()\n        last_ref_layer_after = ref_model.get_parameter(layer_5).data.clone()\n\n        # before optimization ref and model are identical\n        self.assertTrue((first_layer_before == first_ref_layer_before).all())\n        self.assertTrue((last_layer_before == last_ref_layer_before).all())\n        # ref model stays identical after optimization\n        self.assertTrue((first_ref_layer_before == first_ref_layer_after).all())\n        self.assertTrue((last_ref_layer_before == last_ref_layer_after).all())\n        # optimized model changes\n        self.assertTrue(not (first_layer_before == first_layer_after).all())\n        self.assertTrue(not (last_layer_before == last_layer_after).all())\n\n    def test_shared_layers(self):\n        layer_0 = self.layer_format.format(layer=0)\n        layer_1 = self.layer_format.format(layer=1)\n\n        ref_model = create_reference_model(self.model, num_shared_layers=1)\n\n        first_layer_before = self.model.get_parameter(layer_0).data.clone()\n        second_layer_before = self.model.get_parameter(layer_1).data.clone()\n\n        first_ref_layer_before = ref_model.get_parameter(layer_0).data.clone()\n        second_ref_layer_before = ref_model.get_parameter(layer_1).data.clone()\n\n        output = self.model(input_ids=self.test_input, labels=self.test_input)\n        output[1].backward()\n        self.optimizer.step()\n\n        first_layer_after = self.model.get_parameter(layer_0).data.clone()\n        second_layer_after = self.model.get_parameter(layer_1).data.clone()\n\n        first_ref_layer_after = ref_model.get_parameter(layer_0).data.clone()\n        second_ref_layer_after = ref_model.get_parameter(layer_1).data.clone()\n\n        # before optimization ref and model are identical\n        self.assertTrue((first_layer_before == first_ref_layer_before).all())\n        self.assertTrue((second_layer_before == second_ref_layer_before).all())\n        # ref model stays identical after optimization\n        self.assertTrue((first_ref_layer_before == first_ref_layer_after).all())\n        self.assertTrue((second_ref_layer_before == second_ref_layer_after).all())\n        # first layer of optimized model stays the same\n        self.assertTrue((first_layer_before == first_layer_after).all())\n        # other layers in optimized model change\n        self.assertTrue(not (second_layer_before == second_layer_after).all())\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_best_of_n_sampler.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import unittest\n\nimport torch\nfrom transformers import AutoTokenizer, GenerationConfig\n\nfrom trl import AutoModelForCausalLMWithValueHead\nfrom trl.core import LengthSampler\nfrom trl.extras import BestOfNSampler\n\n\ndef queries_to_scores(list_of_strings):\n    return [torch.rand(1).item() for _ in list_of_strings]\n\n\nclass BestOfNSamplerTester(unittest.TestCase):\n    \"\"\"\n    Tests the BestOfNSampler class\n    \"\"\"\n\n    ref_model_name = \"trl-internal-testing/dummy-GPT2-correct-vocab\"\n    output_length_sampler = LengthSampler(2, 6)\n    model = AutoModelForCausalLMWithValueHead.from_pretrained(ref_model_name)\n    tokenizer = AutoTokenizer.from_pretrained(ref_model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n    output_length_sampler = LengthSampler(2, 6)\n\n    def test_different_input_types(self):\n        r\"\"\"\n        Tests if the different input types normalizer works\n        \"\"\"\n\n        generation_config = GenerationConfig(\n            min_length=-1,\n            top_k=0.0,\n            top_p=1.0,\n            do_sample=True,\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n\n        output_length_sampler = LengthSampler(2, 6)\n\n        best_of_n = BestOfNSampler(\n            self.model,\n            self.tokenizer,\n            queries_to_scores,\n            length_sampler=output_length_sampler,\n            generation_config=generation_config,\n        )\n\n        queries = [\"hello world\", \"goodbye world\"]\n        tokenized_queries = [self.tokenizer.encode(query) for query in queries]\n\n        various_queries_formats = [\n            (tokenized_queries[0], 1),\n            (tokenized_queries, 2),\n            (torch.tensor(tokenized_queries[1]), 1),\n            ([torch.tensor(query) for query in tokenized_queries], 2),\n        ]\n\n        for q, expected_length in various_queries_formats:\n            results = best_of_n.generate(q)\n            self.assertIsInstance(results, list)\n            assert len(results) == expected_length\n\n    def test_different_sample_sizes_and_n_candidates_values(self):\n        r\"\"\"\n        Tests different sample sizes and n_candidates values\n        \"\"\"\n        generation_config = GenerationConfig(\n            min_length=-1,\n            top_k=0.0,\n            top_p=1.0,\n            do_sample=True,\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n\n        output_length_sampler = LengthSampler(6, 10)\n\n        for sample_value, n_candidates_values, expected in [\n            (4, 2, 2),\n            (10, 3, 3),\n            (6, 4, 4),\n        ]:\n            best_of_n = BestOfNSampler(\n                self.model,\n                self.tokenizer,\n                queries_to_scores,\n                length_sampler=output_length_sampler,\n                generation_config=generation_config,\n                sample_size=sample_value,\n                n_candidates=n_candidates_values,\n            )\n\n            queries = [\"hello world\", \"troll the world\"]\n            tokenized_queries = [self.tokenizer.encode(query) for query in queries]\n            results = best_of_n.generate(tokenized_queries)\n            for result in results:\n                assert len(result) == expected\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_ppo_trainer.py",
        "recommendation": "The code file is too long to analyze. Please select a shorter file.",
        "code_snippet": "import copy\nimport fnmatch\nimport gc\nimport re\nimport tempfile\nimport unittest\n\nimport pytest\nimport torch\nfrom huggingface_hub import HfApi, HfFolder, delete_repo\nfrom parameterized import parameterized\nfrom pytest import mark\nfrom requests.exceptions import HTTPError\nfrom transformers import AutoTokenizer\n\nfrom trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed\nfrom trl.core import respond_to_batch\n\nfrom .testing_constants import CI_HUB_ENDPOINT, CI_HUB_USER, CI_HUB_USER_TOKEN\nfrom .testing_utils import require_peft, require_torch_multi_gpu\n\n\nEXPECTED_STATS = [\n    \"objective/kl\",\n    \"objective/kl_dist\",\n    \"objective/logprobs\",\n    \"objective/ref_logprobs\",\n    \"objective/kl_coef\",\n    \"objective/entropy\",\n    \"ppo/mean_non_score_reward\",\n    \"ppo/loss/policy\",\n    \"ppo/loss/value\",\n    \"ppo/loss/total\",\n    \"ppo/policy/entropy\",\n    \"ppo/policy/approxkl\",\n    \"ppo/policy/policykl\",\n    \"ppo/policy/clipfrac\",\n    \"ppo/policy/advantages\",\n    \"ppo/policy/advantages_mean\",\n    \"ppo/policy/ratio\",\n    \"ppo/returns/mean\",\n    \"ppo/returns/var\",\n    \"ppo/val/vpred\",\n    \"ppo/val/error\",\n    \"ppo/val/clipfrac\",\n    \"ppo/val/mean\",\n    \"ppo/val/var\",\n    \"ppo/val/var_explained\",\n    \"time/ppo/forward_pass\",\n    \"time/ppo/compute_rewards\",\n    \"time/ppo/optimize_step\",\n    \"time/ppo/calc_stats\",\n    \"time/ppo/total\",\n    \"ppo/learning_rate\",\n]\n\n\nclass DummyDataset(torch.utils.data.Dataset):\n    def __init__(self, query_data, response_data):\n        self.query_data = query_data\n        self.response_data = response_data\n\n    def __len__(self):\n        return len(self.query_data)\n\n    def __getitem__(self, idx):\n        return self.query_data[idx], self.response_data[idx]\n\n\ndef apply_mask(values, mask):\n    unmasked_values = []\n    for v, m in zip(values, mask):\n        if m == 1:\n            unmasked_values.append(v)\n    return torch.Tensor(unmasked_values)\n\n\ndef abs_diff_masked_tensors(tensor_1, tensor_2, mask_1, mask_2):\n    diffs = []\n    for l1, l2, m1, m2 in zip(tensor_1, tensor_2, mask_1, mask_2):\n        diff = apply_mask(l1, m1) - apply_mask(l2, m2)\n        diffs.append(diff.sum())\n    return abs(sum(diffs))\n\n\nclass PPOTrainerTester(unittest.TestCase):\n    \"\"\"\n    A wrapper class for testing PPOTrainer\n    \"\"\"\n\n    @classmethod\n    def setUpClass(cls):\n        set_seed(42)\n        cls._token = CI_HUB_USER_TOKEN\n        cls._api = HfApi(endpoint=CI_HUB_ENDPOINT)\n        HfFolder.save_token(CI_HUB_USER_TOKEN)\n\n        # model_id\n        cls.model_id = \"trl-internal-testing/dummy-GPT2-correct-vocab\"\n\n        # get models and tokenizer\n        cls.gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(cls.model_id)\n        cls.gpt2_model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(cls.model_id)\n        cls.gpt2_tokenizer = AutoTokenizer.from_pretrained(cls.model_id)\n\n        cls.gpt2_tokenizer.pad_token = cls.gpt2_tokenizer.eos_token\n\n        # get bloom as right padding examples:\n        model_id = \"trl-internal-testing/tiny-BloomForCausalLM-correct-vocab\"\n        cls.bloom_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_id)\n        cls.bloom_tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n        model_id = \"trl-internal-testing/tiny-T5ForConditionalGeneration-correct-vocab\"\n        cls.t5_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_id)\n        cls.t5_tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n        # initialize trainer\n        cls.ppo_config = PPOConfig(batch_size=2, mini_batch_size=1, log_with=None)\n\n    @classmethod\n    def tearDownClass(cls):\n        for model in [f\"{CI_HUB_USER}/test-ppo-trainer\"]:\n            try:\n                delete_repo(token=cls._token, repo_id=model)\n            except HTTPError:\n                pass\n\n    def setUp(self):\n        # initialize trainer\n        self.ppo_config = PPOConfig(batch_size=2, mini_batch_size=1, log_with=None)\n        self.gpt2_model.train()\n        return super().setUp()\n\n    def tearDown(self):\n        # free memory\n        gc.collect()\n\n    def _init_dummy_dataset(self):\n        # encode a query\n        query_txt = \"This morning I went to the \"\n        query_tensor = self.gpt2_tokenizer.encode(query_txt, return_tensors=\"pt\")\n        assert query_tensor.shape == (1, 7)\n        # get model response\n        response_tensor = respond_to_batch(self.gpt2_model, query_tensor)\n        assert response_tensor.shape == (1, 20)\n\n        # create a dummy dataset\n        min_length = min(len(query_tensor[0]), len(response_tensor[0]))\n        dummy_dataset = DummyDataset(\n            [query_tensor[:, :min_length].squeeze(0) for _ in range(2)],\n            [response_tensor[:, :min_length].squeeze(0) for _ in range(2)],\n        )\n\n        return dummy_dataset\n\n    def test_drop_last_dataloader(self):\n        self.ppo_config = PPOConfig(batch_size=3, mini_batch_size=1, log_with=None)\n\n        dummy_dataset = self._init_dummy_dataset()\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=self.gpt2_model_ref,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n        dummy_dataloader = ppo_trainer.dataloader\n\n        self.assertEqual(len(dummy_dataloader), 0)\n\n    def test_ppo_step(self):\n        # initialize dataset\n        dummy_dataset = self._init_dummy_dataset()\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=self.gpt2_model_ref,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n        dummy_dataloader = ppo_trainer.dataloader\n        # train model with ppo\n        for query_tensor, response_tensor in dummy_dataloader:\n            # define a reward for response\n            # (this could be any reward such as human feedback or output from another model)\n            reward = [torch.tensor(1.0), torch.tensor(0.0)]\n            # train model\n            train_stats = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n            break\n\n        for param in ppo_trainer.model.parameters():\n            assert param.grad is not None\n\n        for stat in EXPECTED_STATS:\n            assert stat in train_stats.keys()\n\n    def test_ppo_step_with_masks(self):\n        # initialize dataset\n        dummy_dataset = self._init_dummy_dataset()\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=self.gpt2_model_ref,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n        dummy_dataloader = ppo_trainer.dataloader\n        # train model with ppo\n        for query_tensor, response_tensor in dummy_dataloader:\n            # define a reward for response\n            # (this could be any reward such as human feedback or output from another model)\n            reward = [torch.tensor(1.0), torch.tensor(0.0)]\n\n            response_mask = [torch.ones_like(r) for r in response_tensor]\n\n            # train model\n            train_stats = ppo_trainer.step(\n                [q for q in query_tensor], [r for r in response_tensor], reward, response_mask\n            )\n            break\n\n        for param in ppo_trainer.model.parameters():\n            assert param.grad is not None\n\n        for stat in EXPECTED_STATS:\n            assert stat in train_stats.keys()\n\n    def test_ppo_step_with_no_ref_sgd(self):\n        # initialize dataset\n        dummy_dataset = self._init_dummy_dataset()\n        optimizer = torch.optim.SGD(self.gpt2_model.parameters(), lr=0.01)\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=None,\n            optimizer=optimizer,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n        dummy_dataloader = ppo_trainer.dataloader\n\n        self.assertTrue(isinstance(ppo_trainer.optimizer.optimizer, torch.optim.SGD))\n\n        # train model with ppo\n        for query_tensor, response_tensor in dummy_dataloader:\n            # define a reward for response\n            # (this could be any reward such as human feedback or output from another model)\n            reward = [torch.tensor(1.0), torch.tensor(0.0)]\n            # train model\n            train_stats = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n            break\n\n        for name, param in ppo_trainer.model.named_parameters():\n            self.assertTrue(param.grad is not None, f\"Parameter {name} has no gradient\")\n\n        # ref model should not be trained\n        for name, param in ppo_trainer.ref_model.named_parameters():\n            self.assertTrue(param.grad is None, f\"Parameter {name} has a gradient\")\n\n        # Finally check stats\n        for stat in EXPECTED_STATS:\n            assert stat in train_stats.keys()\n\n    def test_ppo_step_with_no_ref_sgd_lr_scheduler(self):\n        # initialize dataset\n        dummy_dataset = self._init_dummy_dataset()\n        optimizer = torch.optim.SGD(self.gpt2_model.parameters(), lr=0.01)\n        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=None,\n            optimizer=optimizer,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n            lr_scheduler=lr_scheduler,\n        )\n        dummy_dataloader = ppo_trainer.dataloader\n\n        self.assertTrue(isinstance(ppo_trainer.optimizer.optimizer, torch.optim.SGD))\n        self.assertTrue(isinstance(ppo_trainer.lr_scheduler.scheduler, torch.optim.lr_scheduler.ExponentialLR))\n\n        # train model with ppo\n        for query_tensor, response_tensor in dummy_dataloader:\n            # define a reward for response\n            # (this could be any reward such as human feedback or output from another model)\n            reward = [torch.tensor(1.0), torch.tensor(0.0)]\n            # train model\n            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n            train_stats = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n            break\n\n        for name, param in ppo_trainer.model.named_parameters():\n            self.assertTrue(param.grad is not None, f\"Parameter {name} has no gradient\")\n\n        # ref model should not be trained\n        for name, param in ppo_trainer.ref_model.named_parameters():\n            self.assertTrue(param.grad is None, f\"Parameter {name} has a gradient\")\n\n        # Finally check stats\n        for stat in EXPECTED_STATS:\n            assert stat in train_stats.keys()\n\n        # assert that the LR has increased for exponential decay\n        self.assertTrue(train_stats[\"ppo/learning_rate\"] > self.ppo_config.learning_rate)\n\n    def test_ppo_step_with_no_ref(self):\n        # initialize dataset\n        dummy_dataset = self._init_dummy_dataset()\n        self.gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(self.model_id)\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n        dummy_dataloader = ppo_trainer.dataloader\n        # train model with ppo\n        for query_tensor, response_tensor in dummy_dataloader:\n            # define a reward for response\n            # (this could be any reward such as human feedback or output from another model)\n            reward = [torch.tensor(1.0), torch.tensor(0.0)]\n            # train model\n            train_stats = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n            break\n\n        for name, param in ppo_trainer.model.named_parameters():\n            self.assertTrue(param.grad is not None, f\"Parameter {name} has no gradient\")\n\n        # ref model should not be trained\n        for name, param in ppo_trainer.ref_model.named_parameters():\n            self.assertTrue(param.grad is None, f\"Parameter {name} has a gradient\")\n\n        # initialize a new gpt2 model:\n        model = AutoModelForCausalLMWithValueHead.from_pretrained(self.model_id)\n        for name, param in ppo_trainer.ref_model.named_parameters():\n            if \"v_head\" not in name:\n                name = name.replace(\"pretrained_model.\", \"\")\n\n                self.assertTrue(\n                    torch.allclose(param.cpu(), model.state_dict()[name].cpu()),\n                    f\"Parameter {name} has changed from the original model\",\n                )\n\n        # Finally check stats\n        for stat in EXPECTED_STATS:\n            assert stat in train_stats.keys()\n\n    def test_ppo_step_with_no_ref_custom_layers(self):\n        \"\"\"\n        Test PPO step with no reference model and custom layers\n        For shared layers configuration, all the layers after the `num_shared_layers` are considered as custom layers\n        therefore the gradients should be computed for these layers only.\n        \"\"\"\n        # initialize dataset\n        dummy_dataset = self._init_dummy_dataset()\n        self.gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(self.model_id)\n        num_shared_layers = 1\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n            num_shared_layers=num_shared_layers,\n        )\n        dummy_dataloader = ppo_trainer.dataloader\n        # train model with ppo\n        for query_tensor, response_tensor in dummy_dataloader:\n            # define a reward for response\n            # (this could be any reward such as human feedback or output from another model)\n            reward = [torch.tensor(1.0), torch.tensor(0.0)]\n            # train model\n            train_stats = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n            break\n\n        pattern = r\".*transformer\\.h\\.(\\d+)\\..*\"\n        final_layers = [\"ln_f\", \"v_head\", \"lm_head\"]\n\n        for name, param in ppo_trainer.model.named_parameters():\n            if re.match(pattern, name):\n                layer_number = int(re.match(pattern, name).groups(0)[0])\n                if layer_number < num_shared_layers:\n                    self.assertTrue(param.grad is None, f\"Parameter {name} has a gradient\")\n                else:\n                    self.assertTrue(param.grad is not None, f\"Parameter {name} has no gradient\")\n            elif any([layer in name for layer in final_layers]):\n                self.assertTrue(param.grad is not None, f\"Parameter {name} has no gradient\")\n\n        # ref model should not be trained\n        for name, param in ppo_trainer.ref_model.named_parameters():\n            self.assertTrue(param.grad is None, f\"Parameter {name} has a gradient\")\n\n        for stat in EXPECTED_STATS:\n            assert stat in train_stats.keys()\n\n    def test_ppo_step_with_ref_and_custom_layers_warning(self):\n        \"\"\"\n        Test PPO step with a reference model and custom layers\n        The trainer should raise a warning if the argument `num_shared_layers` is set\n        together with a reference model.\n        \"\"\"\n        # initialize dataset\n        dummy_dataset = self._init_dummy_dataset()\n\n        num_shared_layers = 6\n\n        with self.assertWarns(UserWarning):\n            _ = PPOTrainer(\n                config=self.ppo_config,\n                model=self.gpt2_model,\n                ref_model=self.gpt2_model_ref,\n                tokenizer=self.gpt2_tokenizer,\n                dataset=dummy_dataset,\n                num_shared_layers=num_shared_layers,\n            )\n\n    def test_ppo_step_rewards_shape(self):\n        \"\"\"\n        Test if the rewards shape is correct by asserting that if a wrong reward shape is passed, we get\n        a value error.\n        \"\"\"\n\n        # initialize dataset\n        dummy_dataset = self._init_dummy_dataset()\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n        dummy_dataloader = ppo_trainer.dataloader\n        # train model with ppo\n        for query_tensor, response_tensor in dummy_dataloader:\n            # define a reward for response\n            # (this could be any reward such as human feedback or output from another model)\n            reward = [torch.tensor([[1.0]]), torch.tensor([[0.0]])]\n            # train model - this should raise an error\n            with self.assertRaises(ValueError):\n                _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n\n            reward = [torch.tensor([1.0]), torch.tensor([0.0])]\n            # train model - this should work\n            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n            break\n\n        # check if the gradients are computed for the model\n        for name, param in ppo_trainer.model.named_parameters():\n            self.assertTrue(param.grad is not None, f\"Parameter {name} has no gradient\")\n\n        # ref model should not be trained\n        for name, param in ppo_trainer.ref_model.named_parameters():\n            self.assertTrue(param.grad is None, f\"Parameter {name} has a gradient\")\n\n    def test_ppo_step_input_shape(self):\n        \"\"\"\n        Test if the shape of the expected inputs are correct\n        \"\"\"\n        # initialize dataset\n        dummy_dataset = self._init_dummy_dataset()\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n        dummy_dataloader = ppo_trainer.dataloader\n        # train model with ppo\n        for query_tensor, response_tensor in dummy_dataloader:\n            # define a reward for response\n            # (this could be any reward such as human feedback or output from another model)\n            reward = [torch.tensor([1.0]), torch.tensor([0.0])]\n            # train model - this should raise an error\n            bs = ppo_trainer.config.batch_size\n\n            queries, responses, _, _ = ppo_trainer._step_safety_checker(\n                bs, [q for q in query_tensor], [r for r in response_tensor], reward\n            )\n\n            self.assertTrue(isinstance(queries, list), f\"queries should be a list, got {type(queries)}\")\n            self.assertTrue(isinstance(responses, list), f\"responses should be a list, got {type(responses)}\")\n\n            # check the shapes\n            for i in range(bs):\n                self.assertEqual(queries[i].shape, torch.Size([7]))\n                self.assertEqual(responses[i].size(), torch.Size([7]))\n            break\n\n    def test_ppo_step_no_dataset(self):\n        \"\"\"\n        Test if the training loop works fine without passing a dataset\n        \"\"\"\n        query_txt = \"This morning I went to the \"\n        query_tensor = self.gpt2_tokenizer.encode(query_txt, return_tensors=\"pt\")\n        self.ppo_config.batch_size = 1\n\n        response_tensor = respond_to_batch(self.gpt2_model, query_tensor)\n\n        # Check that this warns the user about batch size\n        with self.assertWarns(UserWarning):\n            ppo_trainer = PPOTrainer(\n                config=self.ppo_config,\n                model=self.gpt2_model,\n                ref_model=self.gpt2_model_ref,\n                tokenizer=self.gpt2_tokenizer,\n            )\n        # train model with ppo\n        reward = [torch.tensor([1.0])]\n        # train model - this should work fine\n        train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)\n\n        # check gradients\n        for name, param in ppo_trainer.model.named_parameters():\n            self.assertTrue(param.grad is not None, f\"Parameter {name} has no gradient\")\n\n        # ref model should not be trained\n        for name, param in ppo_trainer.ref_model.named_parameters():\n            self.assertTrue(param.grad is None, f\"Parameter {name} has a gradient\")\n\n        # check train stats\n        for stat in EXPECTED_STATS:\n            self.assertTrue(stat in train_stats, f\"Train stats should contain {stat}\")\n\n    def test_loss_trainer(self):\n        \"\"\"\n        Test if the loss trainer works fine\n        \"\"\"\n        # initialize dataset\n        dummy_dataset = self._init_dummy_dataset()\n\n        self.gpt2_model.eval()\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n\n        dummy_queries = [torch.tensor([1, 2, 3, 4]), torch.tensor([1, 2, 3, 4, 5, 6, 7])]\n        dummy_responses = [torch.tensor([5, 6, 7, 8, 9]), torch.tensor([8, 9, 10, 11, 12, 13])]\n        dummy_scores = torch.Tensor([1, 2])\n\n        ppo_trainer.config.mini_batch_size = 1\n        ppo_trainer.config.batch_size = 1\n        model_inputs = ppo_trainer.prepare_model_inputs(dummy_queries, dummy_responses)\n        all_logprobs, _, values, mask = ppo_trainer.batched_forward_pass(\n            self.gpt2_model, dummy_queries, dummy_responses, model_inputs\n        )\n\n        # dummy values\n        ref_logprobs = all_logprobs + 1\n        logits = torch.exp(all_logprobs)\n        vpreds = values + 0.1\n\n        score, non_score, kls = ppo_trainer.compute_rewards(dummy_scores, all_logprobs, ref_logprobs, mask)\n        values, advantages, returns = ppo_trainer.compute_advantages(values, score, mask)\n\n        # just make sure a dummy loss is computed\n        idx = 0\n        pg_loss, v_loss, _ = ppo_trainer.loss(\n            all_logprobs[idx].unsqueeze(0),\n            values[idx].unsqueeze(0),\n            logits[idx].unsqueeze(0),\n            vpreds[idx].unsqueeze(0),\n            ref_logprobs[idx].unsqueeze(0),\n            mask[idx].unsqueeze(0),\n            advantages[idx].unsqueeze(0),\n            returns[idx].unsqueeze(0),\n        )\n\n        self.assertAlmostEqual(pg_loss.item(), 2.0494, 4)\n        self.assertAlmostEqual(v_loss.item(), 0.07110, 4)\n\n        # check if we get same results with masked parts removed\n        pg_loss_unmasked, v_loss_unmasked, _ = ppo_trainer.loss(\n            apply_mask(all_logprobs[idx], mask[idx]).unsqueeze(0),\n            apply_mask(values[idx], mask[idx]).unsqueeze(0),\n            apply_mask(logits[idx], mask[idx]).unsqueeze(0),\n            apply_mask(vpreds[idx], mask[idx]).unsqueeze(0),\n            apply_mask(ref_logprobs[idx], mask[idx]).unsqueeze(0),\n            apply_mask(mask[idx], mask[idx]).unsqueeze(0),\n            apply_mask(advantages[idx], mask[idx]).unsqueeze(0),\n            apply_mask(returns[idx], mask[idx]).unsqueeze(0),\n        )\n        self.assertAlmostEqual(pg_loss_unmasked.item(), 2.0494, 4)\n        self.assertAlmostEqual(v_loss_unmasked.item(), 0.07110, 4)\n\n    @parameterized.expand(\n        [\n            [\"gpt2\"],\n            [\"bloom\"],\n            [\"t5\"],\n        ]\n    )\n    def test_batched_forward_pass(self, name):\n        \"\"\"\n        Test if the loss trainer works fine\n        \"\"\"\n        # initialize dataset\n        dummy_dataset = self._init_dummy_dataset()\n\n        dummy_queries = [torch.tensor([1, 2, 3, 4]), torch.tensor([1, 2, 3, 4, 5, 6, 7])]\n        dummy_responses = [torch.tensor([5, 6, 7, 8, 9]), torch.tensor([8, 9, 10, 11, 12, 13])]\n\n        if name == \"gpt2\":\n            model = self.gpt2_model\n            tokenizer = self.gpt2_tokenizer\n        elif name == \"bloom\":\n            model = self.bloom_model\n            tokenizer = self.bloom_tokenizer\n        elif name == \"t5\":\n            model = self.t5_model\n            tokenizer = self.t5_tokenizer\n\n        model.eval()\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=model,\n            ref_model=None,\n            tokenizer=tokenizer,\n            dataset=dummy_dataset,\n        )\n\n        # we test all combinations of fwd_bs and bs:\n        # if fwd_bs=bs=1: no padding is applied and only one forward pass\n        # if fwd_bs=1/bs=2: padding is applied and results computed in two fwd passes\n        # if fwd_bs=bs=2: padding is applied and results computed in one fwd pass\n\n        ppo_trainer.config.mini_batch_size = 1\n        ppo_trainer.config.batch_size = 1\n\n        model_inputs = ppo_trainer.prepare_model_inputs([dummy_queries[0]], [dummy_responses[0]])\n        logprobs_0, logits_0, values_0, mask_0 = ppo_trainer.batched_forward_pass(\n            model, [dummy_queries[0]], [dummy_responses[0]], model_inputs\n        )\n\n        ppo_trainer.config.batch_size = 2\n        model_inputs = ppo_trainer.prepare_model_inputs(dummy_queries, dummy_responses)\n        logprobs_1, logits_1, values_1, mask_1 = ppo_trainer.batched_forward_pass(\n            model, dummy_queries, dummy_responses, model_inputs\n        )\n\n        ppo_trainer.config.mini_batch_size = 2\n        model_inputs = ppo_trainer.prepare_model_inputs(dummy_queries, dummy_responses)\n        logprobs_2, logits_2, values_2, mask_2 = ppo_trainer.batched_forward_pass(\n            model, dummy_queries, dummy_responses, model_inputs\n        )\n\n        self.assertLessEqual(abs_diff_masked_tensors(logprobs_1, logprobs_2, mask_1, mask_2), 1e-4)\n        self.assertLessEqual(abs_diff_masked_tensors(values_1, values_2, mask_1, mask_2), 1e-4)\n\n        self.assertLessEqual(abs_diff_masked_tensors(logprobs_0, logprobs_2[:1], mask_0, mask_2[:1]), 1e-4)\n        self.assertLessEqual(abs_diff_masked_tensors(values_0, values_2[:1], mask_0, mask_2[:1]), 1e-4)\n\n    def test_ppo_trainer_max_grad_norm(self):\n        \"\"\"\n        Test if the `max_grad_norm` feature works as expected\n        \"\"\"\n        # initialize dataset\n        dummy_dataset = self._init_dummy_dataset()\n\n        self.ppo_config.max_grad_norm = 0.00001\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n\n        dummy_dataloader = ppo_trainer.dataloader\n\n        # train model with ppo\n        for query_tensor, response_tensor in dummy_dataloader:\n            # define a reward for response\n            # (this could be any reward such as human feedback or output from another model)\n            reward = [torch.tensor(1.0), torch.tensor(0.0)]\n            # train model\n            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n            break\n\n        # check gradients\n        for name, param in ppo_trainer.model.named_parameters():\n            self.assertTrue(param.grad is not None, f\"Parameter {name} has no gradient\")\n            self.assertTrue(\n                torch.all(param.grad.abs() <= self.ppo_config.max_grad_norm),\n                f\"Parameter {name} has a gradient larger than max_grad_norm\",\n            )\n\n    def test_ppo_trainer_kl_penalty(self):\n        dummy_dataset = self._init_dummy_dataset()\n\n        log_probs = torch.Tensor([[0.5, 0.2, 0.1], [0.6, 0.2, 0.1]])\n        ref_log_probs = torch.Tensor([[0.4, 0.3, 0.0], [0.7, 0.1, 0.3]])\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n\n        expected_output = torch.Tensor([[0.1000, -0.1000, 0.1000], [-0.1000, 0.1000, -0.2000]])\n        self.assertTrue(torch.allclose(ppo_trainer._kl_penalty(log_probs, ref_log_probs), expected_output))\n\n        self.ppo_config.kl_penalty = \"abs\"\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n\n        expected_output = torch.Tensor([[0.1000, 0.1000, 0.1000], [0.1000, 0.1000, 0.2000]])\n        self.assertTrue(torch.allclose(ppo_trainer._kl_penalty(log_probs, ref_log_probs), expected_output))\n\n        self.ppo_config.kl_penalty = \"mse\"\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n\n        expected_output = torch.Tensor([[0.0050, 0.0050, 0.0050], [0.0050, 0.0050, 0.0200]])\n        self.assertTrue(torch.allclose(ppo_trainer._kl_penalty(log_probs, ref_log_probs), expected_output))\n\n    def test_ppo_trainer_full_kl_penalty(self):\n        # a few more extensive tests for the full kl option as it is more involved\n        dummy_dataset = self._init_dummy_dataset()\n\n        self.ppo_config.kl_penalty = \"full\"\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n\n        # Test on tensors for size B,S,T = (1,2,3)\n        # test for when the two dists are the same\n        log_probs = torch.Tensor(\n            [\n                [\n                    [0.1, 0.2, 0.7],\n                    [0.3, 0.4, 0.3],\n                ]\n            ]\n        ).exp()\n\n        ref_log_probs = torch.Tensor(\n            [\n                [\n                    [0.1, 0.2, 0.7],\n                    [0.3, 0.4, 0.3],\n                ]\n            ]\n        ).exp()\n\n        expected_output = torch.Tensor(\n            [[0.0, 0.0]],\n        )\n        output = ppo_trainer._kl_penalty(log_probs, ref_log_probs)\n        self.assertTrue(output.shape == (1, 2))\n        self.assertTrue(torch.allclose(output, expected_output))\n\n        # test for when the two dists are almost not overlapping\n        log_probs = torch.Tensor(\n            [\n                [\n                    [0.98, 0.01, 0.01],\n                    [0.01, 0.98, 0.01],\n                ]\n            ]\n        ).log()\n\n        ref_log_probs = torch.Tensor(\n            [\n                [\n                    [0.01, 0.01, 0.98],\n                    [0.01, 0.01, 0.98],\n                ]\n            ]\n        ).log()\n\n        expected_output = torch.Tensor(\n            [[4.4474, 4.4474]],\n        )\n        output = ppo_trainer._kl_penalty(log_probs, ref_log_probs)\n        self.assertTrue(output.shape == (1, 2))\n        self.assertTrue(torch.allclose(output, expected_output))\n\n        # test for when the two dists are almost not overlapping\n        log_probs = torch.Tensor(\n            [\n                [\n                    [0.49, 0.02, 0.49],\n                    [0.49, 0.02, 0.49],\n                ]\n            ]\n        ).log()\n\n        ref_log_probs = torch.Tensor(\n            [\n                [\n                    [0.01, 0.98, 0.01],\n                    [0.49, 0.02, 0.49],\n                ]\n            ]\n        ).log()\n\n        expected_output = torch.Tensor(\n            [[3.7361, 0.0]],\n        )\n        output = ppo_trainer._kl_penalty(log_probs, ref_log_probs)\n        self.assertTrue(output.shape == (1, 2))\n        self.assertTrue(torch.allclose(output, expected_output, atol=1e-4))\n\n    @require_peft\n    @mark.peft_test\n    def test_peft_model_ppo_trainer(self):\n        from peft import LoraConfig, get_peft_model\n        from transformers import AutoModelForCausalLM\n\n        lora_config = LoraConfig(\n            r=16,\n            lora_alpha=32,\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n        gpt2_model = AutoModelForCausalLM.from_pretrained(self.model_id)\n\n        # this line is very important\n        def make_inputs_require_grad(module, input, output):\n            output.requires_grad_(True)\n\n        gpt2_model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n        peft_model = get_peft_model(gpt2_model, lora_config)\n        model = AutoModelForCausalLMWithValueHead.from_pretrained(peft_model)\n\n        dummy_dataset = self._init_dummy_dataset()\n        self.ppo_config.batch_size = 2\n        self.ppo_config.mini_batch_size = 1\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=model,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n\n        self.assertTrue(ppo_trainer.ref_model is None)\n\n        dummy_dataloader = ppo_trainer.dataloader\n\n        # train model with ppo\n        for query_tensor, response_tensor in dummy_dataloader:\n            # define a reward for response\n            # (this could be any reward such as human feedback or output from another model)\n            reward = [torch.tensor(1.0), torch.tensor(0.0)]\n            # train model by running a step twice\n            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n\n            ppo_trainer.model.train()\n            ppo_trainer.model.gradient_checkpointing_enable()\n            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n            break\n\n        # check gradients\n        for name, param in model.named_parameters():\n            if \"lora\" in name or \"v_head\" in name:\n                self.assertTrue(param.grad is not None, f\"Parameter {name} has a no gradient\")\n            else:\n                self.assertTrue(param.grad is None, f\"Parameter {name} has a gradient\")\n\n    @require_peft\n    @mark.peft_test\n    def test_peft_model_ppo_adapter_rm_trainer(self):\n        from peft import LoraConfig, get_peft_model\n        from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification\n\n        dummy_inputs = torch.LongTensor([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]])\n        rm_lora_config = LoraConfig(\n            r=16,\n            lora_alpha=32,\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"SEQ_CLS\",\n        )\n\n        reward_model = AutoModelForSequenceClassification.from_pretrained(self.model_id)\n        reward_model = get_peft_model(reward_model, rm_lora_config)\n        dummy_optim = torch.optim.Adam(filter(lambda p: p.requires_grad, reward_model.parameters()), lr=1e-3)\n\n        previous_rm_logits = reward_model(dummy_inputs).logits\n        loss = previous_rm_logits.mean()\n        loss.backward()\n\n        dummy_optim.step()\n        reward_model.eval()\n\n        original_rm_logits = reward_model(dummy_inputs).logits\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            reward_model.save_pretrained(tmpdirname)\n\n            lora_config = LoraConfig(\n                r=16,\n                lora_alpha=32,\n                lora_dropout=0.05,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n            gpt2_model = AutoModelForCausalLM.from_pretrained(self.model_id)\n\n            # this line is very important\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n\n            gpt2_model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n            peft_model = get_peft_model(gpt2_model, lora_config)\n            model = AutoModelForCausalLMWithValueHead.from_pretrained(\n                peft_model,\n                reward_adapter=tmpdirname,\n            )\n\n            dummy_dataset = self._init_dummy_dataset()\n            self.ppo_config.batch_size = 2\n            self.ppo_config.mini_batch_size = 1\n\n            ppo_trainer = PPOTrainer(\n                config=self.ppo_config,\n                model=model,\n                ref_model=None,\n                tokenizer=self.gpt2_tokenizer,\n                dataset=dummy_dataset,\n            )\n\n            self.assertTrue(ppo_trainer.ref_model is None)\n\n            dummy_dataloader = ppo_trainer.dataloader\n\n            # train model with ppo\n            for query_tensor, response_tensor in dummy_dataloader:\n                # define a reward for response\n                # (this could be any reward such as human feedback or output from another model)\n                reward = [torch.tensor(1.0), torch.tensor(0.0)]\n                # train model by running a step twice\n                _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n\n                ppo_trainer.model.train()\n                ppo_trainer.model.gradient_checkpointing_enable()\n                _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n                break\n\n            new_logits = ppo_trainer.model.compute_reward_score(dummy_inputs)\n            self.assertTrue(not torch.allclose(previous_rm_logits, new_logits[:, -1, :]))\n            self.assertTrue(torch.allclose(original_rm_logits, new_logits[:, -1, :]))\n\n            # check gradients\n            for name, param in model.named_parameters():\n                if (\"lora\" in name or \"v_head\" in name) and (\"reward\" not in name):\n                    self.assertTrue(param.grad is not None, f\"Parameter {name} has a no gradient\")\n                else:\n                    self.assertTrue(param.grad is None, f\"Parameter {name} has a gradient\")\n\n    @unittest.skip(\"Fix by either patching `whomai()` to work in the staging endpoint or use a dummy prod user.\")\n    def test_push_to_hub(self):\n        REPO_NAME = \"test-ppo-trainer\"\n        repo_id = f\"{CI_HUB_USER}/{REPO_NAME}\"\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=self.gpt2_model,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=self._init_dummy_dataset(),\n        )\n        with tempfile.TemporaryDirectory():\n            url = ppo_trainer.push_to_hub(repo_id=repo_id, token=self._token, api_endpoint=CI_HUB_ENDPOINT)\n            # Extract repo_name from the url\n            re_search = re.search(CI_HUB_ENDPOINT + r\"/([^/]+/[^/]+)/\", url)\n            self.assertTrue(re_search is not None)\n            hub_repo_id = re_search.groups()[0]\n            # Check we created a Hub repo\n            self.assertEqual(hub_repo_id, repo_id)\n            # Ensure all files are present\n            files = sorted(self._api.list_repo_files(hub_repo_id))\n            assert all(\n                fnmatch.fnmatch(file, expected_file)\n                for file, expected_file in zip(\n                    files,\n                    [\n                        \".gitattributes\",\n                        \"README.md\",\n                        \"config.json\",\n                        \"merges.txt\",\n                        \"pytorch_model.bin\",\n                        \"special_tokens_map.json\",\n                        \"tokenizer_config.json\",\n                        \"vocab.json\",\n                    ],\n                )\n            )\n\n    @require_peft\n    @require_torch_multi_gpu\n    @mark.peft_test\n    def test_peft_model_ppo_trainer_multi_gpu(self):\n        from peft import LoraConfig, get_peft_model\n        from transformers import AutoModelForCausalLM\n\n        lora_config = LoraConfig(\n            r=16,\n            lora_alpha=32,\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n        gpt2_model = AutoModelForCausalLM.from_pretrained(\n            \"gpt2\", device_map=\"balanced\", max_memory={0: \"500MB\", 1: \"500MB\"}\n        )\n\n        self.assertTrue(set(gpt2_model.hf_device_map.values()) == {0, 1})\n\n        # this line is very important\n        def make_inputs_require_grad(module, input, output):\n            output.requires_grad_(True)\n\n        gpt2_model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n        peft_model = get_peft_model(gpt2_model, lora_config)\n        model = AutoModelForCausalLMWithValueHead.from_pretrained(peft_model)\n\n        self.assertTrue(model.is_sequential_parallel)\n\n        dummy_dataset = self._init_dummy_dataset()\n        self.ppo_config.batch_size = 2\n        self.ppo_config.mini_batch_size = 1\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=model,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n\n        self.assertTrue(ppo_trainer.ref_model is None)\n\n        dummy_dataloader = ppo_trainer.dataloader\n\n        # train model with ppo\n        for query_tensor, response_tensor in dummy_dataloader:\n            # define a reward for response\n            # (this could be any reward such as human feedback or output from another model)\n            reward = [torch.tensor(1.0), torch.tensor(0.0)]\n            # train model by running a step twice\n            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n\n            ppo_trainer.model.train()\n            ppo_trainer.model.gradient_checkpointing_enable()\n            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n            break\n\n        # check gradients\n        for name, param in model.named_parameters():\n            if \"lora\" in name or \"v_head\" in name:\n                self.assertTrue(param.grad is not None, f\"Parameter {name} has a no gradient\")\n            else:\n                self.assertTrue(param.grad is None, f\"Parameter {name} has a gradient\")\n\n    def test_generation(self):\n        dummy_dataset = self._init_dummy_dataset()\n\n        model = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\n        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=model,\n            ref_model=None,\n            tokenizer=tokenizer,\n            dataset=dummy_dataset,\n        )\n\n        input_texts = [\"this is a test\", \"this is another, longer test\"]\n\n        generation_kwargs = {\"do_sample\": False, \"max_new_tokens\": 4, \"pad_token_id\": tokenizer.eos_token_id}\n\n        tokenizer.pad_token = tokenizer.eos_token\n\n        model_inputs = [tokenizer(txt, return_tensors=\"pt\").input_ids.squeeze() for txt in input_texts]\n\n        generations_batched = ppo_trainer.generate(model_inputs, batch_size=2, **generation_kwargs)\n        generations_batched = tokenizer.batch_decode(generations_batched)\n\n        generations_single = [ppo_trainer.generate(inputs, **generation_kwargs).squeeze() for inputs in model_inputs]\n        generations_single = tokenizer.batch_decode(generations_single)\n\n        self.assertEqual(generations_single, generations_batched)\n\n    def test_grad_accumulation(self):\n        dummy_dataset = self._init_dummy_dataset()\n\n        torch.manual_seed(0)\n        gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(self.model_id, summary_dropout_prob=0.0)\n        gpt2_model_clone = copy.deepcopy(gpt2_model)\n\n        self.ppo_config.mini_batch_size = 2\n        self.ppo_config.ppo_epochs = 1\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=gpt2_model,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n\n        dummy_dataloader = ppo_trainer.dataloader\n\n        # train model with ppo\n        for query_tensor, response_tensor in dummy_dataloader:\n            # define a reward for response\n            # (this could be any reward such as human feedback or output from another model)\n            reward = [torch.tensor(1.0), torch.tensor(1.0)]\n            # train model by running a step twice\n            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n            break\n\n        model_grad = gpt2_model.v_head.summary.weight\n\n        self.ppo_config.mini_batch_size = 1\n        self.ppo_config.gradient_accumulation_steps = 2\n\n        ppo_trainer = PPOTrainer(\n            config=self.ppo_config,\n            model=gpt2_model_clone,\n            ref_model=None,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n\n        dummy_dataloader = ppo_trainer.dataloader\n\n        # train model with ppo\n        for query_tensor, response_tensor in dummy_dataloader:\n            # define a reward for response\n            # (this could be any reward such as human feedback or output from another model)\n            reward = [torch.tensor(1.0), torch.tensor(1.0)]\n            # train model by running a step twice\n            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n            break\n\n        model_grad_acc = gpt2_model_clone.v_head.summary.weight\n        self.assertTrue(torch.allclose(model_grad_acc, model_grad, rtol=1e-3, atol=1e-3))\n\n    @unittest.skip(\"Fix by either patching `whomai()` to work in the staging endpoint or use a dummy prod user.\")\n    def test_push_to_hub_if_best_reward(self):\n        REPO_NAME = \"test-ppo-trainer\"\n        repo_id = f\"{CI_HUB_USER}/{REPO_NAME}\"\n\n        dummy_dataset = self._init_dummy_dataset()\n\n        push_to_hub_if_best_kwargs = {\"repo_id\": repo_id}\n\n        ppo_config = PPOConfig(\n            batch_size=2,\n            mini_batch_size=1,\n            log_with=None,\n            push_to_hub_if_best_kwargs=push_to_hub_if_best_kwargs,\n            compare_steps=1,\n        )\n\n        ppo_trainer = PPOTrainer(\n            config=ppo_config,\n            model=self.gpt2_model,\n            ref_model=self.gpt2_model_ref,\n            tokenizer=self.gpt2_tokenizer,\n            dataset=dummy_dataset,\n        )\n\n        dummy_dataloader = ppo_trainer.dataloader\n        # train model with ppo\n        for query_tensor, response_tensor in dummy_dataloader:\n            # define a reward for response\n            # (this could be any reward such as human feedback or output from another model)\n            reward = [torch.tensor(1.0), torch.tensor(0.0)]\n            # train model\n            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)\n            break\n\n    def test_batch_size_check(self):\n        with pytest.raises(ValueError):\n            PPOConfig(batch_size=2, mini_batch_size=2, gradient_accumulation_steps=2)\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_peft_models.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import os\nimport tempfile\nimport unittest\n\nimport torch\nfrom pytest import mark\nfrom transformers import AutoModelForCausalLM\n\nfrom trl import AutoModelForCausalLMWithValueHead, is_peft_available\n\n\nif is_peft_available():\n    from peft import get_peft_model, LoraConfig\n\nfrom .testing_utils import require_bitsandbytes, require_peft\n\n\n@require_peft\n@mark.peft_test\nclass PeftModelTester(unittest.TestCase):\n    def setUp(self):\n        self.causal_lm_model_id = \"trl-internal-testing/tiny-random-GPTNeoXForCausalLM\"\n        self.lora_config = LoraConfig(\n            r=16,\n            lora_alpha=32,\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n\n    def test_create_peft_model(self):\n        r\"\"\"\n        Simply creates a peft model and checks that it can be loaded.\n        \"\"\"\n        causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)\n        pretrained_model = get_peft_model(causal_lm_model, self.lora_config)\n\n        _ = AutoModelForCausalLMWithValueHead.from_pretrained(pretrained_model)\n\n    def test_peft_requires_grad(self):\n        r\"\"\"\n        Check that the value head of the returned model has requires_grad=True.\n        \"\"\"\n        causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)\n        pretrained_model = get_peft_model(causal_lm_model, self.lora_config)\n\n        model = AutoModelForCausalLMWithValueHead.from_pretrained(pretrained_model)\n\n        # Check that the value head has requires_grad=True\n        self.assertTrue(model.v_head.summary.weight.requires_grad)\n\n    def test_check_peft_model_nb_trainable_params(self):\n        r\"\"\"\n        Check that the number of trainable parameters is correct.\n        \"\"\"\n        causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)\n        pretrained_model = get_peft_model(causal_lm_model, self.lora_config)\n\n        model = AutoModelForCausalLMWithValueHead.from_pretrained(pretrained_model)\n\n        # Check that the number of trainable parameters is correct\n        nb_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        self.assertEqual(nb_trainable_params, 10273)\n\n        # Check that the number of trainable param for the non-peft model is correct\n        non_peft_model = AutoModelForCausalLMWithValueHead.from_pretrained(self.causal_lm_model_id)\n        nb_trainable_params = sum(p.numel() for p in non_peft_model.parameters() if p.requires_grad)\n        self.assertEqual(nb_trainable_params, 99578)\n\n    def test_create_peft_model_from_config(self):\n        r\"\"\"\n        Simply creates a peft model and checks that it can be loaded.\n        \"\"\"\n        trl_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n            self.causal_lm_model_id, peft_config=self.lora_config\n        )\n        # Check that the number of trainable parameters is correct\n        nb_trainable_params = sum(p.numel() for p in trl_model.parameters() if p.requires_grad)\n        self.assertEqual(nb_trainable_params, 10273)\n\n        causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)\n        trl_model = AutoModelForCausalLMWithValueHead.from_pretrained(causal_lm_model, peft_config=self.lora_config)\n        # Check that the number of trainable parameters is correct\n        nb_trainable_params = sum(p.numel() for p in trl_model.parameters() if p.requires_grad)\n        self.assertEqual(nb_trainable_params, 10273)\n\n    @require_bitsandbytes\n    def test_create_bnb_peft_model_from_config(self):\n        r\"\"\"\n        Simply creates a peft model and checks that it can be loaded.\n        \"\"\"\n        from bitsandbytes.nn import Linear8bitLt\n\n        trl_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n            self.causal_lm_model_id, peft_config=self.lora_config, load_in_8bit=True\n        )\n        # Check that the number of trainable parameters is correct\n        nb_trainable_params = sum(p.numel() for p in trl_model.parameters() if p.requires_grad)\n        self.assertEqual(nb_trainable_params, 10273)\n        self.assertTrue(\n            trl_model.pretrained_model.model.gpt_neox.layers[0].mlp.dense_h_to_4h.__class__ == Linear8bitLt\n        )\n\n        causal_lm_model = AutoModelForCausalLM.from_pretrained(\n            self.causal_lm_model_id, load_in_8bit=True, device_map=\"auto\"\n        )\n        trl_model = AutoModelForCausalLMWithValueHead.from_pretrained(causal_lm_model, peft_config=self.lora_config)\n        # Check that the number of trainable parameters is correct\n        nb_trainable_params = sum(p.numel() for p in trl_model.parameters() if p.requires_grad)\n        self.assertEqual(nb_trainable_params, 10273)\n        self.assertTrue(\n            trl_model.pretrained_model.model.gpt_neox.layers[0].mlp.dense_h_to_4h.__class__ == Linear8bitLt\n        )\n\n    def test_save_pretrained_peft(self):\n        r\"\"\"\n        Check that the model can be saved and loaded properly.\n        \"\"\"\n        causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)\n        pretrained_model = get_peft_model(causal_lm_model, self.lora_config)\n\n        model = AutoModelForCausalLMWithValueHead.from_pretrained(pretrained_model)\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            model.save_pretrained(tmp_dir)\n\n            # check that the files `adapter_model.safetensors` and `adapter_config.json` are in the directory\n            self.assertTrue(\n                os.path.isfile(f\"{tmp_dir}/adapter_model.safetensors\"),\n                msg=f\"{tmp_dir}/adapter_model.safetensors does not exist\",\n            )\n            self.assertTrue(\n                os.path.exists(f\"{tmp_dir}/adapter_config.json\"),\n                msg=f\"{tmp_dir}/adapter_config.json does not exist\",\n            )\n            # check also for `pytorch_model.bin` and make sure it only contains `v_head` weights\n            self.assertTrue(\n                os.path.exists(f\"{tmp_dir}/pytorch_model.bin\"),\n                msg=f\"{tmp_dir}/pytorch_model.bin does not exist\",\n            )\n            maybe_v_head = torch.load(f\"{tmp_dir}/pytorch_model.bin\")\n            # check that only keys that starts with `v_head` are in the dict\n            self.assertTrue(\n                all(k.startswith(\"v_head\") for k in maybe_v_head.keys()),\n                msg=f\"keys in {tmp_dir}/pytorch_model.bin do not start with `v_head`\",\n            )\n\n            model_from_pretrained = AutoModelForCausalLMWithValueHead.from_pretrained(tmp_dir)\n\n            # check all the weights are the same\n            for p1, p2 in zip(model.named_parameters(), model_from_pretrained.named_parameters()):\n                self.assertTrue(torch.allclose(p1[1], p2[1]), msg=f\"{p1[0]} != {p2[0]}\")\n\n    def test_load_pretrained_peft(self):\n        r\"\"\"\n        Check that the model saved with peft class interface can be loaded properly.\n        \"\"\"\n        causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)\n        pretrained_model = get_peft_model(causal_lm_model, self.lora_config)\n\n        model = AutoModelForCausalLMWithValueHead.from_pretrained(pretrained_model)\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            pretrained_model.save_pretrained(tmp_dir)\n            model_from_pretrained = AutoModelForCausalLMWithValueHead.from_pretrained(tmp_dir)\n\n            # check that the files `adapter_model.safetensors` and `adapter_config.json` are in the directory\n            self.assertTrue(\n                os.path.isfile(f\"{tmp_dir}/adapter_model.safetensors\"),\n                msg=f\"{tmp_dir}/adapter_model.safetensors does not exist\",\n            )\n            self.assertTrue(\n                os.path.exists(f\"{tmp_dir}/adapter_config.json\"),\n                msg=f\"{tmp_dir}/adapter_config.json does not exist\",\n            )\n\n            # check all the weights are the same\n            for p1, p2 in zip(model.named_parameters(), model_from_pretrained.named_parameters()):\n                if p1[0] not in [\"v_head.summary.weight\", \"v_head.summary.bias\"]:\n                    self.assertTrue(torch.allclose(p1[1], p2[1]), msg=f\"{p1[0]} != {p2[0]}\")\n\n    def test_continue_training_peft_model(self):\n        r\"\"\"\n        Load peft and checks that it can continue training.\n        \"\"\"\n        causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)\n        pretrained_model = get_peft_model(causal_lm_model, self.lora_config)\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            pretrained_model.save_pretrained(tmp_dir)\n            # set is_trainable to True\n            model = AutoModelForCausalLMWithValueHead.from_pretrained(tmp_dir, is_trainable=True)\n            # Check that the number of trainable parameters is correct\n            nb_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n            self.assertEqual(nb_trainable_params, 10273)\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/test_core.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import unittest\n\nimport torch\n\nfrom trl.core import masked_mean, masked_var, masked_whiten, whiten\n\n\nclass CoreTester(unittest.TestCase):\n    \"\"\"\n    A wrapper class for testing core utils functions\n    \"\"\"\n\n    @classmethod\n    def setUpClass(cls):\n        cls.test_input = torch.Tensor([1, 2, 3, 4])\n        cls.test_mask = torch.Tensor([0, 1, 1, 0])\n        cls.test_input_unmasked = cls.test_input[1:3]\n\n    def test_masked_mean(self):\n        self.assertEqual(torch.mean(self.test_input_unmasked), masked_mean(self.test_input, self.test_mask))\n\n    def test_masked_var(self):\n        self.assertEqual(torch.var(self.test_input_unmasked), masked_var(self.test_input, self.test_mask))\n\n    def test_masked_whiten(self):\n        whiten_unmasked = whiten(self.test_input_unmasked)\n        whiten_masked = masked_whiten(self.test_input, self.test_mask)[1:3]\n        diffs = (whiten_unmasked - whiten_masked).sum()\n        self.assertAlmostEqual(diffs, 0)\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/testing_utils.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import unittest\n\nimport torch\n\nfrom trl import (\n    is_bitsandbytes_available,\n    is_diffusers_available,\n    is_peft_available,\n    is_wandb_available,\n    is_xpu_available,\n)\n\n\ndef require_peft(test_case):\n    \"\"\"\n    Decorator marking a test that requires peft. Skips the test if peft is not available.\n    \"\"\"\n    if not is_peft_available():\n        test_case = unittest.skip(\"test requires peft\")(test_case)\n    return test_case\n\n\ndef require_bitsandbytes(test_case):\n    \"\"\"\n    Decorator marking a test that requires bnb. Skips the test if bnb is not available.\n    \"\"\"\n    if not is_bitsandbytes_available():\n        test_case = unittest.skip(\"test requires bnb\")(test_case)\n    return test_case\n\n\ndef require_diffusers(test_case):\n    \"\"\"\n    Decorator marking a test that requires diffusers. Skips the test if diffusers is not available.\n    \"\"\"\n    if not is_diffusers_available():\n        test_case = unittest.skip(\"test requires diffusers\")(test_case)\n    return test_case\n\n\ndef require_wandb(test_case, required: bool = True):\n    \"\"\"\n    Decorator marking a test that requires wandb. Skips the test if wandb is not available.\n    \"\"\"\n    # XOR, i.e.:\n    # skip if available and required = False and\n    # skip if not available and required = True\n    if is_wandb_available() ^ required:\n        test_case = unittest.skip(\"test requires wandb\")(test_case)\n    return test_case\n\n\ndef require_no_wandb(test_case):\n    \"\"\"\n    Decorator marking a test that requires no wandb. Skips the test if wandb is available.\n    \"\"\"\n    return require_wandb(test_case, required=False)\n\n\ndef require_torch_multi_gpu(test_case):\n    \"\"\"\n    Decorator marking a test that requires multiple GPUs. Skips the test if there aren't enough GPUs.\n    \"\"\"\n    if torch.cuda.device_count() < 2:\n        test_case = unittest.skip(\"test requires multiple GPUs\")(test_case)\n    return test_case\n\n\ndef require_torch_multi_xpu(test_case):\n    \"\"\"\n    Decorator marking a test that requires multiple XPUs. Skips the test if there aren't enough XPUs.\n    \"\"\"\n    if torch.xpu.device_count() < 2 and is_xpu_available():\n        test_case = unittest.skip(\"test requires multiple XPUs\")(test_case)\n    return test_case\n"
    },
    {
        "file": "kwaai-pai/app/trl/tests/testing_constants.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\nCI_HUB_USER = \"__DUMMY_TRANSFORMERS_USER__\"\nCI_HUB_USER_FULL_NAME = \"Dummy User\"\nCI_HUB_USER_TOKEN = \"hf_94wBhPGp6KrrTH3KDchhKpRxZwd6dmHWLL\"\n\nCI_HUB_ENDPOINT = \"https://hub-ci.huggingface.co\"\n"
    },
    {
        "file": "kwaai-pai/app/app/wsgi.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"\nWSGI config for app project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/4.0/howto/deployment/wsgi/\n\"\"\"\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'app.settings')\n\napplication = get_wsgi_application()\n"
    },
    {
        "file": "kwaai-pai/app/app/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/app/urls.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"app URL Configuration\n\nThe `urlpatterns` list routes URLs to views. For more information please see:\n    https://docs.djangoproject.com/en/4.0/topics/http/urls/\nExamples:\nFunction views\n    1. Add an import:  from my_app import views\n    2. Add a URL to urlpatterns:  path('', views.home, name='home')\nClass-based views\n    1. Add an import:  from other_app.views import Home\n    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')\nIncluding another URLconf\n    1. Import the include() function: from django.urls import include, path\n    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))\n\"\"\"\nfrom django.contrib import admin\nfrom django.urls import path, include\nfrom drf_spectacular.views import (\n    SpectacularAPIView,\n    SpectacularSwaggerView,\n)\n\nurlpatterns = [\n    path('admin/', admin.site.urls),\n    path('api/schema/', SpectacularAPIView.as_view(), name='api-schema'),\n    path(\n        'api/docs/',\n        SpectacularSwaggerView.as_view(url_name='api-schema'),\n        name='api-docs',\n    ),\n    path('api/llm/', include('llm.api.urls')),\n    path('api/sentbox-email/', include('sentbox_email.api.urls')),\n    path('api/inbox-email/', include('inbox_email.api.urls')),\n    path('api/rag-add/', include('rag_add.api.urls')),\n    path('api/rag-chat/', include('rag_chat.api.urls')),\n]\n"
    },
    {
        "file": "kwaai-pai/app/app/asgi.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"\nASGI config for app project.\n\nIt exposes the ASGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/4.0/howto/deployment/asgi/\n\"\"\"\n\nimport os\n\nfrom django.core.asgi import get_asgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'app.settings')\n\napplication = get_asgi_application()\n"
    },
    {
        "file": "kwaai-pai/app/app/settings.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"\nDjango settings for app project.\n\nGenerated by 'django-admin startproject' using Django 4.0.10.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/4.0/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/4.0/ref/settings/\n\"\"\"\n\nfrom pathlib import Path\nimport os\nfrom celery.schedules import crontab\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/4.0/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'django-insecure-3o4m&km2df6+x-gr4c)f4*g-=b(&u53oi!28sp7=&05*6!vkeu'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = ['*']\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n\n    'rest_framework',\n    'rest_framework.authtoken',\n    'drf_spectacular',\n    'corsheaders',\n\n    'core',\n    'job',\n    'llm',\n    'inbox_email',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'app.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'app.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/4.0/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql',\n        'NAME': os.environ.get('DB_NAME', 'devdb'),\n        'USER': os.environ.get('DB_USER', 'devuser'),\n        'PASSWORD': os.environ.get('DB_PASS', 'changeme'),\n        'HOST': os.environ.get('DB_HOST', 'db'),\n        'PORT': os.environ.get('DB_PORT', '5432'),\n    }\n}\n\nINBOX_DB_CONFIG = { \n    \"host\": \"db\",\n    \"port\": \"5432\",\n    \"dbname\": \"devdb\",\n    \"user\": \"devuser\",\n    \"password\": \"changeme\",   \n    \n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/4.0/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/4.0/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/4.0/howto/static-files/\n\nSTATIC_URL = '/static/static/'\nMEDIA_URL = '/static/media/'\n\nMEDIA_ROOT = '/vol/web/media'\nSTATIC_ROOT = '/vol/web/static'\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/4.0/ref/settings/#default-auto-field\n\nREST_FRAMEWORK = {\n    'DEFAULT_SCHEMA_CLASS': 'drf_spectacular.openapi.AutoSchema',\n}\n\nSPECTACULAR_SETTINGS = {\n    'COMPONENT_SPLIT_REQUEST': True,\n    'TITLE': 'Kwaai PAI',\n    'DESCRIPTION': 'API for Kwaai PAI',\n    'VERSION': '1.0.0',\n    'SERVE_INCLUDE_SCHEMA': False,\n    'SWAGGER_UI_SETTINGS': {\n        'deepLinking': True,\n    },\n}\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n\nCELERY_BROKER_URL = 'redis://redis:6379/0'\nCELERY_RESULT_BACKEND = 'redis://redis:6379/0'\nCELERY_ACCEPT_CONTENT = ['json']\nCELERY_TASK_SERIALIZER = 'json'\nCELERY_RESULT_SERIALIZER = 'json'\nCELERY_TIMEZONE = 'UTC'\nCELERY_BEAT_SCHEDULE = {\n    'process_new_emails': {\n        'task': 'job.tasks.process_unseen_emails',\n        'schedule': crontab(minute='*/1'),\n    },\n}"
    },
    {
        "file": "kwaai-pai/app/core/admin.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from django.contrib import admin\n\n# Register your models here.\n"
    },
    {
        "file": "kwaai-pai/app/core/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/core/models.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from django.conf import settings\nfrom django.db import models\nfrom django.utils.translation import gettext_lazy as _\n\n    \nclass ImapCredentials(models.Model):\n    \"\"\"Email Credentials model.\"\"\"\n    email = models.EmailField(primary_key=True)\n    password = models.CharField(max_length=255)\n    imap_server = models.CharField(max_length=255)\n\n    def __str__(self):\n        return self.email\n    \n\nclass ImapEmail(models.Model):\n    \"\"\"Email model.\"\"\"\n    subject = models.CharField(max_length=255)\n    from_email = models.CharField(max_length=255)\n    timestamp = models.CharField(max_length=255)\n    body = models.TextField()\n\n    def __str__(self):\n        return self.subject\n    \nclass LLMResponse(models.Model):\n    \"\"\"LLM Response model.\"\"\"\n    prompt = models.TextField()\n    timestamp = models.DateTimeField(auto_now_add=True)\n\n    def __str__(self):\n        return self.response\n    \nclass InboxEmail(models.Model):\n    \"\"\"Inbox Email model.\"\"\"\n    id = models.CharField(max_length=255, primary_key=True)\n    subject = models.CharField(max_length=255)\n    sender = models.CharField(max_length=255)\n    date = models.CharField(max_length=255)\n    body = models.TextField()\n    message_id = models.CharField(max_length=255)\n\n    def __str__(self):\n        return self.id\n    \nclass RagChat(models.Model):\n    \"\"\"RAG Chat model.\"\"\"\n    prompt = models.TextField()\n    timestamp = models.DateTimeField(auto_now_add=True)\n \n    def __str__(self):\n        return self.prompt\n"
    },
    {
        "file": "kwaai-pai/app/core/tests.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from django.test import TestCase\n\n# Create your tests here.\n"
    },
    {
        "file": "kwaai-pai/app/core/views.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from django.shortcuts import render\n\n# Create your views here.\n"
    },
    {
        "file": "kwaai-pai/app/core/apps.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from django.apps import AppConfig\n\n\nclass CoreConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'core'\n"
    },
    {
        "file": "kwaai-pai/app/core/migrations/0001_initial.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# Generated by Django 4.0.10 on 2024-01-31 23:27\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='ImapCredentials',\n            fields=[\n                ('email', models.EmailField(max_length=254, primary_key=True, serialize=False)),\n                ('password', models.CharField(max_length=255)),\n                ('imap_server', models.CharField(max_length=255)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='ImapEmail',\n            fields=[\n                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('subject', models.CharField(max_length=255)),\n                ('from_email', models.CharField(max_length=255)),\n                ('timestamp', models.CharField(max_length=255)),\n                ('body', models.TextField()),\n            ],\n        ),\n        migrations.CreateModel(\n            name='LLMResponse',\n            fields=[\n                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('prompt', models.TextField()),\n                ('timestamp', models.DateTimeField(auto_now_add=True)),\n            ],\n        ),\n    ]\n"
    },
    {
        "file": "kwaai-pai/app/core/migrations/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/core/migrations/0002_inboxemail.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "# Generated by Django 4.0.10 on 2024-02-23 00:00\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('core', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='InboxEmail',\n            fields=[\n                ('id', models.CharField(max_length=255, primary_key=True, serialize=False)),\n                ('subject', models.CharField(max_length=255)),\n                ('sender', models.CharField(max_length=255)),\n                ('date', models.CharField(max_length=255)),\n                ('body', models.TextField()),\n                ('message_id', models.CharField(max_length=255)),\n            ],\n        ),\n    ]\n"
    },
    {
        "file": "kwaai-pai/app/core/management/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/core/management/commands/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/core/management/commands/wait_for_db.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"\nDjango command to wait for the database to be available.\n\"\"\"\nimport time\n\nfrom psycopg2 import OperationalError as Psycopg2OpError\n\nfrom django.db.utils import OperationalError\nfrom django.core.management.base import BaseCommand\n\n\nclass Command(BaseCommand):\n    \"\"\"Django command to wait for database.\"\"\"\n\n    def handle(self, *args, **options):\n        \"\"\"Entrypoint for command.\"\"\"\n        self.stdout.write('Waiting for database...')\n        db_up = False\n        while db_up is False:\n            try:\n                self.check(databases=['default'])\n                db_up = True\n            except (Psycopg2OpError, OperationalError):\n                self.stdout.write(self.style.WARNING('Database unavailable, waiting 1 second...'))\n                time.sleep(1)\n\n        self.stdout.write(self.style.SUCCESS('Database available!'))"
    },
    {
        "file": "kwaai-pai/app/rag_add/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/rag_add/apps.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from django.apps import AppConfig\n\n\nclass RagAddConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'rag_add'\n"
    },
    {
        "file": "kwaai-pai/app/rag_add/api/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/rag_add/api/views.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from rest_framework import status, generics\nfrom rest_framework.response import Response\nimport logging\nfrom utilities.add_rag_source import add_rag_source\nimport os\n\nclass RAGAddView(generics.GenericAPIView):\n    \"\"\"   \n        Handle POST Adds a new source to the Embedchain app.\n                            \n    \"\"\"\n    \n    def post(self, request,*args, **kwargs):\n        try:\n            path = 'utilities/'\n            csv_files = [f for f in os.listdir(path) if f.startswith('inbox_') and f.endswith('.csv')]           \n\n            if len(csv_files) > 0:\n                 source =add_rag_source(path + csv_files[-1])\n            return Response(source, status=status.HTTP_200_OK)   \n        except Exception as e:            \n            logging.exception(\"Unexpected error occurred when adding RAG resources.\")\n            return Response({\"detail\": \" An unexpected error occurred, \" + str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR) \n\n"
    },
    {
        "file": "kwaai-pai/app/rag_add/api/urls.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"URLs for RAG Add API handler\"\"\"\nfrom django.urls import path\nfrom rag_add.api.views import RAGAddView\n\nurlpatterns = [\n    path('', RAGAddView.as_view(), name='rag'),\n]"
    },
    {
        "file": "kwaai-pai/app/rag_chat/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/rag_chat/apps.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from django.apps import AppConfig\n\n\nclass RagChatConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'rag_chat'\n"
    },
    {
        "file": "kwaai-pai/app/rag_chat/api/serializers.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from rest_framework import serializers\nfrom core.models import RagChat\n \nclass RagSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = RagChat\n        fields = '__all__'\n        read_only_fields = ('id',)"
    },
    {
        "file": "kwaai-pai/app/rag_chat/api/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/rag_chat/api/views.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from rest_framework import status, generics\nfrom rest_framework.response import Response\nimport logging\nfrom utilities.get_rag_response import get_rag_response\nfrom rag_chat.api.serializers import RagSerializer\n\nclass RAGChatView(generics.GenericAPIView):\n    \"\"\"  \n        Handle POST requests for RAG chat. \n        Returns: rag_response     \n    \"\"\"\n    serializer_class = RagSerializer\n \n    def post(self, request, *args, **kwargs):  \n        serializer = self.get_serializer(data=request.data)\n        try:\n            serializer.is_valid(raise_exception=True)\n            validated_data = serializer.validated_data\n            rag_response = get_rag_response(validated_data['prompt'])\n            return Response(rag_response, status=status.HTTP_200_OK)\n          \n        except Exception as e:            \n            logging.exception(\"Unexpected error occurred when adding RAG resources.\")\n            return Response({\"detail\": \" An unexpected error occurred, \" + str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)"
    },
    {
        "file": "kwaai-pai/app/rag_chat/api/urls.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"URLs for RAG Add API handler\"\"\"\nfrom django.urls import path\nfrom rag_chat.api.views import RAGChatView\n\nurlpatterns = [\n    path('', RAGChatView.as_view(), name='rag'),\n]"
    },
    {
        "file": "kwaai-pai/app/utilities/add_rag_source.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from utilities.config import  MODEL_RAG\nfrom utilities.rag_handler import RAGHandler\n\n\n\ndef add_rag_source(path) -> str:\n    rag_manager = RAGHandler(MODEL_RAG)\n    rag_manager.add_source_csv(path)"
    },
    {
        "file": "kwaai-pai/app/utilities/imap_email_handler.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from imaplib import IMAP4_SSL\nfrom email import message_from_bytes\nfrom email.utils import parsedate_to_datetime\nimport re\n\n\nclass ImapEmailHandler:   \n    def __init__(self, user, password, imap_server='imap.gmail.com') -> None:\n        self.user = user\n        self.password = password\n        self.imap_server = imap_server\n        self.mail = IMAP4_SSL(imap_server)\n        self.mail.login(user, password)\n        self.mail.select('Inbox')\n\n    def search_emails(self, key, value = None ):\n        _, data = self.mail.search(None, key, value)\n        mail_id_list = data[0].split()\n        return mail_id_list\n\n    def fetch_emails(self, mail_id_list):\n        msgs = []\n        for num in mail_id_list:\n            _, data = self.mail.fetch(num, '(RFC822)')\n            msgs.append(data)\n        return msgs\n    \n    def preprocess_body(self, body: str) -> str:\n        body = re.sub(r'\\S+@\\S+', 'EMAIL', body)\n        body = re.sub(r'\\d{10}', 'PHONE', body)\n        body = body.lower()\n        body = re.sub(r'\\d', 'NUM', body)\n        body = re.sub(r'[^a-z0-9\\s]', ' ', body)\n        return body\n    \n    def process_emails(self, emails_data: list) -> list:\n        processed_emails = []\n\n        for email_data in emails_data:\n            raw_email = email_data[0][1]\n            msg = message_from_bytes(raw_email)\n\n            subject = msg.get('Subject', '')\n            sender = msg.get('From', '')\n            date_str = msg.get('Date', '')\n\n            date = parsedate_to_datetime(date_str) if date_str else None\n\n            body = self.extract_body(msg)\n            body = self.preprocess_body(body)\n\n            processed_email = {\n                'Subject': subject,\n                'From': sender,\n                'Date': date,\n                'Body': body,\n            }\n\n            processed_emails.append(processed_email)\n\n        return processed_emails\n\n    def extract_body(self, msg):\n        for part in msg.walk():\n            if part.get_content_type() == 'text/plain':\n                return part.get_payload()\n\n        return ''\n    \n    def processing_emails_label(self, mail_id_list):\n        for num in mail_id_list:\n            self.mail.copy(num, 'Processing...')"
    },
    {
        "file": "kwaai-pai/app/utilities/config.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import os\n\nMODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\nMODEL_SPAM_FILTER = \"NotShrirang/albert-spam-filter\"\nMODEL_TAGGING = \"gpt-3.5-turbo\"\nMODEL_RAG = \"utilities/mistral.yaml\"\nRAG_SOURCE = \"utilities/emails.csv\"\n\n\nEMAILS_DATA_SET_PATH = './sent_box_emails.csv'\nKEYWORD = 'FROM'\n\nEMAIL_CREDENTIALS = {\n    \"email\": os.environ.get(\"IMAP_EMAIL\"),\n    \"password\": os.environ.get(\"IMAP_PASSWORD\"),\n    \"imap_server\": os.environ.get(\"IMAP_SERVER\"),\n}\n\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\nHUGGINGFACE_ACCESS_TOKEN = os.environ.get(\"HUGGINGFACE_ACCESS_TOKEN\")\n\nEC_APP_CONFIG = {\n    \"app\": {\n        \"config\": {\n            \"id\": \"embedchain-demo-app\",\n        }\n    },\n    \"llm\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"gpt-3.5-turbo-1106\",\n        }\n    }\n}"
    },
    {
        "file": "kwaai-pai/app/utilities/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/utilities/get_rag_response.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from utilities.config import MODEL_RAG\nfrom utilities.rag_handler import RAGHandler\n\n\ndef get_rag_response(question) -> str:\n    rag_manager = RAGHandler(MODEL_RAG)\n    response = rag_manager.get_rag_response(question)\n    return response\n    "
    },
    {
        "file": "kwaai-pai/app/utilities/create_email_draft.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"This module creates an email draft using the LLM response model\"\"\"\nfrom utilities.config import EMAIL_CREDENTIALS\nfrom utilities.imap_draft_handler import ImapDraftHandler\nfrom utilities.llm_response import stream_response_and_concatenate\n\ndef create_email_draft(\n    id: str,    \n    message_id: str,\n    to_address: str,\n    subject: str,\n    prompt: str,\n) -> str:\n    \"\"\"Create an email draft using llm response model\"\"\"\n    draft_manager = ImapDraftHandler(\n        EMAIL_CREDENTIALS['email'],\n        EMAIL_CREDENTIALS['password'],\n        EMAIL_CREDENTIALS['imap_server']\n    )\n\n    # llm_response = stream_response_and_concatenate(prompt)\n\n\n    draft_manager.login()\n    draft_manager.select_drafts_mailbox()\n    response = draft_manager.create_draft(message_id, to_address, subject, prompt)\n    draft_manager.remove_genrating_label(id)\n    draft_manager.logout()\n\n    return response"
    },
    {
        "file": "kwaai-pai/app/utilities/imap_draft_handler.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import imaplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom utilities.config import EMAIL_CREDENTIALS\nimport time\n\nIMAP_LIST_COMMAND = \"[Gmail]/Drafts\"\nclass ImapDraftHandler:\n    \n    def __init__(\n            self, \n            email_address=EMAIL_CREDENTIALS['email'], \n            password=EMAIL_CREDENTIALS['password'],\n            imap_server=EMAIL_CREDENTIALS['imap_server']\n        ):\n        self.email_address = email_address\n        self.password = password\n        self.mail = imaplib.IMAP4_SSL(imap_server)\n\n    def login(self):\n        self.mail.login(self.email_address, self.password)\n\n    def select_drafts_mailbox(self):\n        self.mail.select(IMAP_LIST_COMMAND)\n\n    def create_draft(self, message_id, to_address, subject, body):\n        message = MIMEMultipart()\n        message['From'] = self.email_address\n        message['To'] = to_address\n        message['Subject'] = subject\n        message['In-Reply-To'] = message_id\n\n        message.attach(MIMEText(body, 'plain'))\n        message.attach(MIMEText(f\"\"\"\n        <strong>\n        <br><br>\n        <p style=\"color:blue;\">This email was automatically generated by Kwaai PAI.</p>\n        </strong>\n        \"\"\", 'html'))\n        message_bytes = message.as_bytes()       \n\n        _, encoded_message = self.mail.append(IMAP_LIST_COMMAND, '', imaplib.Time2Internaldate(time.time()), message_bytes.decode('utf-8', 'ignore').encode('utf-8'))\n        \n        return encoded_message\n    \n    def logout(self):\n        self.mail.logout()\n\n    def remove_genrating_label(self, id):\n        self.mail.select('Inbox')\n        self.mail.store(id, '-X-GM-LABELS', 'Generating...')\n"
    },
    {
        "file": "kwaai-pai/app/utilities/apps.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from django.apps import AppConfig\n\n\nclass UtilitiesConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'utilities'\n"
    },
    {
        "file": "kwaai-pai/app/utilities/get_unseen_emails.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from utilities.config import EMAIL_CREDENTIALS\nfrom utilities.imap_email_handler import ImapEmailHandler\nfrom utilities.imap_inbox_handler import ImapInboxHandler\nimport logging\n\nDIRECTORIES = ['Personal', 'Professional', 'Processing...','Generating...', 'Undefined']\nWORDS = ['no-reply', 'unsubscribe', 'noreply','-noreply','no_reply', 'not reply']\nEMAIL_DOMAINS =['@gmail.com', '@yahoo.com', '@hotmail.com', '@outlook.com']\n\ndef get_unseen_emails() -> list:\n    \"\"\"This function checks for unseen emails, filters them\n       and tags them. Then it returns a list of dictionaries\n       representing the unseen emails with the keys of:\n       'Id', 'Subject', 'From', 'Date', 'Body' and 'Message-ID'.\n    \"\"\"\n    try:\n        email = EMAIL_CREDENTIALS['email']\n        password = EMAIL_CREDENTIALS['password']\n        imap_server = EMAIL_CREDENTIALS['imap_server']\n\n        email_manager = ImapEmailHandler(email, password, imap_server)\n        email_inbox_manager = ImapInboxHandler(email, password, imap_server)\n        email_inbox_manager.create_directories(DIRECTORIES)\n        \n        unseen_mail_ids = email_manager.search_emails('UNSEEN')\n        email_manager.processing_emails_label(unseen_mail_ids)\n        if unseen_mail_ids:\n            msgs_unformatted = email_manager.fetch_emails(unseen_mail_ids)\n            donot_reply_mail_ids = email_inbox_manager.get_donot_reply_emails(msgs_unformatted, WORDS)\n            reply_email_ids = email_inbox_manager.filter_emails_by_domain(msgs_unformatted, EMAIL_DOMAINS)\n            df_unseen_emails = email_inbox_manager.create_df_unseen_emails(msgs_unformatted)\n\n            result_list, df_final, df_final_to_csv = email_inbox_manager.filter_unseen_emails(df_unseen_emails, donot_reply_mail_ids, reply_email_ids)\n            email_inbox_manager.create_csv(df_final_to_csv)\n            # email_inbox_manager.create_inbox_db(df_final_to_csv)\n            email_inbox_manager.tag_emails(df_final, DIRECTORIES)        \n            logging.info(\"result_list:\",result_list)\n            return result_list\n        else:\n            return ([])\n    except Exception as e:\n        logging.exception(\"Unexpected error occurred when checking unseen emails.\")\n        return ({\"detail\": \" An unexpected error occurred, \" + str(e)}) \n        "
    },
    {
        "file": "kwaai-pai/app/utilities/imap_inbox_handler.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\nfrom datetime import datetime\nfrom imaplib import IMAP4_SSL\n\nfrom email import message_from_bytes\nfrom email.header import decode_header\nimport polars as pl\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nimport torch.nn.functional as F\nimport logging\nfrom rest_framework.response import Response\nfrom rest_framework import status, generics\nfrom utilities.config import MODEL_SPAM_FILTER, OPENAI_API_KEY, MODEL_TAGGING\nfrom core.models import InboxEmail\n\nfrom openai import OpenAI\n\nCLIENT = OpenAI(api_key= OPENAI_API_KEY)\n\nclass ImapInboxHandler:\n    def __init__(\n            self, \n            user, \n            password, \n            imap_server = 'imap.gmail.com'\n        ) -> None:\n        self.user = user\n        self.password = password\n        self.imap_server = imap_server\n        self.mail = IMAP4_SSL(imap_server)\n        self.mail.login(user, password)\n        self.mail.select('Inbox')\n        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_SPAM_FILTER)\n        self.model = AutoModelForSequenceClassification.from_pretrained(MODEL_SPAM_FILTER, from_tf=True)\n        self.mail.select(mailbox='Inbox', readonly=False)\n        \n\n    def extract_email_info(self, msg_data):\n        \"\"\"\n        This function extracts the email information from the email message.        \n        Args:\n            msg_data (tuple): Tuple of email message data.            \n        Returns:\n            id (str): Email message id.\n            subject (str): Email message subject.\n            sender (str): Email message sender.\n            date (str): Email message date.\n            body (str): Email message body.\n            message_id (str): Email Message ID.\n        \"\"\"\n        msg_bytes = msg_data[0][1]\n        msg = message_from_bytes(msg_bytes)\n        \n        id = msg_data[0][0].decode().split()[0]\n\n        subject, encoding = decode_header(msg.get('Subject', 'No Subject'))[0]\n        if isinstance(subject, bytes):\n            subject = subject.decode(encoding or 'utf-8')\n\n        sender, encoding = decode_header(msg.get('From', 'No Sender'))[0]\n        if isinstance(sender, bytes):\n            sender = sender.decode(encoding or 'utf-8')\n\n        message_id, encoding = decode_header(msg.get('Message-ID', 'No Message ID'))[0]\n        if isinstance(message_id, bytes):\n            message_id = message_id.decode(encoding or 'utf-8')            \n\n        date = msg.get('Date', 'No Date')\n\n        body = 'No Body' \n        if msg.is_multipart():\n            for part in msg.walk():\n                if part.get_content_type() == 'text/plain':\n                    body = part.get_payload(decode=True).decode(part.get_content_charset() or 'utf-8').lower()\n                    break\n        else:\n            body = msg.get_payload(decode=True).decode(msg.get_content_charset() or 'utf-8')\n\n        return id, subject, sender, date, body, message_id    \n    \n    \n    def create_df_unseen_emails(self, unseen_msgs): \n        \"\"\"\n        This function creates a polars DataFrame of unseen emails.        \n        Args:\n            unseen_msgs (list): List of unseen emails.            \n        Returns:\n            df_unseen_emails (pl.DataFrame): DataFrame of unseen emails.\n        \"\"\"       \n        data_list = []\n        try:\n            if not unseen_msgs:\n                logging.info(\"No unseen emails\")\n                df_unseen_emails = pl.DataFrame()\n                return  df_unseen_emails \n            else:\n                for msg in unseen_msgs:\n                    id, subject, sender, date, body, message_id = self.extract_email_info(msg)                                       \n                    data_list.append({'Id': id, 'Subject': subject, 'Sender': sender, 'Date': date, 'Body': body, 'Message-ID': message_id})\n                            \n                df_unseen_emails = pl.DataFrame(data_list).select(pl.all())        \n                return df_unseen_emails\n        except Exception as e:\n            logging.exception(f\"Error creating DataFrame of unseen emails:\")\n            return Response({'error': str(e)}, status=status.HTTP_400_BAD_REQUEST)  \n        \n    \n    def get_donot_reply_emails(self, unseen_msgs, words):\n        \"\"\"\n        This function returns a list of ids of \"do not reply emails\".        \n        Args:\n            unseen_msgs (list): List of unseen emails.\n            WORDS (list): List of words.\n        Returns:\n            donot_reply_mail_ids (list): List of \"do not reply emails\".\n        \"\"\"       \n        donot_reply_mail_ids = []\n        data_list = []\n        \n        for msg in unseen_msgs:\n            id, subject, sender, _, body, message_id  = self.extract_email_info(msg)\n            data_list.append({'Id': id, 'Subject': subject, 'Sender': sender, 'Body': body,'Message-ID': message_id})\n           \n        for email_data in data_list:\n            subject_lower = email_data['Subject'].lower()\n            sender_lower = email_data['Sender'].lower()\n            body_lower = email_data['Body'].lower()\n\n            if any(word in sender_lower or word in body_lower or word in subject_lower for word in words):\n                donot_reply_mail_ids.append(email_data['Id'])\n            \n        return donot_reply_mail_ids\n            \n                \n    def create_directories(self, new_directories):\n        \"\"\"\n        This function extract a list of directories.\n        If some directory does'nt exist, it will be created.        \n        Args:\n            new_directories (list): List of new directories.\n        \"\"\"\n        directories=[]\n        for directory in self.mail.list()[1]:\n            directories.append(directory.decode().split(' \"/\" ')[1])\n        for new_directory in new_directories:\n            if new_directory not in directories:\n                self.mail.create(new_directory)\n        \n\n    def filter_emails_by_domain(self, unseen_msgs, EMAIL_DOMAINS):\n        \"\"\"\n        This function filters emails by domain.\n        Args:\n            unseen_msgs (list): List of unseen emails.\n        Returns:\n            reply_mail_ids (list): List of reply emails.\n        \"\"\"        \n        reply_mail_ids = []\n        data_list = []\n\n        for msg in unseen_msgs:\n            id, _, sender, _, _, _  = self.extract_email_info(msg)\n            data_list.append({'Id': id, 'Sender': sender})\n\n\n        for email_data in data_list:\n            sender_lower = email_data['Sender'].lower()\n            if any(domain in sender_lower for domain in EMAIL_DOMAINS):\n                reply_mail_ids.append(email_data['Id'])\n\n        return reply_mail_ids\n    \n\n    def filter_unseen_emails(self, df_unseen_emails, donot_answer_mail_ids, reply_email_ids): \n        \"\"\"\n        This function rules out emails no-reply, \n        unsuscribe or potential spam, and keeps a list of clean emails.        \n        Args:\n            df_unseen_emails (pl.DataFrame): DataFrame of unseen emails.        \n        Returns:\n            result_list (list): List of available emails to create a draft reply.\n        \"\"\"\n        try:\n            df_unseen_emails = df_unseen_emails.with_columns(\n                pl.concat_str(\n                    df_unseen_emails['Id'],\n                    df_unseen_emails['Subject'],\n                    df_unseen_emails['Sender'],\n                    df_unseen_emails['Date'],\n                    df_unseen_emails['Body'],\n                    df_unseen_emails['Message-ID'],\n                    separator='||'\n                ).alias('text'),\n            )\n            \n            X_test = df_unseen_emails['text'].to_list()\n\n            batch_encoding = self.tokenizer(X_test, truncation=True, padding=True,  return_tensors=\"pt\")\n\n            with torch.no_grad():\n                outputs = self.model(**batch_encoding)\n                predictions = F.softmax(outputs.logits, dim=1)\n                labels = torch.argmax(predictions, dim=1)   \n\n            df_final = pl.DataFrame({'text': X_test, 'label_hf': labels.tolist(), 'Id': df_unseen_emails['Id'].cast(pl.Int64)})                      \n            \n            logging.info(\"df_final with hf tag\", df_final)\n\n            df_unseen_emails = df_unseen_emails.with_columns(df_unseen_emails['Id'].cast(pl.Int64))\n            donot_answer_mail_ids = list(map(int, donot_answer_mail_ids))\n            reply_email_ids = list(map(int, reply_email_ids))\n\n            df_reply_emails = df_final.filter(pl.col('Id').is_in(reply_email_ids))\n\n\n            df_reply_emails = df_reply_emails.with_columns(\n                df_reply_emails['label_hf'].replace([1], [0])\n            )\n\n            logging.info(\"df_reply_emails with hf tag changed\", df_reply_emails)\n\n            df_final = df_final.join(df_reply_emails, on='Id', how='left').fill_null(df_final['label_hf'])\n            df_final = df_final.drop(['text_right', 'label_hf'])\n            df_final = df_final.rename({'label_hf_right': 'label'})\n\n            logging.info(\"df_final with reply tag\", df_final)\n\n            df_no_reply = df_final.filter(pl.col('Id').is_in(donot_answer_mail_ids)) \n\n            df_no_reply = df_no_reply.with_columns(\n                df_no_reply['label'].replace([0], [1])\n            )\n\n            df_final = df_final.join(df_no_reply, on='Id', how='left').fill_null(df_final['label'])            \n            df_final = df_final.drop(['text_right', 'label'])\n            df_final = df_final.rename({'label_right': 'label'})\n\n            logging.info(\"df_final\", df_final)\n            \n            clean_ids = []\n            clean_emails= []           \n            for i in range(len(df_final)):\n                if df_final['label'][i] == 0:\n                    clean_emails.append(df_final['text'][i])\n                    clean_ids.append(df_final['Id'][i])\n                    \n            df_final_to_csv = df_unseen_emails.filter(pl.col('Id').is_in(clean_ids))\n            logging.info(\"df_final_to_csv:\", df_final_to_csv)\n\n            df_final = df_final.with_columns(df_final['Id'].cast(str)) \n\n            result_list = []\n            for item in clean_emails:\n                parts = item.split(\"||\")  \n                id, subject, sender, date, body, message_id= parts[:6]  \n                result_dict = {'Id': id , 'Subject': subject, 'From': sender, 'Date': date, 'Body': body, 'Message-ID': message_id}\n                result_list.append(result_dict)\n\n            return result_list, df_final, df_final_to_csv\n        \n        except pl.exceptions.PolarsError as e:            \n            return [],[], pl.DataFrame({'error_message': [f\"Empty Data Error: {e}\"]})\n        except ValueError as e:\n            return [],[], pl.DataFrame({'error_message': [f\"Value Error: {e}\"]})\n        except Exception as e:\n            return [],[], pl.DataFrame({'error_message': [f\"Unexpected Error: {e}\"]})\n\n    def create_csv(self, df_final_to_csv):\n        \"\"\"\n        This function creates a csv file of clean emails.        \n        Args:\n            df_final_to_csv (pl.DataFrame): DataFrame of clean emails.\n        \"\"\"\n        try: \n            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n            if df_final_to_csv.shape[0] > 0:          \n                df_final_to_csv.write_csv('utilities/inbox_' + timestamp + '.csv')\n\n        except Exception as e:\n            logging.exception(f\"Error creating CSV file of unseen emails:\")\n            return Response({'error': str(e)}, status=status.HTTP_400_BAD_REQUEST)\n        \n        \n    def tag_emails(self, df_final, new_directories):\n            \"\"\"\n            This function classifies clean emails into 2 categories:\n            personal and professional.\n        \n            Args:\n                clean_emails (list): List of clean emails.\n                df_final (pl.DataFrame): DataFrame of unseen emails.\n                new_directories (list): List of new directories.\n            \"\"\"\n            new_directories = [item for item in new_directories if item != 'Undefined']\n            for i in range(len(df_final)):\n                self.mail.store(df_final['Id'][i], '-X-GM-LABELS', 'Processing...')\n                if df_final['label'][i] == 1:\n                    self.mail.copy(df_final['Id'][i], 'Undefined')\n                else:\n                    self.mail.copy(df_final['Id'][i], 'Generating...')\n                    completion = CLIENT.chat.completions.create(\n                        model= MODEL_TAGGING,\n                        messages=[\n                            {\"role\": \"system\", \"content\": \"You're a classifier email bot.\"},\n                            {\"role\": \"user\", \"content\": f\"Classify the purpose of the following email as either  {', '.join(map(str, new_directories))} based on its content. Please provide a clear and concise categorization without explaining the reasons for your classification, Be carefully with all the context of the email. I just need 1 word,  {', '.join(map(str, new_directories))}:\\n\\n{df_final[i]}\"}\n                        ]\n                    )\n                    chosen_label = completion.choices[0].message.content\n    \n                    if chosen_label in new_directories:\n                        self.mail.copy(df_final['Id'][i], chosen_label)                        \n                    else:                        \n                        logging.error(f\"Unhandled label: {chosen_label}\")\n\n    def create_inbox_db(self, df_final_to_csv):\n        \"\"\"\n        This function creates a database of unseen emails.        \n        Args:\n            df_final (pl.DataFrame): DataFrame of unseen emails.\n        \"\"\"\n        try:\n            for msg in range(len(df_final_to_csv)):\n                InboxEmail.objects.create(\n                    id = df_final_to_csv['Id'][msg],\n                    subject = df_final_to_csv['Subject'][msg],\n                    sender = df_final_to_csv['Sender'][msg],\n                    date = df_final_to_csv['Date'][msg],\n                    body = df_final_to_csv['Body'][msg],\n                    message_id = df_final_to_csv['Message-ID'][msg]\n                )\n        except Exception as e:\n            logging.exception(f\"Error creating database of unseen emails:\")\n            return Response({'error': str(e)}, status=status.HTTP_400_BAD_REQUEST)"
    },
    {
        "file": "kwaai-pai/app/utilities/command_executor.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import subprocess\nfrom utilities.config import MODEL_NAME, EMAILS_DATA_SET_PATH\n\nclass CommandExecutor:\n    \"\"\"Command executor for fine-tunning a model.\"\"\"\n\n    def __init__(self):\n        self.model_name = MODEL_NAME\n\n    def execute_fine_tunning(self):\n        \"\"\"Execute the fine-tunning command.\"\"\"\n\n        base_command = \"python3 ./examples/scripts/sft.py\"\n        model_option = f\"--model_name {MODEL_NAME}\"\n        dataset_option = f\"--dataset_name {EMAILS_DATA_SET_PATH}\"\n        load_option = \"--load_in_4bit\"\n        use_peft_option = \"--use_peft\"\n\n        command = f\"{base_command} {model_option} {dataset_option} {load_option} {use_peft_option}\"\n\n        # try:\n        #     # Run the command\n        #     subprocess.run(command, shell=True, check=True)\n        # except subprocess.CalledProcessError as e:\n        #     print(f\"Command execution failed with error code {e.returncode}\")\n        #     print(e)\n"
    },
    {
        "file": "kwaai-pai/app/utilities/llm_response.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import requests\nimport json\n\ndef stream_response_and_concatenate(prompt):\n    \"\"\"Stream the response of a POST request and concatenate the content.\"\"\"\n    DOCKER_SERVER_URL = \"http://host.docker.internal:8080/completion\"\n    HEADERS = {\n        'Connection': 'keep-alive',\n        'Content-Type': 'application/json',\n        'Accept': 'text/event-stream',\n    }\n    PAYLOAD = {\n            \"stream\": True, \n            \"n_predict\": 500, \n            \"temperature\": 0.2, \n            \"stop\": [\"</s>\"],\n            \"prompt\": prompt,\n        }\n    content_pieces = []\n\n    response = requests.post(\n        DOCKER_SERVER_URL, \n        data=json.dumps(PAYLOAD), \n        headers=HEADERS, \n        stream=True\n    )\n\n    if response.status_code == 200:\n        try:\n            for line in response.iter_lines():\n                if line:\n                    decoded_line = line.decode('utf-8')\n                    if decoded_line.startswith('data:'):\n                        data_json = json.loads(decoded_line[5:])\n                        content_pieces.append(data_json['content'])\n        except KeyboardInterrupt:\n            print(\"Stream stopped by user.\")\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n    else:\n        print(f\"Failed to get a proper response, status code: {response.status_code}\")\n\n    complete_content = \"\".join(content_pieces)\n    return complete_content"
    },
    {
        "file": "kwaai-pai/app/utilities/rag_handler.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "import os\nimport logging\nimport warnings\nfrom utilities.config import HUGGINGFACE_ACCESS_TOKEN\nfrom embedchain import App\n\nos.environ[\"HUGGINGFACE_ACCESS_TOKEN\"] = HUGGINGFACE_ACCESS_TOKEN\n\nclass RAGHandler:\n    _instance = None\n    \n    def __new__(cls, model_config):\n        if cls._instance is None:\n            cls._instance = super(RAGHandler, cls).__new__(cls)\n            cls._instance._rag_app = App.from_config(model_config)\n        return cls._instance\n\n    @property\n    def rag_app(self):\n        return self._rag_app\n    \n    def get_rag_response(self, question) -> str:        \n        try:\n            logging.info(f\"Executing query: {question}\")\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", category=FutureWarning)\n                response = self.rag_app.query(question)\n                logging.info(f\"Response: {response} \\n\")\n                return response\n        except Exception as e:\n            logging.exception(\"Unexpected error occurred when generating rag response.\")\n            return ({\"detail\": \" An unexpected error occurred, \" + str(e)}) \n        \n        \n    def add_source_csv(self, path):\n        self.rag_app.add(path, data_type='csv')"
    },
    {
        "file": "kwaai-pai/app/llm/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/llm/apps.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from django.apps import AppConfig\n\n\nclass LlmConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'llm'\n"
    },
    {
        "file": "kwaai-pai/app/llm/api/serializers.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"Email IMAP serializer\"\"\"\nfrom rest_framework import serializers\nfrom core.models import LLMResponse\n\nclass LLMInferenceSerializer(serializers.ModelSerializer):\n    \"\"\"Serializer for the LLMResponse object\"\"\"\n    class Meta:\n        model = LLMResponse\n        fields = '__all__'\n        read_only_fields = ('id',)"
    },
    {
        "file": "kwaai-pai/app/llm/api/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/llm/api/views.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"Views for Email IMAP API\"\"\"\nfrom rest_framework import status, generics\nfrom rest_framework.response import Response\n\nfrom llm.api.serializers import LLMInferenceSerializer\nfrom utilities.create_email_draft import create_email_draft\nimport json\n\nclass LLMInferenceViewSet(generics.GenericAPIView):\n    serializer_class = LLMInferenceSerializer\n\n    def post(self, request, *args, **kwargs):\n        serializer = self.get_serializer(data=request.data)\n\n        try:\n            serializer.is_valid(raise_exception=True)\n            validated_data = serializer.validated_data\n\n            llm_response = create_email_draft(\n                to_address='apacheco@arkusnexus.com',\n                subject='Collaboration on Research',\n                prompt=validated_data['prompt'],\n            )\n\n            return Response({\n                'llm_response': llm_response,\n                'message': 'LLM response generated successfully',\n                }, status=status.HTTP_200_OK)\n        except json.JSONDecodeError as e:\n            return Response({'error': str(e)}, status=status.HTTP_400_BAD_REQUEST)"
    },
    {
        "file": "kwaai-pai/app/llm/api/urls.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"URLs for Email API handler\"\"\"\nfrom django.urls import path\nfrom llm.api.views import LLMInferenceViewSet\n\nurlpatterns = [\n    path('llama2/', LLMInferenceViewSet.as_view(), name='llama2-inference'),\n]"
    },
    {
        "file": "kwaai-pai/app/job/celery.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from __future__ import absolute_import, unicode_literals\nimport os\nfrom celery import Celery\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'app.settings')\n\ncelery_app = Celery('app')\n\ncelery_app.config_from_object('django.conf:settings', namespace='CELERY')\n\ncelery_app.autodiscover_tasks()"
    },
    {
        "file": "kwaai-pai/app/job/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/job/tasks.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from celery import Celery, shared_task\nfrom utilities.create_email_draft import create_email_draft\nfrom utilities.get_unseen_emails import get_unseen_emails\nimport logging\nfrom utilities.add_rag_source import add_rag_source\nfrom utilities.get_rag_response import get_rag_response\nimport os\nimport datetime\n\n\ncelery = Celery('tasks', broker='redis://redis:6379/0')\n\n\n@shared_task\ndef process_unseen_emails():\n    try:\n        unseen_emails = get_unseen_emails()        \n\n        logging.info(f\"Directory: {os.popen('ls -lha /app/utilities/').read()}\")\n        if not unseen_emails:\n            logging.info(\"No unseen emails\")\n            return\n        else:\n            path = 'utilities/'\n            csv_files = [f for f in os.listdir(path) if f.startswith('inbox_') and f.endswith('.csv')]\n            logging.info(\"csv_files list:\", csv_files)\n            if len(csv_files) > 0:\n                add_rag_source(path + csv_files[-1])\n            for unseen_email in unseen_emails:                \n                context = get_rag_response(\"Contextualize this email:\"+ unseen_email['Body'])\n                llm_response = create_email_draft(\n                    id = unseen_email['Id'],\n                    message_id = unseen_email['Message-ID'],\n                    to_address = unseen_email['From'],\n                    subject = 'Re: ' + unseen_email['Subject'], \n                    prompt= get_rag_response(\"Write an email draft that follows these guidelines:\"\n                                             + \"\\n\\n1. Answer to the sender's name '\" + unseen_email['From'] + \"'.\"\n                                             + \"\\n\\n2. Maintain the tone and style of George W Bush.\"\n                                             + \"\\n\\n3. Directly address the content provided by the sender in the email body: '\" + unseen_email['Body'] + \"', and answer it.\"\n                                             + \"\\n\\n4. Stay relevant to the thread ID: \" + unseen_email['Message-ID'] + \", and avoid mixing or referencing other email threads or conversations.\"\n                                             + \"\\n\\n5. Must exclude any phrases starting with 'Subject: Re:' in your response.\"\n                                             + \"\\n\\n6. If the context '\"+ context + \"' suggests an invitation (e.g., to a meeting, cinema, fair, cafe, just to chat or hang out), respond appropriately and invite to check my Google Calendar by clicking on the following link: 'https://calendar.app.google/4CV2J22aYaLiWRxL8' .\"\n                                             + \"\\n\\n7. Respond only to the context provided without diverging into unrelated topics.\"\n                                             + \"\\n\\n8. Ensure that your draft is concise, polite, and to the point, respecting the conversation's history '\"+ unseen_email['Message-ID'] +\"'.\"\n                                             + \"\\n\\n9. End the email with a respectful and personalized closing like 'Best regards' or 'Sincerely'.\"\n                                             + \"\\n\\n10. Do not include any disclaimers or explanatory notes within the response (e.g, Note:).\"\n                                             + \"\\n\\nFocus solely on responding within the context and requirements of this specific email thread '\"+ unseen_email['Message-ID'] +\"'.\"\n                                             )\n                )\n            \n            logging.info(\"llm_response: %s\", llm_response)\n    except Exception as e:\n        logging.exception(f\"Error processing unseen emails: {e}\")\n\n@celery.task\ndef process_new_emails(emails):\n    logging.info(f\"Processing new emails: {emails}\")\n\n"
    },
    {
        "file": "kwaai-pai/app/job/apps.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from django.apps import AppConfig\n\n\nclass JobConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'job'\n"
    },
    {
        "file": "kwaai-pai/app/sentbox_email/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/sentbox_email/apps.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from django.apps import AppConfig\n\n\nclass SentboxEmailConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'sentbox_email'\n"
    },
    {
        "file": "kwaai-pai/app/sentbox_email/api/serializers.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"Email IMAP serializer\"\"\"\nfrom rest_framework import serializers\nfrom core.models import ImapCredentials\n\nclass ImapEmailSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = ImapCredentials\n        fields = '__all__'"
    },
    {
        "file": "kwaai-pai/app/sentbox_email/api/__init__.py",
        "recommendation": "No code found in file",
        "code_snippet": ""
    },
    {
        "file": "kwaai-pai/app/sentbox_email/api/views.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"Views for Email IMAP API\"\"\"\nfrom rest_framework import status, generics\nfrom rest_framework.response import Response\nfrom core.models import ImapEmail\n\nfrom utilities.command_executor import CommandExecutor\nfrom utilities.imap_email_handler import ImapEmailHandler\nfrom utilities.config import KEYWORD, EMAIL_CREDENTIALS\nimport json\n\nclass ImapEmailViewSet(generics.GenericAPIView):\n    \"\"\"Email Credentials View\"\"\"\n\n    def get(self, request, *args, **kwargs):\n        queryset = ImapEmail.objects.all()\n        return Response(queryset.values(), status=status.HTTP_200_OK)\n\n    def post(self, request, *args, **kwargs):\n        try:\n            email = EMAIL_CREDENTIALS['email']\n            password = EMAIL_CREDENTIALS['password']\n            imap_server = EMAIL_CREDENTIALS['imap_server']\n\n            email_manager = ImapEmailHandler(email, password, imap_server)\n            mail_id_list = email_manager.search_emails(KEYWORD, email)\n            msgs_unformatted = email_manager.fetch_emails(mail_id_list)\n            msgs = email_manager.process_emails(msgs_unformatted)\n\n            for msg in msgs:\n                ImapEmail.objects.create(\n                    subject=msg.get('Subject', ''),\n                    from_email=msg.get('From', ''),\n                    timestamp=msg.get('Date', ''),\n                    body=msg.get('Body', '')\n                )\n            \n            executor = CommandExecutor()\n            executor.execute_fine_tunning()\n            \n            return Response(msgs, status=status.HTTP_200_OK)\n        \n        except json.JSONDecodeError as e:\n            return Response({'error': str(e)}, status=status.HTTP_400_BAD_REQUEST)\n"
    },
    {
        "file": "kwaai-pai/app/sentbox_email/api/urls.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "\"\"\"URLs for Email API handler\"\"\"\nfrom django.urls import path\nfrom sentbox_email.api.views import ImapEmailViewSet\n\nurlpatterns = [\n    path('', ImapEmailViewSet.as_view(), name='email-credentials'),\n]"
    },
    {
        "file": "kwaai-pai/quantization/merge_model.py",
        "recommendation": "Error analyzing code file: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
        "code_snippet": "from peft import PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nbase_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\nadapter_model_name = \"./checkpoints/checkpoint-100\" \n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name)\nmodel = PeftModel.from_pretrained(model, adapter_model_name)\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\nmodel = model.merge_and_unload()\nmodel.save_pretrained(\"merged_adapters\")\ntokenizer.save_pretrained(\"merged_adapters\")"
    }
]
